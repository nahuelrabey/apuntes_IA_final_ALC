# Soluci贸n del Ejercicio 1 (Examen 21 de julio de 2025 - SVD)

> **Ejercicio 1.** Sea $A$ una matriz con coeficientes reales de $n \times 2$. Sean $U$, $\Sigma$ y $V$ las matrices que dan su descomposici贸n SVD, con $u_i$ la columna $i$-茅sima de $U$, $\Sigma_{ii} = \sigma_i$ (con $\sigma_i \neq \sigma_j$ si $i \neq j$), y $v_i$ la columna $i$-茅sima de $V$. Sea $\tilde{A} = \sigma_1 u_1 v_1^t$ una aproximaci贸n de rango 1 de $A$.
>
> a) Si $x \in \mathbb{R}^2$ es un vector perteneciente al c铆rculo unitario, mostrar que el error cometido al calcular $Ax$ como $\tilde{A}x$ est谩 acotado por $\sigma_2$.
> 
> b) Sea $B = A^tA$ y $x \in \mathbb{R}^2$ elegido al azar. Mostrar que el siguiente algoritmo converge al vector $v_1$ cuando $N \to \infty$:
>
> - Para $k \in 1, \dots, N$:
>
>   - $x = Bx$
>
>   - $x = x / ||x||$
>
> c) Escriba una rutina que calcule la mejor aproximaci贸n de rango 1 de una matriz real de $n \times 2$ en el sentido de la norma 2. Toda funci贸n que involucre operaciones m谩s complejas que el producto matricial debe ser definida expl铆citamente.

---

## Interpretaci贸n del enunciado

Dado que la matriz $A$ es de dimensiones $n \times 2$, su descomposici贸n SVD $A = U \Sigma V^t$ nos indica rigurosamente, por propiedades algebraicas de dimensi贸n, que:

- $U$ es una matriz ortogonal de $n \times n$, por ende sus vectores columna la componen como $U = [u_1, u_2, \dots, u_n]$.

- $V$ es una matriz ortogonal de $2 \times 2$, ergo se compone acotadamente como $V = [v_1, v_2]$.

- $\Sigma$ es una matriz de $n \times 2$ (mismas dimensiones que $A$) que alberga a los valores singulares $\sigma_i$ estrictamente a lo largo de su "diagonal principal" (donde el 铆ndice de fila coincide con el de columna, $\Sigma_{ii}$). **En consecuencia, todos los dem谩s elementos que no pertenecen a esta diagonal son estrictamente nulos ($\Sigma_{ij} = 0$ para todo $i \neq j$)**. Esta es una propiedad basal inquebrantable de la SVD. A la hora de iterar, notamos que todas las filas contenidas entre $n=3$ hasta la 煤ltima ($n$) ser谩n filas llenas enteramente de ceros.

- La expresi贸n $\tilde{A} = \sigma_1 u_1 v_1^t$ se denomina **aproximaci贸n de rango 1**. El **Teorema de Eckart-Young-Mirsky** ([fuente matem谩tica](https://es.wikipedia.org/wiki/Teorema_de_Eckart-Young-Mirsky)) en 谩lgebra lineal demuestra infaliblemente que al truncar la SVD reteniendo 煤nicamente el o los mayores valores singulares, se obtiene la matriz "m谩s cercana" posible a la original minimizando el margen de error, en el sentido de la Norma Rectangular Espectral (Norma-2) y de Frobenius. Por ende, la conjunci贸n de los mayores vectores singulares $\sigma_1 u_1 v_1^t$ conforma la proyecci贸n hiper-dimensional estricta de mayor asertividad para explicar la matriz general reduci茅ndola a un solo espectro principal (un solo vector-base).

## Soluci贸n Inciso A
> a) Si $x \in \mathbb{R}^2$ es un vector perteneciente al c铆rculo unitario, mostrar que el error cometido al calcular $Ax$ como $\tilde{A}x$ est谩 acotado por $\sigma_2$.


Sabemos por definici贸n de la SVD truncada que como la matriz original $A$ es de orden $n \times 2$, s贸lo poseer谩 a lo sumo 2 valores singulares no nulos. Podemos entonces descomponerla como la suma de sus componentes de rango 1:

$$A = \sum_{i=1}^{k} \sigma_i u_i v_i^t = \sigma_1 u_1 v_1^t + \sigma_2 u_2 v_2^t$$

??? info "Profundizaci贸n: Descomposici贸n en Sumatoria de Matrices de Rango 1"
    Para cualquier matriz general $M \in \mathbb{R}^{m \times n}$ con rango $r$, la Descomposici贸n en Valores Singulares $M = U \Sigma V^t$ puede ser reescrita l贸gicamente usando multiplicaci贸n por bloques como una sumatoria exacta de $r$ matrices individuales, cada una de rango estrictamente 1:
    
    $$M = \sum_{i=1}^{r} \sigma_i u_i v_i^t$$
    
    Teniendo en cuenta que $u_i$ y $v_i$ son vectores columna, cada t茅rmino individual $u_i v_i^t$ (el **producto exterior** u *outer product* entre el $i$-茅simo vector singular izquierdo y su equivalente derecho interpuesto) arroja una matriz bidimensional completa de $m \times n$, pero de riguroso **rango 1** (al ser la multiplicaci贸n cruzada de meras vectores columnas lineales, todas las columnas de la matriz resultante terminan siendo m煤ltiplos de una 煤nica columna $u_i$).
    
    Al escalarla individualmente por su respectivo valor singular $\sigma_i$ (que act煤a como el "peso" escalado o la magnificaci贸n de esa componente a nivel espectral), la suma de estas "capas" de rango 1 superpuestas reconstruye milim茅tricamente la integralidad de $M$, ponderando y priorizando los elementos de mayor dominancia (las direcciones singulares principales).
    
     *Para verificar visualmente la demostraci贸n matem谩tica en pizarra impartida desde cero, pod茅s mirar la [Clase 29 (Singular Value Decomposition) dictada por Gilbert Strang para MIT 18.06 OpenCourseWare](https://www.youtube.com/watch?v=mBcLRGuAFUk).*

Dado que se nos informa que $\tilde{A} = \sigma_1 u_1 v_1^t$ es la aproximaci贸n de mayor rango, podemos definir el error vectorial de efectuar dicha predicci贸n como $e = A x - \tilde{A} x$.

Reemplazando los t茅rminos, el residuo es exactamente la componente descartada de la matriz SVD:

$$e = (A - \tilde{A}) x = (\sigma_2 u_2 v_2^t) x$$

Nos piden probar que la norma de este error se encuentra acotada por $\sigma_2$. Aplicamos la norma euclidiana o Norma-2 ($|| \cdot ||_2$) en ambos lados:

$$||e||_2 = ||\sigma_2 u_2 v_2^t x||_2$$

Como un escalar positivo puede extraerse de la norma:

??? info "Observaci贸n Te贸rica: 驴Por qu茅 $\sigma_2$ es un escalar puramente positivo?"
    Por definici贸n intr铆nseca de la descomposici贸n SVD, todos los valores singulares escalares $\sigma_i$ que componen a $\Sigma$ son siempre n煤meros reales **no negativos ($\sigma_i \geq 0$)**. Matem谩ticamente, esto deviene de que los valores $\sigma$ se calculan extrayendo la ra铆z cuadrada de los autovalores algebraicos de la matriz gramiana $A^tA$.
    
    Toda matriz pre-multiplicada por su transpuesta ($A^tA$) genera autom谩ticamente una matriz sim茅trica semi-definida positiva. Las matrices de este tipo subyacen inevitablemente a un espectro limitante de autovalores reales y positivos ($\lambda_i \geq 0$), imposibilitando en la abstracci贸n la existencia de ra铆ces imaginarias o valores singulares que fuesen negativos.
    
    Adicionalmente, como el enunciado aclara que los valores singulares no admiten repetici贸n ($\sigma_i \neq \sigma_j$) y provienen del habitual ordenamiento secular de magnitud descendente $\sigma_1 > \sigma_2 > 0$, concluimos fehacientemente que **$\sigma_2 > 0$**. 
    
    Al refrendar que es un escalar puramente positivo para todo escenario, nuestra expresi贸n queda habilitada l铆citamente para desacoplar a $\sigma_2$ por fuera de la funci贸n valor absoluto de la norma m茅trica subyacente de la que era parte impunemente: $|\sigma_2| = \sigma_2$ y $||\sigma_2 u|| = \sigma_2 ||u||$.
    
     *Para consultar la probanza algebraica oficial de estas propiedades imperativas, pod茅s remitirte a la [Clase 27 (Positive Definite Matrices and Minima) dictada por Gilbert Strang para MIT 18.06 OpenCourseWare](https://www.youtube.com/watch?v=vF7eyJ2g3kU).*

$$||e||_2 = \sigma_2 ||u_2 (v_2^t x)||_2$$

El t茅rmino entre par茅ntesis $(v_2^t x)$ resulta en un n煤mero escalar del producto interno vectorial. Lo podemos extraer en valor absoluto:

$$||e||_2 = \sigma_2 |v_2^t x| \cdot ||u_2||_2$$

Dado que las matrices de la SVD componen isometr铆as ortogonales perfectas, $U$ y $V$ se componen de vectores columna ortonormales unitarios. Es una certeza, por ende, que el vector singular de la izquierda $u_2$ posee norma estricta igual a 1 ($||u_2||_2 = 1$):

$$||e||_2 = \sigma_2 |v_2^t x|$$

Al observar minuciosamente la parte restante correspondiente al producto interno $|v_2^t x|$, descubrimos que ambos vectores provienen del disco estandarizado: sabemos que $x \in \mathbb{R}^2$ con $||x||_2 = 1$ (nos dicen que pertenece al "c铆rculo unitario") y paralelamente $v_2$ siempre es un vector ortonormal de $V$, por lo cual $||v_2||_2 = 1$.

Recurriendo a la c茅lebre inecuaci贸n de Cauchy-Schwarz:

$$|v_2^t x| \leq ||v_2||_2 \cdot ||x||_2 = 1 \cdot 1 = 1$$

Consumiendo nuestra afirmaci贸n, arribamos a que:

$$||e||_2 \leq \sigma_2 \cdot 1 = \sigma_2$$

Queda as铆 demostrado que el error cometido en la aproximaci贸n $\|A x - \tilde{A} x\|_2$ siempre estar谩 acotado por arriba por la magnitud del segundo valor singular descartado, $\sigma_2$.

---

## Soluci贸n Inciso B
> b) Sea $B = A^tA$ y $x \in \mathbb{R}^2$ elegido al azar. Mostrar que el siguiente algoritmo converge al vector $v_1$ cuando $N \to \infty$:
>
> - Para $k \in 1, \dots, N$:
> 
>   - $x = Bx$
> 
>   - $x = x / ||x||$


Si $A = U \Sigma V^t$, podemos sustituirlo en lo provisto por la letra del ejercicio $B = A^t A$:

$$B = (U \Sigma V^t)^t (U \Sigma V^t) = V \Sigma^t U^t U \Sigma V^t$$

Dada la ortogonalidad de la matriz $U$, sabemos que su autotranspuesta obedece $U^t U = I$. Aplicando este axioma simplificador:

$$B = V \Sigma^t \Sigma V^t$$

Dado que la matriz pre-multiplicada $\Sigma^t \Sigma$ conforma invariablemente una matriz diagonal cuadrada en la que surgen iterativamente los valores singulares originales elevados al cuadrado ($\Sigma_{ii}^2 = \sigma_i^2$), entonces los autovalores formales de $B$ (que recordemos es de $2 \times 2$), llamados convencionalmente $\lambda_1$ y $\lambda_2$, son por a帽adidura:

$$\lambda_1 = \sigma_1^2$$

$$\lambda_2 = \sigma_2^2$$

Los autovectores de $B$ son precisamente las columnas de la matriz $V$, denotados como $v_1$ y $v_2$.

??? info "Observaci贸n Te贸rica: 驴Por qu茅 los autovectores de $B$ son las columnas de $V$?"
    A partir de la ecuaci贸n superior deducimos que $B = V (\Sigma^t \Sigma) V^t$. Si a la matriz diagonal central la rebautizamos como $\Lambda = \Sigma^t \Sigma$, nos queda formulado:
    
    $$B = V \Lambda V^t$$
    
    Por definici贸n de la SVD, sabemos que $V$ conforma una matriz **ortogonal** perfecta. Las matrices ortogonales gozan de la propiedad inversa elemental en la que $V^t = V^{-1}$. Sustituyendo esto en la ecuaci贸n:
    
    $$B = V \Lambda V^{-1}$$
    
    Esta gloriosa disposici贸n coincide sim茅tricamente con la definici贸n can贸nica universal de la **Diagonalizaci贸n de Matrices por Autovalores** ($M = P D P^{-1}$), donde el teorema espectral dicta irrefutablemente que $D$ (nuestra $\Lambda$) es la matriz diagonal que aloja de forma descendente los **autovalores**, y $P$ (nuestra $V$) es la matriz de paso cuyas columnas albergan los **autovectores** ortonormalizados linealmente independientes correspondientes a cada escal贸n de $\Lambda$.
    
    Por alineaci贸n axiom谩tica directa, las columnas de $V$ ($v_1, v_2$) son sin lugar a dudas los autovectores de la matriz sim茅trica $B$.
    
     *Para verificar visualmente la demostraci贸n completa del Teorema Espectral y el mecanismo de diagonalizaci贸n $\Lambda$, te sugiero consultar la [Clase 22 (Diagonalization and Powers of A) dictada por Gilbert Strang para MIT 18.06 OpenCourseWare](https://www.youtube.com/watch?v=13r9QY6cmjc).*

En base a esto, y conociendo que los valores singulares de SVD exigen que $\sigma_i \neq \sigma_j$ y vienen t铆picamente ordenados descendiendo $\sigma_1 > \sigma_2 > 0$, deducimos que $\lambda_1 > \lambda_2 \geq 0$.

El algoritmo planteado eval煤a un simple bucle $k \in 1, \dots, N$ sobre la operaci贸n iterada:

$$x^{(k)} = \frac{B x^{(k-1)}}{||B x^{(k-1)}||} = \frac{B^k x^{(0)}}{||B^k x^{(0)}||}$$

??? info "Observaci贸n Te贸rica: 驴Por qu茅 la iteraci贸n asume esta forma colapsada general?"
    El mecanismo fundamental del **M茅todo de la Potencia** normalizado a menudo confunde porque en cada paso iterativo se divide por la norma entera del vector obtenido, pareciendo perder el hilo del vector primigenio $x^{(0)}$. No obstante, podemos probar por inducci贸n que esto es un simple cambio de escala unidimensional:
    
    1. En el paso 1, multiplicamos e inyectamos la norma: $x^{(1)} = \frac{B x^{(0)}}{||B x^{(0)}||}$.
    2. En el paso 2, insertamos $x^{(1)}$: $x^{(2)} = \frac{B x^{(1)}}{||B x^{(1)}||} = \frac{B \left( \frac{B x^{(0)}}{||B x^{(0)}||} \right)}{||B \left( \frac{B x^{(0)}}{||B x^{(0)}||} \right)||}$.
    
    Dado que los denominadores son estrictamente escalares num茅ricos reales ($c = ||B x^{(0)}||$), las propiedades de linealidad de normas y matrices nos permiten sacar los escalares afuera tanto del numerador matricial como de la norma del denominador ($B(c v) = c B v$ y $||c v|| = |c| ||v||$). 
    
    Al sacarlo de ambos lados simult谩neamente, **el escalar acumulativo del paso anterior se cancela exactamente a s铆 mismo en el cociente**:
    
    $$x^{(2)} = \frac{\frac{1}{||B x^{(0)}||} \cdot B^2 x^{(0)}}{\frac{1}{||B x^{(0)}||} \cdot ||B^2 x^{(0)}||} = \frac{B^2 x^{(0)}}{||B^2 x^{(0)}||}$$
    
    Efectuando este patr贸n colapsable iterativamente $k$ veces (todos los divisores escalares de los pasos intermedios nacen y mueren mutuamente cancelados por linealidad), arribamos a la inmaculada conclusi贸n de que sin importar cu谩ndo o cu谩ntas veces re-normalicemos el vector a magnitud 1 durante el bucle de For, la direcci贸n espacial que apunta $x^{(k)}$ proviene indefectiblemente de elevar emp铆ricamente a $B$ a la potencia $k$ desde el inicio ($B^k x^{(0)}$) y dividir todo ese armatoste final por su propia norma universal ($||B^k x^{(0)}||$) reci茅n al terminar.

Se nos explica que $x \in \mathbb{R}^2$ es elegido al azar, por lo que podemos representarlo en funci贸n de la base ortonormal completa del plano compuesto por $v_1$ y $v_2$:

$$x^{(0)} = c_1 v_1 + c_2 v_2$$

??? info "Observaci贸n Te贸rica: 驴Por qu茅 $v_1$ y $v_2$ forman una Base Ortonormal completa para $\mathbb{R}^2$?"
    En la descomposici贸n SVD original ($A = U \Sigma V^t$), la matriz $V$ tiene dimensiones $2 \times 2$. Por los teoremas angulares fundamentales de la SVD, sabemos inquebrantablemente que $V$ es una **Matriz Ortogonal** cuadrada.
    
    Toda matriz ortogonal cuadrada posee un espectro de vectores columna que son, por rigor axiom谩tico, mutuamente ortogonales (su producto punto interno es cero consecutivo, $v_1 \cdot v_2 = 0$) y de norma unitaria ($||v_i||_2 = 1$).
    
    El espacio vectorial $\mathbb{R}^2$ (el "plano") tiene por definici贸n dimensi贸n 2. Como $v_1$ y $v_2$ son 2 vectores linealmente independientes (atestado irrefutablemente por ser ortogonales entre s铆), constituyen un conjunto maximalmente extenso para este espacio dimensional. 
    
    Al tener la misma cantidad de vectores ortogonales que dimensiones tiene el espacio, se erigen autom谩ticamente como una **Base Ortonormal Completa**. Por lo tanto, cualquier vector misterioso extra铆do al azar de $\mathbb{R}^2$ (como nuestro $x^{(0)}$), puede ser modelado sint谩ctica y perfectamente a trav茅s de una combinaci贸n lineal bi-factorial directa de esta suprema "br煤jula" param茅trica: $x^{(0)} = c_1 v_1 + c_2 v_2$.

Aplicando el 谩lgebra del operador iterativo con iteraciones tendientes a $\infty$:

$$B^k x^{(0)} = c_1 \lambda_1^k v_1 + c_2 \lambda_2^k v_2$$

??? info "Observaci贸n Te贸rica: 驴C贸mo opera $B^k$ iterativamente sobre la base de autovectores?"
    Cuando aplicamos la matriz $B$ sucesivas veces sobre el vector inicial, estamos efectuando la operaci贸n $B^k x^{(0)}$.
    
    Por las propiedades definitorias del problema de autovalores, sabemos que pre-multiplicar la matriz $B$ por cualquiera de sus autovectores ($v_i$) da un resultado vectorial id茅ntico a simplemente multiplicar ese vector por su escalar asimilado ($\lambda_i$). En otras palabras, la matriz se comporta como un n煤mero y solo genera un estiramiento unidimensional, sin rotarlo en el espacio: $B v_i = \lambda_i v_i$.
    
    Si imponemos esta transformaci贸n $k$ veces sucesivas ("potencias de la matriz"), los factores de escala escalatorios crecen naturalmente de manera exponencial: $B^k v_i = \lambda_i^k v_i$.
    
    Sustituyendo en esta expresi贸n nuestro vector original partido en componentes del plano:
    
    $$B^k x^{(0)} = B^k (c_1 v_1 + c_2 v_2)$$
    
    Propagando la transformaci贸n de la matriz por distributiva matricial pura:
    
    $$B^k x^{(0)} = c_1 (B^k v_1) + c_2 (B^k v_2)$$
    
    Sustituyendo la conducta matricial proyectada en las componentes individuales para desacoplarnos matem谩ticamente del producto matricial y transformarlo en escalares algebraicos:
    
    $$B^k x^{(0)} = c_1 \lambda_1^k v_1 + c_2 \lambda_2^k v_2$$
    
     *El basamento doctrinario supremo para asimilar c贸mo una matriz iterativa se deconstruye y expande sus propios autovalores en el tiempo es ilustrado majestuosamente en la [Clase 22 (Diagonalization and Powers of A) - MIT 18.06 Linear Algebra, Fall 2005 por Gilbert Strang](https://www.youtube.com/watch?v=13r9QY6cmjc).*


Factorizando para independizarnos del exponente del autovalor dominante:

$$B^k x^{(0)} = \lambda_1^k \left( c_1 v_1 + c_2 \left( \frac{\lambda_2}{\lambda_1} \right)^k v_2 \right)$$

Dado que denotamos ex-ante que $\lambda_1 > \lambda_2$, la fracci贸n es decididamente un n煤mero en el recuadro menor a la unidad $( \frac{\lambda_2}{\lambda_1} < 1 )$. Por propiedad de los l铆mites asint贸ticos exponenciales:

$$\lim_{k \to \infty} \left( \frac{\lambda_2}{\lambda_1} \right)^k = 0$$

Al aproximarse velozmente todo el segundo t茅rmino aditivo al valor cero, obtenemos con precisi贸n que la iteraci贸n de nuestro vector de partida se alinear谩 siendo puramente colineal con el primer autovector:

$$B^k x^{(0)} \approx \left( \lambda_1^k \cdot c_1 \right) v_1$$

Evidentemente si nuestro vector estoc谩stico naci贸 sin componente principal ($c_1 = 0$, ortogonalidad perfecta elegida para nuestra semilla inicial), el proceso fallar谩 al converger a $v_2$. Tal salvedad, en algoritmos estoc谩sticos que operan en floats (cuyos conjuntos de medida afirman que extraer el exacto plano euclidiano en flotantes ortogonales tiene te贸ricamente "probabilidad cero"), carece de sustento para descalificar la iteraci贸n.

Como para rematar cada ciclo el vector se normaliza contra s铆 mismo $x = \frac{x}{||x||}$, todo componente escalar global $\lambda$ decae por divisiones intr铆nsecas, dejando con exclusividad un vector de tama帽o 1 alineado a la direcci贸n principal:

$$\lim_{N \to \infty} x^{(N)} = \pm v_1$$

Evidenciando la validez asint贸tica te贸rica de lo que universalmente nombramos "M茅todo de la Potencia".

---

## Soluci贸n Inciso C: Rutina en Python
> c) Escriba una rutina que calcule la mejor aproximaci贸n de rango 1 de una matriz real de $n \times 2$ en el sentido de la norma 2. Toda funci贸n que involucre operaciones m谩s complejas que el producto matricial debe ser definida expl铆citamente.


A continuaci贸n, la rutina desarrollada sin emplear funciones de grado superlativo como factorizaciones directas en la librer铆a algor铆tmica. Para hallar el valor singular predominante nos basamos en el cociente de Rayleigh, aplicando producto punto cl谩sico entre la iteraci贸n convergiendo de las potencias.

```python
--8<-- "Examen_2025_07_21/01_svd/verificacion.py"
```
