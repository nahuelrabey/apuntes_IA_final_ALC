# Soluci칩n del Ejercicio 2 (Examen 21 de julio de 2025 - Diagonalizaci칩n)

> **Ejercicio 2.** Sea una matriz $A \in \mathbb{R}^{n \times n}$ con $n$ autovalores mayores a cero y distintos entre s칤. Sean $\lambda_1, \dots, \lambda_n$ sus autovalores y $v_1, \dots, v_n$ los autovectores asociados. Mostrar que:
>
> a) $\{v_1, \dots, v_n\}$ forma una base de $\mathbb{R}^n$. Justificar.
> 
> b) La matriz $C \in \mathbb{R}^{n \times n}$ cuyas columnas est치n dadas por los vectores $v_1, \dots, v_n$ es inversible y cumple con que $AC = CS$, con $S$ una matriz diagonal con $\lambda_1, \dots, \lambda_n$ en la diagonal.
>
> c) La matriz $A$ es diagonalizable.

---

## Soluci칩n Inciso A

> a) $\{v_1, \dots, v_n\}$ forma una base de $\mathbb{R}^n$. Justificar.

El teorema fundamental sobre autovectores establece que "autovectores correspondientes a autovalores distintos son linealmente independientes". 

Dado que por hip칩tesis se nos confirma que la matriz $A$ posee $n$ autovalores estrictamente **distintos entre s칤** ($\lambda_i \neq \lambda_j$ para todo $i \neq j$), este lema nos garantiza de forma deductiva que el conjunto de sus correspondientes autovectores $\{v_1, \dots, v_n\}$ constituye un conjunto de exactamente $n$ vectores **linealmente independientes**.

??? info "Demostraci칩n Te칩rica: Independencia Lineal por Autovalores Distintos"
    El porqu칠 un "abanico" de autovalores distintos garantiza de forma obligatoria y deductiva que sus autovectores asociados no pueden colapsar formando dependencias espaciales, se demuestra axiom치ticamente mediante el Principio de Inducci칩n Fuerte Matem치tica.
    
    游늷 *Para consultar paso a paso la justificaci칩n anal칤tica y matem치tica detr치s de este Lema de Independencia (junto con su validador masivo estoc치stico en Python), puedes remitirte a: [Demostraci칩n: Independencia Lineal de Autovectores](../../demostraciones/autovalores_distintos.md).*

Sabemos que cualquier conjunto de $n$ vectores linealmente independientes dentro de un espacio vectorial eucl칤deo de dimensi칩n $n$ (como es en este caso $\mathbb{R}^n$) obligatoriamente genera dicho espacio (sirve como sistema generador) y, por consiguiente, forma inherentemente una **Base**. Queda justificado anal칤ticamente.

---

## Soluci칩n Inciso B

> b) La matriz $C \in \mathbb{R}^{n \times n}$ cuyas columnas est치n dadas por los vectores $v_1, \dots, v_n$ es inversible y cumple con que $AC = CS$, con $S$ una matriz diagonal con $\lambda_1, \dots, \lambda_n$ en la diagonal.

A partir del inciso (A), hemos concluido que los autovectores $\{v_1, \dots, v_n\}$ estructuran una base de $\mathbb{R}^n$ y por tanto son independientes. La matriz columna unificada $C$ se define en bloques como:

$$C = \begin{pmatrix} | & | & & | \\ v_1 & v_2 & \dots & v_n \\ | & | & & | \end{pmatrix}$$

Como sus columnas son vectores estrictamente **linealmente independientes**, su determinante no ser치 nulo y obligatoriamente existir치 su inversa (la matriz $C$ es inversible / no singular). 

??? info "Observaci칩n Te칩rica: 쯃os autovectores siempre son ortogonales entre s칤?"
    **No, rotundamente no.** El Lema demostrado en el inciso anterior 칰nicamente nos provey칩 las garant칤as algebraicas de que los autovectores son **Linealmente Independientes** por provenir de ra칤ces caracter칤sticas (autovalores) distintas. 
    
    Que sean linealmente independientes significa que "no son combinaci칩n lineal entre s칤" y su span basta para cubrir las dimensiones del espacio, logrando por definici칩n que el determinante de la matriz formada $C$ sea distinto de cero (inversible).
    
    Sin embargo, **la ortogonalidad (que formen 치ngulos perfectos de 90춿 o que su producto interno $v_i \cdot v_j = 0$) es una propiedad de 칠lite reservada de manera exclusiva y rigurosa para las Matrices Sim칠tricas Reales** (por aplicaci칩n del c칠lebre *Teorema Espectral*). 
    
    Para una matriz cuadrada $A$ general asim칠trica, sus autovectores construir치n firmemente una base para $\mathbb{R}^n$, pero en la inmensa mayor칤a de los casos ser치 una **base oblicua** (independientes pero **no ortogonales**).
    
    游늷 *Para constatar la rigurosidad conceptual de esta distinci칩n vital (que la ortogonalidad se forja como propiedad exclusiva del Teorema Espectral ante matrices sim칠tricas reales), cons칰ltese la [Clase 25 (Symmetric Matrices and Positive Definiteness) - Prof. Gilbert Strang (MIT 18.06 OpenCourseWare)](https://www.youtube.com/watch?v=13r9QY6cmjc&list=PLE7DDD91010BC51F8&index=26) o la [Wikipedia: Spectral Theorem (Symmetric matrices)](https://en.wikipedia.org/wiki/Spectral_theorem).*

A continuaci칩n debemos probar la aseveraci칩n anal칤tica de igualdad. Evaluemos el producto en el miembro izquierdo $AC$:

$$AC = A \begin{pmatrix} | & & | \\ v_1 & \dots & v_n \\ | & & | \end{pmatrix} = \begin{pmatrix} | & & | \\ Av_1 & \dots & Av_n \\ | & & | \end{pmatrix}$$

Por la naturaleza te칩rica de un autovector asosciado a su respectivo autovalor, sustituimos que la transformaci칩n sobre ella resulta en un reescalado hom칩tetico por el propio autovalor $Av_i = \lambda_i v_i$:

$$AC = \begin{pmatrix} | & & | \\ \lambda_1 v_1 & \dots & \lambda_n v_n \\ | & & | \end{pmatrix}$$

Por el otro flanco, evaluemos el miembro derecho mediante el producto de $C$ por la matriz diagonal espectral $S = \text{diag}(\lambda_1, \dots, \lambda_n)$:

$$CS = \begin{pmatrix} | & & | \\ v_1 & \dots & v_n \\ | & & | \end{pmatrix} \begin{pmatrix} \lambda_1 & & 0 \\ & \ddots & \\ 0 & & \lambda_n \end{pmatrix}$$

La regla subyacente de la multiplicaci칩n por derecha de una matriz escalar diagonal afirma que cada elemento multiplicar치 la columna entera de su misma posici칩n indexada. Esto resulta, irrebatiblemente, en id칠ntico resultado que nuestro primer paso matem치tico evaluado superiormente:

$$CS = \begin{pmatrix} | & & | \\ \lambda_1 v_1 & \dots & \lambda_n v_n \\ | & & | \end{pmatrix}$$

Dado que ambos caminos algebraicos convergen a la misma matriz constituida por los vectores proyectados, se decreta que **$AC = CS$**.

---

## Soluci칩n Inciso C

> c) La matriz $A$ es diagonalizable.

En el inciso estipulado previamente demostramos que subsiste la igualdad operacional:

$$AC = CS$$

En ese mismo desarrollo inicializamos nuestra justificaci칩n validando que $C$ formaba una matriz con inversa existencial $C^{-1}$. Con esta autoridad l칤cita, podemos optar por premultiplicar y despejar ambos extremos val칤endonos por derecha de dicha inversa:

$$(AC)C^{-1} = (CS)C^{-1}$$

Asumiendo la propiedad asociativa, $CC^{-1} = I$ (siendo $I$ la Matriz Identidad) liberando a $A$:

$$A = C S C^{-1}$$

쯈u칠 significa algebraicamente que $A$ pueda re-enunciarse como un sistema rec칤proco $C S C^{-1}$?. Por la definici칩n ontol칩gica expuesta en el 치lgebra estructural matricial: **una matriz cuadrada es diagonalizable si y s칩lo si es semejante a una matriz diagonal**.

La igualdad demostrada verifica de manual dicha condici칩n. Puesto que confirmamos que $A$ est치 formulada por una matriz de pasaje de base en t치ndem con la matriz diagonal de sus autovalores ($S$), **entonces la matriz A es incuestionablemente diagonalizable**, ergo sus operaciones vectoriales sobre $\mathbb{R}^n$ se reducen a proyecciones re-escalables y ortogr치ficas en el subespacio rotado.

---

## Verificaci칩n Computacional en Python

```python
--8<-- "Examen_2025_07_21/02_diagonalizacion/verificacion.py"
```
