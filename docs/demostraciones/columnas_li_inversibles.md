# Demostraci√≥n: Independencia Lineal e Invertibilidad

## Interpretaci√≥n del Enunciado

> ¬øUna matriz cuadrada $A$, bajo el supuesto fundamental de que posee todas sus columnas conformando un conjunto cerrado de **vectores linealmente independientes**, gozar√° *siempre* de la propiedad un√≠voca de ser **inversible**?

La respuesta a esta conjetura no s√≥lo es afirmativa a nivel te√≥rico, sino que representa el n√∫cleo central para entender a priori el comportamiento hol√≠stico de una transformaci√≥n lineal sin necesidad real de observar sus entra√±as. Si entendemos a las columnas como "ejes" direccionales en el espacio base, la preexistencia de la misma cantidad de ejes independientes que las dimensiones del cuerpo real garantiza un mapeo no destructivo.

---

## Soluci√≥n Anal√≠tica

Desarrollaremos la demostraci√≥n del **Teorema de la Matriz Inversible (IMT)** conectando la dependencia lineal escalar con la suryectividad (rango) a trav√©s de una deducci√≥n formal progresiva.

Abordamos una matriz cuadrada arbitraria $A \in \mathbb{R}^{n \times n}$, que se encuentra constituida matem√°ticamente por una serie concatenada horizontal de $n$ vectores columna, los cuales denotaremos anal√≠ticamente como $\{a_1, a_2, \dots, a_n\}$.

$$
A = \begin{bmatrix} | & | & & | \\ a_1 & a_2 & \dots & a_n \\ | & | & & | \end{bmatrix}
$$

### Fase 1: An√°lisis del Espacio Nulo (Kernel)

Como dicta la consigna originaria, los precursores o vectores columna $\{a_1, \dots, a_n\}$ son un postulado de elementos **estrictamente Linealmente Independientes**.

En el universo del √°lgebra lineal, la definici√≥n universal estipula f√©rreamente que si un conjunto es linealmente independiente (L.I.), la √∫nica manera causal por la cual una combinaci√≥n escalar paralela de esos vectores lograr√° igualar y devolver el vector nulo ($\mathbf{0}$), es forzosamente a trav√©s de la soluci√≥n trivial donde los escalares valen nulo matem√°ticamente.

$$
x_1 a_1 + x_2 a_2 + \dots + x_n a_n = \mathbf{0} \quad \implies \quad x_1 = x_2 = \dots = x_n = 0
$$

Por definici√≥n b√°sica del producto de una Matriz por un Vector, sabemos que **multiplicar una matriz $A$ por un vector $\mathbf{x}$ es exactamente lo mismo que armar una combinaci√≥n lineal usando las columnas de $A$**, donde los escalares multiplicadores son justamente los elementos individuales de ese vector $\mathbf{x}$.

Llevando este concepto a nuestro caso particular de estar igualado al vector nulo, la ecuaci√≥n del sistema homog√©neo ($A \mathbf{x} = \mathbf{0}$) queda planteada como:

$$
(Eq. 1) \quad A \cdot \mathbf{x} = \mathbf{0}
$$

Dada la premisa anterior de vectores independientes y su estricta obediencia de colapsar la sumatoria en ceros, probamos innegablemente que su soluci√≥n subyacente obliga a que $\mathbf{x} = (0, 0, \dots, 0)^T = \mathbf{0}$.

Que la soluci√≥n √∫nica en todo su hiperespacio dimensional real sea ineludiblemente trivial, certifica entonces geom√©tricamente que el **N√∫cleo de la transformaci√≥n** (*Kernel o Null Space*, $\text{Nul}(A)$) se encuentra totalmente desierto, contenido √∫nicamente por el propio origen:

$$
\text{Nul}(A) = \{\mathbf{0}\}
$$

Que el kernel se haya comprobado nulo implica la preexistencia de un mapeo estrictamente inyectivo (uno a uno). Al ser $A$ una inyecci√≥n perfecta, dos vectores nunca colapsar√°n sobre un mismo punto hom√≥nimo destructivo tras la linealidad.

### Fase 2: Aplicaci√≥n del Teorema del Rango

El segundo paso de eslabonamiento dogm√°tico de nuestro silogismo consiste en apoyarnos en la fortaleza del **Teorema del Rango-Nulidad** (*Rank-Nullity Theorem*). La ley cardinal vinculante entre subespacios dictamina que para cualquier matriz de $n$ columnas su relaci√≥n inter-espacial dimensional ha de ser balanceada:

$$
\dim(\text{Col}(A)) + \dim(\text{Nul}(A)) = n
$$

*(Donde $\text{Col}(A)$ es el subespacio columna que denota hacia c√∫antas dimensiones estamos estirando el mapeo, y el nulidad ata√±e las colapsadas).*

Basados en el colosal descubrimiento obtenido en nuestra *Fase 1* ($\dim(\text{Nul}(A)) = 0$), su reemplazo un√≠voco arroja la siguiente verdad indomable por despeje directo:

$$
\dim(\text{Col}(A)) + 0 = n
$$

$$
\text{Rango}(A) = n
$$

El dictamen expone en base a esto que el alcance perimetral de las combinaciones posibles ($\text{Col}(A)$), logra abarcar imperiosamente toda la totalidad de dimensiones del subconjunto destino de imagen (es un operador $\mathbb{R}^n \to \mathbb{R}^n$). Como el subespacio es coincidente y abarcativo con todo el espacio de base, demostramos irrefutablemente que **la matriz opera de manera totalmente sobreyectiva**.

### Conclusi√≥n Axiom√°tica

Demostramos emp√≠ricamente c√≥mo al concatenar l√≥gicamente las ramificaciones abstractas de las columnas:

1. Al albergar exclusivamente vectores independientes, su n√∫cleo colaps√≥ en $\mathbf{0}$ garantizando la **Inyectividad** pura matricial.
2. Esta nulidad dictamin√≥, v√≠a Teorema del Rango, un Rank $n$ de cobertura completa que certificaba que las columnas eran una base real, validando la **Sobreyectividad** paralela.

En todo campo abstracto funcional, cualquier mapa originario o transformaci√≥n abstracta en $n \times n$ que garantice gozar en plenitud y en simultaneo de propiedades inyectivas como sobreyectivas, detenta geom√©tricamente el t√≠tulo de *Funci√≥n Biyectiva*.

Por doctrina un√≠voca matem√°tica, **toda transformaci√≥n lineal que sostenga de forma ininterrumpida una correlaci√≥n biyectiva es l√≥gicamente reversible en el espacio**, y el acto algebraico singular correlativo a una funci√≥n que se puede deshacer es, indudablemente, que **su operadora subyacente ($A$) califique como matriz universalmente inversible**, o no-singular ($\exists A^{-1}$ y $\det(A) \neq 0$).

‚àé

---

## Verificaci√≥n Emp√≠rica Computacional

La correspondencia te√≥rica subyacente planteada entre el rango o independencia de un set vectorial original que compone la estructura medular (*core*) de una matriz y el consecuente nacimiento instant√°neo en paralelo de su propiedad de invertibilidad, es testeada incansablemente bajo el siguiente simulador en Python.

```python
--8<-- "demostraciones/columnas_li_inversibles.py"
```

---

## Bibliograf√≠a y Recursos Educativos

Para la consolidaci√≥n y anclaje mnemot√©cnico al respecto de este fen√≥meno, referirse a:

### üìñ Libros de Texto y Art√≠culos

- **Libro: √Ålgebra Lineal y sus Aplicaciones (David C. Lay)**. *Cap√≠tulo 2.3: Caracterizaciones de matrices invertibles*. El autor fundamenta all√≠ el renombrado y pilar **Teorema de la Matriz Inversible** (IMT). Dicho compendio abarca un listado inquebrantable de $12$ afirmaciones hom√≥logas estoc√°sticas, donde demuestra que los equivalentes $A$ es *invertible* (Afirmaci√≥n A) est√°n atados per s√©cular a que *La ecuaci√≥n $Ax=0$ s√≥lo admite la soluci√≥n trivial* (C), y a su vez *Las columnas de $A$ operan como un conjunto L.I.* (E). Si una resiste el fallo, resiste sistem√°ticamente todas al ser homom√≥rficamente an√°logas.

### üåê Sitios Web Universitarios

- **[Interactive Linear Algebra (Georgia Tech)](https://textbooks.math.gatech.edu/ila/invertible-matrix-thm.html)**: Libro de texto interactivo abierto creado por la universidad Georgia Tech. En la subsecci√≥n *3.6 - The Invertible Matrix Theorem*, prueba de manera concisa y an√°loga a nuestro apunte c√≥mo el hecho de que el n√∫cleo sea $0$ (o lo que es lo mismo, independencia lineal) fuerza a que el rango de la matriz sea pleno y, por ende, goce de la propiedad de ser invertida por existir una correspondencia uno a uno (biyectiva).

### üá∫üá∏ Videos en Ingl√©s

- **[The Invertible Matrix Theorem (Dr. Trefor Bazett)](https://www.youtube.com/watch?v=kYJj06Gz0Cg)**: El Dr. Bazett explica pedag√≥gica y geom√©tricamente la lista gigantesca de condiciones que colisionan y significan exactamente lo mismo al momento de hablar de matrices invertibles, partiendo justamente desde el requerimiento de conformar Independencia Lineal y el Nulaje trivial del kernel.
