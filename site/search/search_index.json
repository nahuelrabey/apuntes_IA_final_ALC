{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Meta Documento: Framework y Metodolog\u00eda de Resoluci\u00f3n","text":"<p>Este documento busca funcionar como un \"blueprint\" o plantilla metodol\u00f3gica. A partir del ejercicio de \u00e1lgebra lineal resuelto (propiedades de la semejanza de matrices), hemos abstra\u00eddo una arquitectura operativa pensada para abordar futuras tareas similares que involucren una resoluci\u00f3n matem\u00e1tica seguida de una verificaci\u00f3n y validaci\u00f3n con programaci\u00f3n.</p>"},{"location":"#arquitectura-del-flujo-de-trabajo","title":"\ud83c\udfd7 Arquitectura del Flujo de Trabajo","text":"<p>El flujo de trabajo unificado se compone de tres pilares, ejecutados habitualmente en esta misma cronolog\u00eda:</p>"},{"location":"#1-fase-pura-razonamiento-teorico-el-que-y-el-por-que","title":"1. Fase Pura / Razonamiento Te\u00f3rico (El \"Qu\u00e9\" y el \"Por Qu\u00e9\")","text":"<p>Antes de cualquier l\u00ednea de c\u00f3digo, el problema es comprendido y deconstruido: - Identificaci\u00f3n de Definiciones: Entender sem\u00e1nticamente qu\u00e9 significan las f\u00f3rmulas. Ejemplo: \"\\(S A S^{-1} = B\\) representa el cambio de base, lo que implica que \\(A\\) y \\(B\\) manejan la misma transformaci\u00f3n original.\" - Descomposici\u00f3n T\u00e1ctica: Separar los problemas en componentes digeribles. \"Comprobar equivalencia\" no es un \u00fanico bloque, sino un checklist de (Reflexividad, Simetr\u00eda, Transitividad). - Desarrollo Anal\u00edtico: Plasmar el formalismo matem\u00e1tico de modo detallado, donde cada paso es l\u00f3gicamente deducible desde el anterior (usando las hip\u00f3tesis para destrabar el desarrollo de la tesis).</p>"},{"location":"#2-fase-de-traduccion-el-puente-logico-computacional","title":"2. Fase de Traducci\u00f3n (El Puente L\u00f3gico-Computacional)","text":"<p>Un teorema puede ser herm\u00e9tico y ajeno al c\u00f3digo, por lo que demanda una \"traducci\u00f3n\": - Abstracci\u00f3n a Modelos Estoc\u00e1sticos: Dado que no podemos corroborar el infinito, probamos con aleatoriedad (\"Randomization Testing\"). Es decir, si el teorema es universal, se debe sostener al alimentar las f\u00f3rmulas con matrices (arreglos n-dimensionales) llenos del espectro continuo flotante (e.g., generadas con elementos desde <code>NumPy</code>).  - Adaptaci\u00f3n de Restricciones: Traducir consideraciones te\u00f3ricas (\"\\(S\\) debe ser invertible\") a instrucciones para la m\u00e1quina (e.g. validaciones contra el determinante distinto de cero en un ciclo <code>While</code> generador).</p>"},{"location":"#3-fase-pragmatica-verificacion-empirica-el-sandbox","title":"3. Fase Pragm\u00e1tica / Verificaci\u00f3n Emp\u00edrica (El \"Sandbox\")","text":"<p>Se codifica el programa verificador que pondr\u00e1 a prueba el desarrollo anal\u00edtico: - Ejecuci\u00f3n y Comprobaci\u00f3n Booleana (Validaciones en Punto Flotante): En computaci\u00f3n cient\u00edfica, las afirmaciones como \"A la matriz original\" o \"Ambas Trazas miden igual\" raramente deben validarse con <code>==</code> (por problemas de redondeo/convergencia de hardware en el tipo float). Se usan metodolog\u00edas como <code>np.isclose()</code> o <code>np.allclose()</code> tolerando peque\u00f1os m\u00e1rgenes estad\u00edsticos de error computacional (\\(\\approx 1e^{-8}\\)). - Depuraci\u00f3n Bidireccional: Si el Test emp\u00edrico falla, esto dispara alarmas. Nos obliga a revisar o el c\u00f3digo de comprobaci\u00f3n (si hay problemas de implementaci\u00f3n), o bien encontrar falacias ocultas dentro de nuestra rigurosa prueba te\u00f3rica en la Fase 1.</p>"},{"location":"#documentacion-de-conclusiones","title":"\ud83d\udccc Documentaci\u00f3n de Conclusiones","text":"<p>Este h\u00edbrido de demostraci\u00f3n anal\u00edtica-matem\u00e1tica seguida de una prueba automatizada en un vector de c\u00e1lculo eficiente (como Python) resulta el paradigma en el estado del arte de la investigaci\u00f3n y aprendizaje. Genera lo que en l\u00f3gica se llama Confianza Incondicional: Si existe certeza sem\u00e1ntica en papel, y el procesador no halla contradicciones luego de ser testeado con caos num\u00e9rico aleatorio, la tarea fue resuelta con el m\u00e1ximo rigor posible.</p>"},{"location":"#lecciones-y-conclusiones-del-ejercicio-2","title":"Lecciones y Conclusiones del Ejercicio 2","text":"<ul> <li>Uso de Invarianzas y Transformaciones Ortogonales: En matem\u00e1ticas (como ocurre con la SVD), aplicar operaciones \"isom\u00e9tricas\" o transformaciones ortogonales (como fue multiplicar por una matriz de permutaci\u00f3n aleatoria computacionalmente, \\(P\\)) resulta invariante para las magnitudes nucleares (como el espectro singular). </li> <li>La Utilidad de la Permutaci\u00f3n Aleatoria Computada: A la hora de verificar propiedades sobre operadores donde \"El orden de las filas no altera el resultado estructural\", utilizar una matriz de permutaci\u00f3n estoc\u00e1stica (<code>P = I[np.random.permutation(n), :]</code>) sobre el c\u00f3digo es un factor de prueba estupendo. Si la propiedad estad\u00edstica persiste (ejemplo, la invariabilidad de <code>np.linalg.svd</code>) probamos emp\u00edricamente la independencia matem\u00e1tica del operador evaluado y confirmamos el modelo num\u00e9rico.</li> <li>Estabilidad de las Normas (Norma-2): El script nos demostr\u00f3 c\u00f3mo este tipo de transformaciones no introducen ruido algor\u00edtmico al \"estiramiento\" m\u00e1ximo de la matriz (la Norma 2) ni a su n\u00famero de condici\u00f3n. Computarizar \\(\\|PA\\|_2\\) arroj\u00f3 consistentemente el mismo resultado de norma debido a que los ortogonales preservan las longitudes vectoriales subyacentes, validando lo deducido con l\u00e1piz y papel.</li> </ul>"},{"location":"#lecciones-y-conclusiones-del-ejercicio-3","title":"Lecciones y Conclusiones del Ejercicio 3","text":"<ul> <li>Del Radio Espectral al C\u00f3digo Emp\u00edrico: Observamos c\u00f3mo la teor\u00eda matricial predice ex\u00e1ctamente el comportamiento iterativo del <code>while-loop</code>. A diferencia del algebra matricial anal\u00edtica, el c\u00e1lculo num\u00e9rico involucra medir tiempos (tasas) de procesamiento. Si el an\u00e1lisis formal sostiene que la tasa de convergencia asint\u00f3tica de una t\u00e9cnica es el doble de la que presenta otra (\\(\\rho(T_{GS}) = \\rho(T_J)^2\\)), computarizar un simple contador de los pasos que toma domar un residuo a una tolerancia l\u00edmite dada (como \\(1e^{-10}\\)) nos ofrecer\u00e1 una corroboraci\u00f3n tajante donde comprobaremos c\u00f3mo, en efecto, el loop computar\u00e1 la mitad de las iteraciones.</li> <li>Micro-optimizaciones Matem\u00e1ticas en el Bloque RAM: La forma iterativa de Gauss-Seidel demuestra conceptualmente un principio de las ciencias computacionales aplicado tempranamente a los algoritmos continuos. La \"actualizaci\u00f3n inmediata\" o in-place updating (donde \\(x_2^{(k+1)}\\) se reutiliza inmediatamente sin demorarnos a la iteraci\u00f3n \\((k+1)\\) en el bloque en memoria) ahorra recursos del cach\u00e9 y virtualmente duplica la velocidad del procesamiento comparado con Jacobi, que exige retener en memoria secundaria una foto en fr\u00edo del vector \u00edntegro \\(X^{(k)}\\) del pasado.</li> </ul>"},{"location":"#lecciones-y-conclusiones-del-ejercicio-4","title":"Lecciones y Conclusiones del Ejercicio 4","text":"<ul> <li>Transformaci\u00f3n de Hip\u00f3stasis Exponenciales: Estudiamos c\u00f3mo la vasta mayor\u00eda de las regresiones emp\u00edricas no-lineales en la naturaleza se reducen, algor\u00edtmicamente, a simples regresiones lineales ordinarias que las computadoras (como <code>numpy.linalg</code>) pueden resolver al instante si aplicamos isomorfismos biyectivos. Bajar el exponente \\(z = a \\cdot y^b\\) v\u00eda logaritmos naturales independiza por la fuerza una funci\u00f3n intratable y nos la otorga en bandeja de plata como modelo param\u00e9trico \\(\\beta_0 + \\beta_1 X\\) compatible con la r\u00edgida Ecuaci\u00f3n Normal de M.C.O.</li> <li>Validaci\u00f3n del Determinante Experimental: En la programaci\u00f3n probabil\u00edstica de datos es usual arrojarle a la m\u00e1quina miles de registros esperando que encuentre promedios ponderados. Este ejercicio resalta el valor sem\u00e1ntico de la Independencia Lineal como pilar subyacente de la Computabilidad. Dise\u00f1ar un array de control min\u00fasculo adrede (de apenas 3 puntos de prueba) y observar que es materialmente el n\u00famero m\u00ednimo insalvable para que el algoritmo arroje un <code>LinAlgError</code> si no reparamos en la dependencia, permite trazar una raya visible entre un algoritmo \"que funciona de casualidad\" y un entendimiento total de las fronteras matem\u00e1ticas de las librer\u00edas estad\u00edsticas subyacentes.</li> </ul>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/","title":"Soluci\u00f3n del Ejercicio 1","text":"<p>Definici\u00f3n previa: Se dice que \\(A \\in \\mathbb{K}^{n \\times n}\\) es semejante a \\(B \\in \\mathbb{K}^{n \\times n}\\) si existe una matriz invertible \\(S \\in \\mathbb{K}^{n \\times n}\\) tal que:</p> <p>Pipo</p> <p>\\(S A S^{-1} = B\\)</p>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#1-demostrar-que-la-relacion-de-semejanza-es-una-relacion-de-equivalencia","title":"1. Demostrar que la relaci\u00f3n de semejanza es una relaci\u00f3n de equivalencia.","text":"<p>Para que una relaci\u00f3n binaria sea de equivalencia, esta debe cumplir tres propiedades fundamentales: reflexividad, simetr\u00eda y transitividad.</p> <p>A. Reflexividad (\\(A \\sim A\\)): Evaluamos si toda matriz cuadrada \\(A\\) es semejante a s\u00ed misma.  Consideremos la matriz identidad \\(I \\in \\mathbb{K}^{n \\times n}\\). Ya que \\(I\\) es invertible y su propia inversa es \\(I\\) (\\(I^{-1} = I\\)), tenemos: $\\(I A I^{-1} = I A I = A\\)$ Como existe al menos una matriz invertible (\\(I\\)) que satisface la igualdad, \\(A \\sim A\\) siempre se cumple.</p> <p>B. Simetr\u00eda (Si \\(A \\sim B\\), entonces \\(B \\sim A\\)): Partimos de la hip\u00f3tesis de que \\(A \\sim B\\). Por definici\u00f3n, existe una matriz invertible \\(S\\) tal que: $\\(S A S^{-1} = B\\)$ Si multiplicamos esta ecuaci\u00f3n por la izquierda por \\(S^{-1}\\) y por la derecha por \\(S\\), obtenemos: $\\(S^{-1} (S A S^{-1}) S = S^{-1} B S\\)$ Aplicando la propiedad asociativa y sabiendo que \\(S^{-1}S = I\\): $\\((S^{-1} S) A (S^{-1} S) = S^{-1} B S\\)$ $\\(I A I = S^{-1} B S \\implies A = S^{-1} B S\\)$ Definamos una nueva matriz \\(T = S^{-1}\\). Dado que \\(S\\) es invertible, su inversa \\(S^{-1}\\) tambi\u00e9n lo es (y su inversa es \\((S^{-1})^{-1} = S\\)). Reemplazando \\(S^{-1}\\) por \\(T\\) y \\(S\\) por \\(T^{-1}\\), la expresi\u00f3n nos queda: $\\(T B T^{-1} = A\\)$ Esto significa que \\(B\\) es semejante a \\(A\\), probando la simetr\u00eda.</p> <p>C. Transitividad (Si \\(A \\sim B\\) y \\(B \\sim C\\), entonces \\(A \\sim C\\)): De nuestras hip\u00f3tesis se concluye que existen matrices invertibles \\(S\\) y \\(P\\) tales que: 1) \\(S A S^{-1} = B\\) 2) \\(P B P^{-1} = C\\)</p> <p>Sustituyendo el valor de \\(B\\) de la primera ecuaci\u00f3n en la segunda, resulta: $\\(P (S A S^{-1}) P^{-1} = C\\)$ Rearrupando por asociatividad: $\\((P S) A (S^{-1} P^{-1}) = C\\)$ Sabemos por las propiedades de matrices invertibles que \\((P S)^{-1} = S^{-1} P^{-1}\\). Reemplazando este t\u00e9rmino: $\\((P S) A (P S)^{-1} = C\\)$ Si llamamos \\(U = P S\\), resultando en otra matriz que sabemos que es invertible porque el producto de dos invertibles lo es: $\\(U A U^{-1} = C\\)$ Esto certifica por definici\u00f3n que \\(A \\sim C\\), probando la transitividad.</p> <p>Al cumplirse las tres condiciones, la semejanza de matrices es efectivamente una relaci\u00f3n de equivalencia.</p>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#2-demostrar-que-si-a-es-semejante-a-b-entonces-texttra-texttrb","title":"2. Demostrar que si \\(A\\) es semejante a \\(B\\), entonces \\(\\text{Tr}(A) = \\text{Tr}(B)\\).","text":"<p>Si \\(A \\sim B\\), deducimos por definici\u00f3n que: $\\(B = S A S^{-1}\\)$ Calculamos la traza en ambos lados: $\\(\\text{Tr}(B) = \\text{Tr}(S A S^{-1})\\)$</p> <p>Aprovechando la sugerencia, usamos la propiedad c\u00edclica de la traza: \\(\\text{Tr}(E C) = \\text{Tr}(C E)\\).  Sea \\(E = S\\) y \\(C = (A S^{-1})\\). Sustituyendo en la propiedad de la traza: $\\(\\text{Tr}(S (A S^{-1})) = \\text{Tr}((A S^{-1}) S)\\)$ Reescribiendo aprovechando la asociatividad dentro de la traza: $\\(\\text{Tr}((A S^{-1}) S) = \\text{Tr}(A (S^{-1} S))\\)$ Como \\(S^{-1} S = I\\), tenemos: $\\(\\text{Tr}(A I) = \\text{Tr}(A)\\)$ Por lo tanto, concluimos que: $\\(\\text{Tr}(B) = \\text{Tr}(A)\\)$</p>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#3-probar-que-si-a-es-diagonalizable-con-valores-propios-0-y-1-entonces-a2-a","title":"3. Probar que si \\(A\\) es diagonalizable con valores propios 0 y 1, entonces \\(A^2 = A\\).","text":"<p>Si \\(A\\) es diagonalizable, entonces es semejante a una matriz diagonal \\(D\\). Por definici\u00f3n existe una matriz invertible \\(P\\) tal que: $\\(A = P^{-1} D P\\)$ (Nota: podemos invertir los roles de \\(P\\) y su inversa definiendo \\(S = P^{-1}\\), con lo cual la forma \\(S D S^{-1} = A\\) y \\(P A P^{-1} = D\\) son hom\u00f3logas a la definici\u00f3n del enunciado).</p> <p>En cualquier matriz diagonal \\(D\\), los elementos de su diagonal principal est\u00e1n formados precisamente por los valores propios de \\(A\\). Dado que nos aseguran que los valores propios son 0 y 1, el contenido de \\(D\\) solamente posee este par de d\u00edgitos. Cuando multiplicamos una matriz diagonal consigo misma (es decir, \\(D^2\\)), los elementos resultantes de su diagonal principal est\u00e1n simplemente elevados al cuadrado.  Como \\(0^2 = 0\\) y \\(1^2 = 1\\), los valores no se ven afectados y podemos asegurar categ\u00f3ricamente que: $\\(D^2 = D\\)$</p> <p>Vamos a la expresi\u00f3n para \\(A^2\\): $\\(A^2 = A \\cdot A = (P^{-1} D P) (P^{-1} D P)\\)$ Aplicamos asociatividad para multiplicar primero las matrices interiores: $\\(A^2 = P^{-1} D (P P^{-1}) D P\\)$ Sustituyendo \\(P P^{-1}\\) por la Identidad \\(I\\): $\\(A^2 = P^{-1} D I D P\\)$ $\\(A^2 = P^{-1} D^2 P\\)$ Dado la particularidad de nuestra matriz donde \\(D^2 = D\\), sustituimos y obtenemos: $\\(A^2 = P^{-1} D P\\)$ Como esto es la definici\u00f3n exacta de nuestra matriz original \\(A\\), queda probado que: $\\(A^2 = A\\)$</p>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":""},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/","title":"Soluci\u00f3n del Ejercicio 2","text":""},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/#1-calcular-la-descomposicion-en-valores-singulares-svd-de-la-matriz-a","title":"1. Calcular la descomposici\u00f3n en valores singulares (SVD) de la matriz \\(A\\)","text":"<p>Dada la matriz: $\\(A = \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix}\\)$</p> <p>Sabemos que la descomposici\u00f3n en valores singulares se estructura como \\(A = U \\Sigma V^T\\). Donde \\(\\Sigma\\) contiene los valores singulares (en la diagonal) que son las ra\u00edces cuadradas de los autovalores de \\(A^T A\\).</p> <p>Calculamos \\(A^T A\\): $\\(A^T A = \\begin{pmatrix} 0 &amp; 2 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} = \\begin{pmatrix} 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 9 \\end{pmatrix}\\)$</p> <p>Dado que \\(A^T A\\) es una matriz diagonal, sus autovalores son directamente los elementos de su diagonal: - \\(\\lambda_1 = 9 \\implies \\sigma_1 = 3\\) - \\(\\lambda_2 = 4 \\implies \\sigma_2 = 2\\) - \\(\\lambda_3 = 1 \\implies \\sigma_3 = 1\\)</p> <p>Por ende, nuestra matriz de valores singulares ordenados en forma decreciente es: $\\(\\Sigma = \\begin{pmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\)$</p> <p>Ahora, hallamos los autovectores ortonormales de \\(A^T A\\) asociados a estos autovalores para construir \\(V\\): - Para \\(\\lambda_1 = 9\\), el vector propio asociado a la tercera columna resulta ser \\(v_1 = (0, 0, 1)^T\\). - Para \\(\\lambda_2 = 4\\), el vector propio asociado a la primera columna resulta ser \\(v_2 = (1, 0, 0)^T\\). - Para \\(\\lambda_3 = 1\\), el vector propio asociado a la segunda columna resulta ser \\(v_3 = (0, 1, 0)^T\\).</p> <p>Concatenando dichos autovectores formamos \\(V\\), y por lo tanto \\(V^T\\): $\\(V = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\end{pmatrix} \\implies V^T = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}\\)$</p> <p>Para hallar \\(U = (\\vec{u}_1, \\vec{u}_2, \\vec{u}_3)\\), utilizamos la relaci\u00f3n \\(u_i = \\frac{1}{\\sigma_i} A v_i\\): - \\(u_1 = \\frac{1}{3} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 0 \\\\ 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\\) - \\(u_2 = \\frac{1}{2} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\) - \\(u_3 = \\frac{1}{1} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\)</p> <p>Reemplazando en \\(U\\): $\\(U = \\begin{pmatrix} 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\end{pmatrix}\\)$</p> <p>La Descomposici\u00f3n completa es finalmente: $\\(A = \\begin{pmatrix} 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}\\)$</p>"},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/#2-probar-que-pa-y-ap-tienen-los-mismos-valores-singulares-que-a","title":"2. Probar que \\(PA\\) y \\(AP\\) tienen los mismos valores singulares que \\(A\\)","text":"<p>Una matriz de permutaci\u00f3n \\(P\\) es siempre una matriz ortogonal; es decir, altera el orden de filas o columnas, pero respeta la isometr\u00eda: $\\(P^T P = P P^T = I\\)$</p> <p>A. Para \\(PA\\): Los valores singulares de \\(PA\\) son las ra\u00edces cuadradas de los autovalores de la matriz sim\u00e9trica \\((PA)^T (PA)\\). Sustituyendo y desarrollando: $\\((PA)^T (PA) = A^T P^T P A\\)$ Como \\(P^T P = I\\), esto simplifica a: $\\(A^T I A = A^T A\\)$ Dado que obtenemos exactamente el mismo n\u00facleo subyacente \\(A^T A\\), la matriz \\(PA\\) tiene estrictamente el mismo espectro de autovalores para dicha expresi\u00f3n, y por tanto, id\u00e9nticos valores singulares que \\(A\\).</p> <p>B. Para \\(AP\\): Buscamos los autovalores de \\((AP)^T (AP)\\): $\\((AP)^T (AP) = P^T A^T A P\\)$ Esta expresi\u00f3n equivale a que \\((AP)^T (AP)\\) y \\(A^T A\\) son matrices semejantes (aqu\u00ed interviene lo demostrado en el Ejercicio 1), ya que \\(P^T = P^{-1}\\). Recordemos adem\u00e1s, que las matrices semejantes preservan id\u00e9nticos autovalores.  Por consiguiente, los valores singulares producidos por \\(P^T A^T A P\\) ser\u00e1n los mismos que los de \\(A^T A\\), dejando en evidencia que \\(AP\\) posee iguales valores singulares a \\(A\\).</p> <p>C. Calcular \\(\\|PA\\|_2\\) y \\(\\kappa_2(PA)\\)</p> <p>Sabemos por sus propiedades fundamentales que: - La norma-2 de una matriz es igual a su mayor valor singular: \\(\\|M\\|_2 = \\sigma_{\\max}\\) - El n\u00famero de condici\u00f3n en base 2 equivale a la proporci\u00f3n de elongamiento l\u00edmite: \\(\\kappa_2(M) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\\)</p> <p>Al haber probado instantes atr\u00e1s que multiplicar por una permutaci\u00f3n no afecta los valores singulares en absoluto, usamos el espectro ya calculado \\(\\sigma \\in \\{3, 2, 1\\}\\):</p> \\[\\|PA\\|_2 = \\sigma_{\\max}(PA) = \\sigma_{\\max}(A) = 3\\] \\[\\kappa_2(PA) = \\frac{\\sigma_{\\max}(PA)}{\\sigma_{\\min}(PA)} = \\frac{3}{1} = 3\\]"},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":""},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/","title":"Soluci\u00f3n del Ejercicio 3","text":"<p>Dada la matriz del sistema: $\\(A = \\begin{pmatrix} 1 &amp; c &amp; 0 \\\\ 0 &amp; 1 &amp; c \\\\ 0 &amp; c &amp; 1 \\end{pmatrix}\\)$</p> <p>Podemos descomponer la matriz en su diagonal (\\(D\\)), su parte estrictamente inferior (\\(L\\)) y su parte estrictamente superior (\\(U\\)), de tal forma que \\(A = D + L + U\\): - \\(D = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} = I\\) - \\(L = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; c &amp; 0 \\end{pmatrix}\\) - \\(U = \\begin{pmatrix} 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; c \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\)</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#1-valores-de-c-para-los-cuales-convergen-jacobi-y-gauss-seidel","title":"1. Valores de \\(c\\) para los cuales convergen Jacobi y Gauss-Seidel","text":"<p>Ambos m\u00e9todos iterativos convergen para cualquier valor inicial si y solo si el radio espectral de sus matrices de iteraci\u00f3n (denotado \\(\\rho\\), que es el m\u00e1ximo valor absoluto de sus autovalores) es estrictamente menor a 1 (\\(\\rho &lt; 1\\)).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#metodo-de-jacobi","title":"M\u00e9todo de Jacobi","text":"<p>La matriz de iteraci\u00f3n de Jacobi est\u00e1 dada por: $\\(T_J = -D^{-1}(L + U) = -I (L + U) = \\begin{pmatrix} 0 &amp; -c &amp; 0 \\\\ 0 &amp; 0 &amp; -c \\\\ 0 &amp; -c &amp; 0 \\end{pmatrix}\\)$</p> <p>Calculamos sus autovalores encontrando el n\u00facleo del polinomio caracter\u00edstico \\(\\det(T_J - \\lambda I) = 0\\): $$ \\begin{vmatrix} -\\lambda &amp; -c &amp; 0 \\ 0 &amp; -\\lambda &amp; -c \\ 0 &amp; -c &amp; -\\lambda \\end{vmatrix} = 0 $$ Desarrollando el determinante evaluando por la primera columna: $$ -\\lambda \\begin{vmatrix} -\\lambda &amp; -c \\ -c &amp; -\\lambda \\end{vmatrix} = -\\lambda (\\lambda^2 - c^2) = 0 $$ Las ra\u00edces son: \\(\\lambda_1 = 0\\), \\(\\lambda_2 = c\\), \\(\\lambda_3 = -c\\). El radio espectral es \\(\\rho(T_J) = \\max(|0|, |c|, |-c|) = |c|\\).</p> <p>Por lo tanto, el m\u00e9todo de Jacobi converge si y solo si \\(|c| &lt; 1\\).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#metodo-de-gauss-seidel","title":"M\u00e9todo de Gauss-Seidel","text":"<p>La matriz de iteraci\u00f3n de Gauss-Seidel est\u00e1 dada por: $\\(T_{GS} = -(D + L)^{-1} U\\)$ Primero, calculamos \\((D + L)^{-1}\\): $\\(D + L = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; c &amp; 1 \\end{pmatrix} \\implies (D + L)^{-1} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -c &amp; 1 \\end{pmatrix}\\)$ Ahora hallamos \\(T_{GS}\\): $\\(T_{GS} = -\\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -c &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; c \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} = - \\begin{pmatrix} 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; c \\\\ 0 &amp; 0 &amp; -c^2 \\end{pmatrix} = \\begin{pmatrix} 0 &amp; -c &amp; 0 \\\\ 0 &amp; 0 &amp; -c \\\\ 0 &amp; 0 &amp; c^2 \\end{pmatrix}\\)$</p> <p>Al ser una matriz triangular superior, sus autovalores se desprenden directamente de la diagonal principal: \\(\\lambda_1 = 0\\), \\(\\lambda_2 = 0\\), \\(\\lambda_3 = c^2\\). El radio espectral es \\(\\rho(T_{GS}) = \\max(|0|, |0|, |c^2|) = c^2\\).</p> <p>Por lo tanto, el m\u00e9todo de Gauss-Seidel converge si y solo si \\(c^2 &lt; 1\\), lo cual ocurre puramente si \\(|c| &lt; 1\\).</p> <p>Ambos m\u00e9todos convergen en el mismo intervalo: \\(c \\in (-1, 1)\\).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#2-comparar-la-velocidad-de-convergencia","title":"2. Comparar la velocidad de convergencia","text":"<p>La tasa asint\u00f3tica de convergencia se define computacionalmente como \\(R(T) = -\\ln(\\rho(T))\\).  A mayor tasa de convergencia te\u00f3rica (\\(R\\)), se requieren en la pr\u00e1ctica menos iteraciones para converger con la computadora. - Para Jacobi: \\(R(T_J) = -\\ln(|c|)\\) - Para Gauss-Seidel: \\(R(T_{GS}) = -\\ln(c^2) = -2\\ln(|c|) = 2 R(T_J)\\)</p> <p>La relaci\u00f3n concluyente es que la tasa de convergencia de Gauss-Seidel es exactamente el doble. Por lo tanto, el m\u00e9todo de Gauss-Seidel converge el doble de r\u00e1pido que el m\u00e9todo de Jacobi. Esperamos que Gauss-Seidel demore anal\u00edticamente la mitad de iteraciones computacionales en confluir para cualquier valor de \\(|c| &lt; 1\\).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#3-plantear-las-iteraciones-correspondientes-para-cada-metodo","title":"3. Plantear las iteraciones correspondientes para cada m\u00e9todo","text":"<p>Para el sistema general \\(A \\vec{x} = \\vec{b}\\), es decir: $$ \\begin{cases}  x_1 + c x_2 = b_1 \\  x_2 + c x_3 = b_2 \\  c x_2 + x_3 = b_3  \\end{cases} $$</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#forma-iterativa-de-jacobi","title":"Forma iterativa de Jacobi:","text":"<p>El m\u00e9todo actualiza todas las variables en la iteraci\u00f3n \\((k+1)\\) en base exclusiva de los valores previos de la iteraci\u00f3n \\((k)\\): $\\(x_1^{(k+1)} = b_1 - c \\cdot x_2^{(k)}\\)$ $\\(x_2^{(k+1)} = b_2 - c \\cdot x_3^{(k)}\\)$ $\\(x_3^{(k+1)} = b_3 - c \\cdot x_2^{(k)}\\)$</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#forma-iterativa-de-gauss-seidel","title":"Forma iterativa de Gauss-Seidel:","text":"<p>Este m\u00e9todo utiliza los escalares m\u00e1s actualizados que encuentra (es decir, usa variables del estrato \\(k+1\\) tan pronto como hayan sido calculadas en cascada natural). $\\(x_1^{(k+1)} = b_1 - c \\cdot x_2^{(k)}\\)$ $\\(x_2^{(k+1)} = b_2 - c \\cdot x_3^{(k)}\\)$ $\\(x_3^{(k+1)} = b_3 - c \\cdot x_2^{(k+1)}\\)$ (N\u00f3tese que en esta \u00faltima l\u00ednea usa formalmente \\(x_2^{(k+1)}\\), que ya fue computado en el paso anterior).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":""},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/","title":"Soluci\u00f3n del Ejercicio 4","text":"<p>Dada la funci\u00f3n no lineal: $\\(z = a \\cdot y^b \\cdot e^{cx + 2}\\)$</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#1-plantear-las-ecuaciones-de-minimos-cuadrados-para-estimar-los-parametros-a-b-y-c","title":"1. Plantear las ecuaciones de m\u00ednimos cuadrados para estimar los par\u00e1metros \\(a, b\\) y \\(c\\).","text":"<p>Para aplicar el m\u00e9todo de M\u00ednimos Cuadrados Lineales Cl\u00e1sicos, primero debemos transformar (linealizar) el modelo geom\u00e9trico/exponencial aplicando logaritmo natural (\\(\\ln\\)) a ambos lados de la ecuaci\u00f3n: $\\(\\ln(z) = \\ln(a \\cdot y^b \\cdot e^{cx + 2})\\)$ Por las propiedades de los logaritmos (el logaritmo de un producto es la suma de los logaritmos, y el exponente baja multiplicando), la expresi\u00f3n queda: $\\(\\ln(z) = \\ln(a) + b \\ln(y) + (cx + 2)\\)$ Reagrupando los t\u00e9rminos para independizar las inc\u00f3gnitas de las constantes conocidas: $\\(\\ln(z) - 2 = \\ln(a) + b \\ln(y) + c x\\)$</p> <p>A partir de esta estructura lineal en sus par\u00e1metros, efectuamos los siguientes cambios de variable para llevarlo a un modelo lineal est\u00e1ndar de la forma \\(Z_i = \\beta_0 + \\beta_1 Y_i + \\beta_2 X_i\\): - \\(Z = \\ln(z) - 2\\) (variable dependiente transformada) - \\(A = \\ln(a)\\) (nuevo par\u00e1metro ordenado al origen, luego \\(a = e^A\\)) - El par\u00e1metro \\(b\\) queda libre. - El par\u00e1metro \\(c\\) queda libre. - La variable asociada a \\(b\\) es \\(\\ln(y)\\) - La variable asociada a \\(c\\) es \\(x\\)</p> <p>Para un conjunto de \\(m\\) puntos experimentales \\((x_i, y_i, z_i)\\), definimos el sistema de ecuaciones sobre-determinado en forma matricial \\(M \\vec{\\theta} = \\vec{Z}\\) como: $$ \\begin{pmatrix}  1 &amp; \\ln(y_1) &amp; x_1 \\ 1 &amp; \\ln(y_2) &amp; x_2 \\ \\vdots &amp; \\vdots &amp; \\vdots \\ 1 &amp; \\ln(y_m) &amp; x_m \\end{pmatrix} \\begin{pmatrix} A \\ b \\ c \\end{pmatrix} = \\begin{pmatrix} \\ln(z_1) - 2 \\ \\ln(z_2) - 2 \\ \\vdots \\ \\ln(z_m) - 2 \\end{pmatrix} $$</p> <p>Las ecuaciones normales de M\u00ednimos Cuadrados se construyen multiplicando por izquierda la transpuesta de la matriz de dise\u00f1o \\(M\\): $\\((M^T M) \\vec{\\theta} = M^T \\vec{Z}\\)$ Resolviendo este sistema lineal \\((3 \\times 3)\\) se obtienen los estimadores param\u00e9tricos \u00f3ptimos \\(\\hat{A}\\), \\(\\hat{b}\\) y \\(\\hat{c}\\). Posteriormente se recupera \\(\\hat{a} = e^{\\hat{A}}\\).</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#2-proponer-puntos-de-datos-para-que-la-solucion-sea-unica","title":"2. Proponer puntos de datos para que la soluci\u00f3n sea \u00fanica.","text":"<p>Para que las ecuaciones de m\u00ednimos cuadrados posean una soluci\u00f3n \u00fanica, la matriz normal cuadrada \\((M^T M)\\) debe ser estrictamente invertible. Esto ocurre si y solo si la matriz de dise\u00f1o \\(M\\) posee rango completo por columnas. Como \\(M\\) tiene 3 columnas, necesitamos que el \\(\\text{Rango}(M) = 3\\). Geom\u00e9tricamente, los vectores columna de la matriz \\(M\\) formados por las mediciones no deben ser dependientes entre s\u00ed.</p> <p>En el contexto f\u00edsico del problema, esto implica que: - Los puntos de los datos \\((x_i, y_i)\\) no deben formar una combinaci\u00f3n lineal perfecta. No vale que para todas las mediciones sea siempre \\(x_i = k \\cdot \\ln(y_i) + C\\) (es decir, no pueden ser colineales en el plano de las caracter\u00edsticas transformadas). - La variable \\(y_i\\) debe ser estrictamente positiva (\\(y_i &gt; 0\\)) para todo \\(i\\), dado que el dominio natural del \\(\\ln(y_i)\\) no admite valores negativos ni ceros.</p> <p>Propuesta de puntos experimentalmente v\u00e1lidos y robustos: (Para garantizar que no sean colineales ni constantes, basta con alterar alternativamente las magnitudes en los ejes): - \\(P_1 = (x_1=1,\\, y_1=1,\\, z_1)\\) - \\(P_2 = (x_2=0,\\, y_2=e,\\, z_2)\\) - \\(P_3 = (x_3=-1,\\, y_3=1,\\, z_3)\\)</p> <p>Evaluemos c\u00f3mo queda nuestra matriz de muestras con estos puntos de prueba para confirmar su independencia: $\\(M_{\\text{propuesta}} = \\begin{pmatrix} 1 &amp; \\ln(1) &amp; 1 \\\\ 1 &amp; \\ln(e) &amp; 0 \\\\ 1 &amp; \\ln(1) &amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\end{pmatrix}\\)$ El determinante de esta matriz \\(3 \\times 3\\) no es cero (en efecto, vale \\(-2\\)): \\(\\det(M_{\\text{propuesta}}) = 1 \\cdot (-1 - 0) - 0 \\cdot (-1 -0) + 1 \\cdot (0 - 1) = -1 - 1 = -2\\). Por tanto, el determinante de la matriz normal \\(\\det(M^T M) = \\det(M)^2 = 4 \\neq 0\\). El rango es completo, y la soluci\u00f3n de los par\u00e1metros est\u00e1 garantizada demostr\u00e1blemente \u00fanica.</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#3-determinar-la-minima-cantidad-de-puntos-necesarios-para-que-la-solucion-sea-unica","title":"3. Determinar la m\u00ednima cantidad de puntos necesarios para que la soluci\u00f3n sea \u00fanica.","text":"<p>El sistema general de m\u00ednimos cuadrados tiene como inc\u00f3gnita el vector \\(\\vec{\\theta} = [A, b, c]^T\\), el cual contiene 3 par\u00e1metros a estimar libremente.</p> <p>Por el Teorema de Rouch\u00e9-Frobenius y el Rango fundamental del \u00e1lgebra matricial: - Si aportamos \\(m &lt; 3\\) puntos, el sistema quedar\u00e1 sub-determinado (tendr\u00e1 infinitas soluciones l\u00f3gicas porque habr\u00e1n variables libres, el rango m\u00e1ximo ser\u00e1 menor a 3 penalizando a \\(M^T M\\)). - Para que la matriz de dise\u00f1o \\(M \\in \\mathbb{R}^{m \\times 3}\\) logre poseer un Rango por Columnas exactamente igual a 3 (condici\u00f3n forzosa e innegociable para que la inversa de \\(M^T M\\) exista), necesitamos aportar un m\u00ednimo estricto de \\(m = 3\\) puntos.</p> <p>Conclusi\u00f3n: Se necesitan como m\u00ednimo 3 puntos emp\u00edricos (siempre y cuando estos no formen un subespacio degenerado de dimensiones menores, seg\u00fan lo exigido en el inciso 2).</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":""}]}