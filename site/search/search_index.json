{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Meta Documento: Framework y Metodolog\u00eda de Resoluci\u00f3n","text":"<p>Este documento busca funcionar como un \"blueprint\" o plantilla metodol\u00f3gica. A partir del ejercicio de \u00e1lgebra lineal resuelto (propiedades de la semejanza de matrices), hemos abstra\u00eddo una arquitectura operativa pensada para abordar futuras tareas similares que involucren una resoluci\u00f3n matem\u00e1tica seguida de una verificaci\u00f3n y validaci\u00f3n con programaci\u00f3n.</p>"},{"location":"#arquitectura-del-flujo-de-trabajo","title":"\ud83c\udfd7 Arquitectura del Flujo de Trabajo","text":"<p>El flujo de trabajo unificado se compone de tres pilares, ejecutados habitualmente en esta misma cronolog\u00eda:</p>"},{"location":"#1-fase-pura-razonamiento-teorico-el-que-y-el-por-que","title":"1. Fase Pura / Razonamiento Te\u00f3rico (El \"Qu\u00e9\" y el \"Por Qu\u00e9\")","text":"<p>Antes de cualquier l\u00ednea de c\u00f3digo, el problema es comprendido y deconstruido:</p> <ul> <li> <p>Identificaci\u00f3n de Definiciones: Entender sem\u00e1nticamente qu\u00e9 significan las f\u00f3rmulas. Ejemplo: \"\\(S A S^{-1} = B\\) representa el cambio de base, lo que implica que \\(A\\) y \\(B\\) manejan la misma transformaci\u00f3n original.\"</p> </li> <li> <p>Descomposici\u00f3n T\u00e1ctica: Separar los problemas en componentes digeribles. \"Comprobar equivalencia\" no es un \u00fanico bloque, sino un checklist de (Reflexividad, Simetr\u00eda, Transitividad).</p> </li> <li> <p>Desarrollo Anal\u00edtico: Plasmar el formalismo matem\u00e1tico de modo detallado, donde cada paso es l\u00f3gicamente deducible desde el anterior (usando las hip\u00f3tesis para destrabar el desarrollo de la tesis).</p> </li> </ul>"},{"location":"#2-fase-de-traduccion-el-puente-logico-computacional","title":"2. Fase de Traducci\u00f3n (El Puente L\u00f3gico-Computacional)","text":"<p>Un teorema puede ser herm\u00e9tico y ajeno al c\u00f3digo, por lo que demanda una \"traducci\u00f3n\":</p> <ul> <li> <p>Abstracci\u00f3n a Modelos Estoc\u00e1sticos: Dado que no podemos corroborar el infinito, probamos con aleatoriedad (\"Randomization Testing\"). Es decir, si el teorema es universal, se debe sostener al alimentar las f\u00f3rmulas con matrices (arreglos n-dimensionales) llenos del espectro continuo flotante (e.g., generadas con elementos desde <code>NumPy</code>). </p> </li> <li> <p>Adaptaci\u00f3n de Restricciones: Traducir consideraciones te\u00f3ricas (\"\\(S\\) debe ser invertible\") a instrucciones para la m\u00e1quina (e.g. validaciones contra el determinante distinto de cero en un ciclo <code>While</code> generador).</p> </li> </ul>"},{"location":"#3-fase-pragmatica-verificacion-empirica-el-sandbox","title":"3. Fase Pragm\u00e1tica / Verificaci\u00f3n Emp\u00edrica (El \"Sandbox\")","text":"<p>Se codifica el programa verificador que pondr\u00e1 a prueba el desarrollo anal\u00edtico:</p> <ul> <li> <p>Ejecuci\u00f3n y Comprobaci\u00f3n Booleana (Validaciones en Punto Flotante): En computaci\u00f3n cient\u00edfica, las afirmaciones como \"A la matriz original\" o \"Ambas Trazas miden igual\" raramente deben validarse con <code>==</code> (por problemas de redondeo/convergencia de hardware en el tipo float). Se usan metodolog\u00edas como <code>np.isclose()</code> o <code>np.allclose()</code> tolerando peque\u00f1os m\u00e1rgenes estad\u00edsticos de error computacional (\\(\\approx 1e^{-8}\\)).</p> </li> <li> <p>Depuraci\u00f3n Bidireccional: Si el Test emp\u00edrico falla, esto dispara alarmas. Nos obliga a revisar o el c\u00f3digo de comprobaci\u00f3n (si hay problemas de implementaci\u00f3n), o bien encontrar falacias ocultas dentro de nuestra rigurosa prueba te\u00f3rica en la Fase 1.</p> </li> </ul>"},{"location":"#documentacion-de-conclusiones","title":"\ud83d\udccc Documentaci\u00f3n de Conclusiones","text":"<p>Este h\u00edbrido de demostraci\u00f3n anal\u00edtica-matem\u00e1tica seguida de una prueba automatizada en un vector de c\u00e1lculo eficiente (como Python) resulta el paradigma en el estado del arte de la investigaci\u00f3n y aprendizaje. Genera lo que en l\u00f3gica se llama Confianza Incondicional: Si existe certeza sem\u00e1ntica en papel, y el procesador no halla contradicciones luego de ser testeado con caos num\u00e9rico aleatorio, la tarea fue resuelta con el m\u00e1ximo rigor posible.</p>"},{"location":"lecciones_aprendidas/","title":"Lecciones y Conclusiones Aprendidas","text":"<p>A trav\u00e9s de las validaciones te\u00f3rico-pr\u00e1cticas elaboradas en nuestra metodolog\u00eda de estudio, derivamos las siguientes conclusiones anal\u00edticas.</p>"},{"location":"lecciones_aprendidas/#examen-21-de-jul-de-2025","title":"Examen 21 de jul de 2025","text":""},{"location":"lecciones_aprendidas/#ejercicio-1-svd","title":"Ejercicio 1 - SVD","text":"<ul> <li> <p>Acotaci\u00f3n de Errores con Valores Singulares: Demostramos te\u00f3ricamente y emp\u00edricamente c\u00f3mo las aproximaciones de rango inferior truncando la SVD acotan su error m\u00e1ximo en norma Eucl\u00eddea por el valor del siguiente valor singular omitido (\\(\\sigma_2\\)). Esto reafirma la contundente utilidad de SVD en compresi\u00f3n matem\u00e1tica de datos con p\u00e9rdidas r\u00edgidamente controladas.</p> </li> <li> <p>Convergencia Pr\u00e1ctica del M\u00e9todo de la Potencia: Demostramos mediante \u00e1lgebra c\u00f3mo iterar estoc\u00e1sticamente \\(x^{(k+1)} = \\frac{Bx^{(k)}}{||Bx^{(k)}||}\\) alinea r\u00e1pidamente el vector a la componente principal dominante purgando a las dem\u00e1s bases por diferencias en el ratio de sus autovalores \\(\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k \\to 0\\). Esto nos permiti\u00f3 desarrollar una rutina de aproximaci\u00f3n de rango 1 que iguala anal\u00edticamente en su resultado a las librer\u00edas industriales complejas.</p> </li> <li> <p>Invarianza Direccional bajo Normalizaci\u00f3n Iterativa: Se comprob\u00f3 anal\u00edticamente por inducci\u00f3n y se verific\u00f3 emp\u00edricamente en Python que la re-normalizaci\u00f3n continua al aplicar el M\u00e9todo de la Potencia no distorsiona el sentido del vector iterativo. Computar escalares divisores paso a paso \u00fanicamente ajusta la magnitud subyacente manteniendo la iteraci\u00f3n estable, pero el resultado sigue colapsando irremediablemente hacia la misma direcci\u00f3n espacial dictada por el marco te\u00f3rico r\u00edgido de calcular directamente \\(\\frac{B^k x^{(0)}}{||B^k x^{(0)}||}\\).</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-2-diagonalizacion","title":"Ejercicio 2 - Diagonalizaci\u00f3n","text":"<ul> <li> <p>Independencia en Espectros Discretos: Corroboramos en papel y m\u00e1quina c\u00f3mo una matriz con valores intr\u00ednsecos no repetidos despliega ineludiblemente un abanico hiper-dimensional completo de autovectores que componen una base lineal sin vac\u00edos formales dentro de \\(\\mathbb{R}^n\\). Toda matriz estoc\u00e1stica generada artificialmente con auto-valores dispares presentar\u00e1 por ley matem\u00e1tica rango pleno en su subespacio.</p> </li> <li> <p>Isomorfismo de la Matriz Semejante C: Entender la ecuaci\u00f3n universal \\(AC = CS\\) elimina en su completitud el \"misterio\" de la diagonalizaci\u00f3n abstracta. Al desgranar paso por paso que la multiplicaci\u00f3n por bloques y extraer las variables demostramos c\u00f3mo esta transformaci\u00f3n de autovectores unificada como matriz sencilla enmascara el escalado puro espectral dentro de la base general del sistema.</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-3-matrices-ortogonales-y-simetricas","title":"Ejercicio 3 - Matrices Ortogonales y Sim\u00e9tricas","text":"<ul> <li> <p>Teorema de la Traza en Espectros de Matrices Involutivas: Ante una matriz de dimensi\u00f3n superior, si el marco te\u00f3rico asegura (debido a su involuci\u00f3n \\(A^2 = I\\)) que el espectro est\u00e1 condenado a valer \\(1\\) o \\(-1\\), la multiplicidad espectral decanta algebraicamente en un sistema de ecuaciones simplificado aplicando la traza. En el c\u00f3digo dedujimos, y Python constat\u00f3, c\u00f3mo \\(Tr(B) = 2\\) es una impronta en la materia que denota implacablemente que \\(k \\cdot 1 + m \\cdot (-1) = 2\\).</p> </li> <li> <p>Transparencia Espectral frente a Factorizaci\u00f3n SVD: Asimilamos conceptualmente que las matrices unitarias / ortogonales puras como \\(A\\) carecen de re-escalado intr\u00ednseco. Como en su factorizaci\u00f3n sus valores singulares \\(\\sigma\\) no son otra cosa que la ra\u00edz de \\(A^t A\\) y \\(A^t A = I\\), toda componente SVD \\(\\Sigma\\) de una matriz ortogonal se colapsa en esencia a la pura Matriz Identidad.</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-4-cuadrados-minimos-y-aproximaciones","title":"Ejercicio 4 - Cuadrados M\u00ednimos y Aproximaciones","text":"<ul> <li> <p>Transparencia Hiperespacial Ortogonal: Cuando se trabaja con un conjunto regido expletamente por Bases Ortonormales en \\(\\mathbb{R}^n\\), plantear la minimizaci\u00f3n expl\u00edcita sobre un sub-hiperplano se destila en proyectar can\u00f3nicamente cada componente de modo independiente. Como la matriz gramiana \\(A^t A\\) es de per se la identidad, todo el colosal artilugio del modelo matricial MCO se simplifica al mero filtrado algor\u00edtmico de productos internos.</p> </li> <li> <p>Teorema de Pit\u00e1goras Multidimensional en Residuos: Constatamos que la norma de un vector residual originado al truncarse la proyecci\u00f3n ortogonal, como aquel error \\(e = (b - p) = \\sum_{k=n+1}^{m} x_k q_k\\) es un c\u00e1lculo exacto. El residuo general de la aproximaci\u00f3n es puramente la suma pitag\u00f3rica del m\u00f3dulo individual esgrimido por las componentes est\u00e1ticas que quedaron ajenas al plano \\(A\\).</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-5-desigualdad-de-cauchy-schwartz","title":"Ejercicio 5 - Desigualdad de Cauchy-Schwartz","text":"<ul> <li> <p>Deducci\u00f3n a partir de Geometr\u00eda Residual: Constatamos c\u00f3mo la inquebrantable desigualdad lineal euclidiana emerge directamente del hecho f\u00edsico de que los errores de proyecci\u00f3n de M\u00ednimos Cuadrados arrastran por definici\u00f3n una norma estrictamente positiva (\\(||e||^2 \\ge 0\\)). Al aislar las magnitudes producto de dichas ecuaciones residuales y conjugarlas, el teorema sale a flote espont\u00e1neamente.</p> </li> <li> <p>Universalidad de Vectores Complejos: El c\u00f3digo en iteraci\u00f3n aleatoria y masiva confirm\u00f3 que la matriz Hermitiana transpuesta conjugada (\\(x^*\\)) absorbe id\u00e9nticamente el mismo marco te\u00f3rico que el dot-product simple en \\(\\mathbb{R}^n\\), probando que Cauchy-Schwartz es una ley del \u00c1lgebra Lineal que trasciende sin perturbarse hacia todo el abanico del espectro complejo (\\(\\mathbb{C}^n\\)).</p> </li> </ul>"},{"location":"lecciones_aprendidas/#examen-24-de-feb-de-2025","title":"Examen 24 de feb de 2025","text":""},{"location":"lecciones_aprendidas/#ejercicio-1-semejanza-de-matrices","title":"Ejercicio 1 - Semejanza de Matrices","text":"<ul> <li> <p>Conservaci\u00f3n Nuclear bajo Cambio de Base: Demostramos computacionalmente que la relaci\u00f3n de semejanza es de equivalencia, operando en Python y verificando que deshacer la transformaci\u00f3n (\\(S^{-1} B S\\)) re-ensambla milim\u00e9tricamente nuestra matriz \\(A\\), comprobando que si \\(A \\sim B\\) ambas enmascaran el mismo sistema subyacente observado desde perspectivas oblicuas diferentes.</p> </li> <li> <p>Invarianza de la Traza y Operaciones C\u00edclicas: Validamos probando estad\u00edsticamente en el c\u00f3digo frente a matrices aleatorias densas que, por consecuencia directa de la regla rastro-c\u00edclica (\\(Tr(EC) = Tr(CE)\\)), deformar un sistema lineal de ecuaciones por pre-y-post multiplicaci\u00f3n con una matriz base \\(S\\) no deforma jam\u00e1s la suma geom\u00e9trica de sus espectros primarios (su traza diagonal se preserva en \\(B\\)).</p> </li> <li> <p>Idempotencia Proyectiva Plena (\\(A^2 = A\\)): Corroboramos en papel y computadora un axioma de la geometr\u00eda lineal: si todos los infinitos valores del espectro propio para dimensiones \\(\\mathbb{R}^n\\) se reducen dogm\u00e1ticamente a los booleanos puros \\(\\{0, 1\\}\\), entonces no existe re-escalado din\u00e1mico que el operador introduzca en la topolog\u00eda asintota. El espacio degenera en una Matriz Idempotente que funciona como pura Proyecci\u00f3n Est\u00e1tica inamovible.</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-2-semejanza-y-svd","title":"Ejercicio 2 - Semejanza y SVD","text":"<ul> <li> <p>Uso de Invarianzas y Transformaciones Ortogonales: En matem\u00e1ticas (como ocurre con la SVD), aplicar operaciones \"isom\u00e9tricas\" o transformaciones ortogonales (como fue multiplicar por una matriz de permutaci\u00f3n aleatoria computacionalmente, \\(P\\)) resulta invariante para las magnitudes nucleares (como el espectro singular). </p> </li> <li> <p>La Utilidad de la Permutaci\u00f3n Aleatoria Computada: A la hora de verificar propiedades sobre operadores donde \"El orden de las filas no altera el resultado estructural\", utilizar una matriz de permutaci\u00f3n estoc\u00e1stica (<code>P = I[np.random.permutation(n), :]</code>) sobre el c\u00f3digo es un factor de prueba estupendo. Si la propiedad estad\u00edstica persiste (ejemplo, la invariabilidad de <code>np.linalg.svd</code>) probamos emp\u00edricamente la independencia matem\u00e1tica del operador evaluado y confirmamos el modelo num\u00e9rico.</p> </li> <li> <p>Estabilidad de las Normas (Norma-2): El script nos demostr\u00f3 c\u00f3mo este tipo de transformaciones no introducen ruido algor\u00edtmico al \"estiramiento\" m\u00e1ximo de la matriz (la Norma 2) ni a su n\u00famero de condici\u00f3n. Computarizar \\(\\|PA\\|_2\\) arroj\u00f3 consistentemente el mismo resultado de norma debido a que los ortogonales preservan las longitudes vectoriales subyacentes, validando lo deducido con l\u00e1piz y papel.</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-3-metodos-iterativos","title":"Ejercicio 3 - M\u00e9todos Iterativos","text":"<ul> <li> <p>Del Radio Espectral al C\u00f3digo Emp\u00edrico: Observamos c\u00f3mo la teor\u00eda matricial predice ex\u00e1ctamente el comportamiento iterativo del <code>while-loop</code>. A diferencia del algebra matricial anal\u00edtica, el c\u00e1lculo num\u00e9rico involucra medir tiempos (tasas) de procesamiento. Si el an\u00e1lisis formal sostiene que la tasa de convergencia asint\u00f3tica de una t\u00e9cnica es el doble de la que presenta otra (\\(\\rho(T_{GS}) = \\rho(T_J)^2\\)), computarizar un simple contador de los pasos que toma domar un residuo a una tolerancia l\u00edmite dada (como \\(1e^{-10}\\)) nos ofrecer\u00e1 una corroboraci\u00f3n tajante donde comprobaremos c\u00f3mo, en efecto, el loop computar\u00e1 la mitad de las iteraciones.</p> </li> <li> <p>Micro-optimizaciones Matem\u00e1ticas en el Bloque RAM: La forma iterativa de Gauss-Seidel demuestra conceptualmente un principio de las ciencias computacionales aplicado tempranamente a los algoritmos continuos. La \"actualizaci\u00f3n inmediata\" o in-place updating (donde \\(x_2^{(k+1)}\\) se reutiliza inmediatamente sin demorarnos a la iteraci\u00f3n \\((k+1)\\) en el bloque en memoria) ahorra recursos del cach\u00e9 y virtualmente duplica la velocidad del procesamiento comparado con Jacobi, que exige retener en memoria secundaria una foto en fr\u00edo del vector \u00edntegro \\(X^{(k)}\\) del pasado.</p> </li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-4-minimos-cuadrados","title":"Ejercicio 4 - M\u00ednimos Cuadrados","text":"<ul> <li> <p>Transformaci\u00f3n de Hip\u00f3stasis Exponenciales: Estudiamos c\u00f3mo la vasta mayor\u00eda de las regresiones emp\u00edricas no-lineales en la naturaleza se reducen, algor\u00edtmicamente, a simples regresiones lineales ordinarias que las computadoras (como <code>numpy.linalg</code>) pueden resolver al instante si aplicamos isomorfismos biyectivos. Bajar el exponente \\(z = a \\cdot y^b\\) v\u00eda logaritmos naturales independiza por la fuerza una funci\u00f3n intratable y nos la otorga en bandeja de plata como modelo param\u00e9trico \\(\\beta_0 + \\beta_1 X\\) compatible con la r\u00edgida Ecuaci\u00f3n Normal de M.C.O.</p> </li> <li> <p>Validaci\u00f3n del Determinante Experimental: En la programaci\u00f3n probabil\u00edstica de datos es usual arrojarle a la m\u00e1quina miles de registros esperando que encuentre promedios ponderados. Este ejercicio resalta el valor sem\u00e1ntico de la Independencia Lineal como pilar subyacente de la Computabilidad. Dise\u00f1ar un array de control min\u00fasculo adrede (de apenas 3 puntos de prueba) y observar que es materialmente el n\u00famero m\u00ednimo insalvable para que el algoritmo arroje un <code>LinAlgError</code> si no reparamos en la dependencia, permite trazar una raya visible entre un algoritmo \"que funciona de casualidad\" y un entendimiento total de las fronteras matem\u00e1ticas de las librer\u00edas estad\u00edsticas subyacentes.</p> </li> </ul>"},{"location":"lecciones_aprendidas/#examen-07-de-ago-de-2025","title":"Examen 07 de ago de 2025","text":""},{"location":"lecciones_aprendidas/#ejercicio-1-proyectores-oblicuos-vs-ortogonales","title":"Ejercicio 1 - Proyectores Oblicuos vs Ortogonales","text":"<ul> <li>Geometr\u00eda de la Intersecci\u00f3n Nula: Validamos computacionalmente que construir un proyector gen\u00e9rico \\(P\\) impone armar su base aglomerando \\(Im(P) \\oplus Nu(P)\\). El c\u00e1lculo del determinante de esta matriz base ensamblada es la prueba irrefutable de que dicha suma directa conforma a todo \\(\\mathbb{R}^n\\), un paso ineludible para garantizar la viabilidad del proyector.</li> <li>Asimetr\u00eda de la Oblicuidad: Contrastamos en c\u00f3digo que si bien \\(P^2 = P\\) dictamina la existencia fundamental del proyector abstracto, la falta de simetr\u00eda estructural \\(P \\neq P^T\\) es el detonante m\u00e9trico exclusivo que lo empuja a ser oblicuo en vez de ortogonal puro, sesgando asim\u00e9tricamente los vectores de entrada al colapsarlos.</li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-2-cadenas-ciclicas-de-markov","title":"Ejercicio 2 - Cadenas C\u00edclicas de Markov","text":"<ul> <li>Frontera Determin\u00edstica del C\u00edrculo Unidad: Corroboramos en papel y Python que las sub-matrices estoc\u00e1sticas cuyas rutas arman \"ciclos herm\u00e9ticos\" no-absorbentes inyectan ineludiblemente ra\u00edces \\(k\\)-\u00e9simas complejas al espectro propio de la matriz global. Esto demuestra formalmente que las cadenas de Markov con estados rotacionales peri\u00f3dicos alojan infinitos autovalores de m\u00f3dulo \\(| \\lambda | = 1\\), evitando el decaimiento asint\u00f3tico en esas sub-secciones.</li> <li>Absorci\u00f3n y Estado Estacionario Unificado: Experimentamos con un modelo fuertemente conexo final, verificando probabil\u00edsticamente que sin importar el vector de estado entrante \\(v_0\\) o su ruido inicial aleatorio, la \u00fanica clase recurrente y aperi\u00f3dica traga absolutamente la masa completa del sistema hacia un inquebrantable autovector \\(\\lambda = 1\\), confirmando el Teorema Fundamental asint\u00f3tico de las Matrices Estoc\u00e1sticas.</li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-3-abstraccion-de-la-pseudoinversa","title":"Ejercicio 3 - Abstracci\u00f3n de la Pseudoinversa","text":"<ul> <li>Estabilidad frente a las Ecuaciones Normales: Verificamos al contrastar f\u00f3rmulas y simulaciones que la Pseudoinversa de Moore-Penrose \\(A^\\dagger\\) no es simplemente una triqui\u00f1uela notacional para ocultar la espantosa matriz Gramiana \\((A^T A)^{-1} A^T\\), sino el verdadero salvavidas algor\u00edtmico subyacente de <code>lstsq</code>, ya que a trav\u00e9s de factorizaciones iteradas en la m\u00e1quina logra saltearse el catastr\u00f3fico acto de potenciar al cuadrado el de por s\u00ed fr\u00e1gil n\u00famero de condici\u00f3n de \\(A\\).</li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-4-sensibilidad-parametrica-y-jacobi","title":"Ejercicio 4 - Sensibilidad Param\u00e9trica y Jacobi","text":"<ul> <li>Amplificaci\u00f3n de Escala no-Lineal: Calculamos manualmente cotas de perturbaci\u00f3n generalizadas que evidenciaron de forma indiscutible c\u00f3mo incorporar variables hiperb\u00f3licas (ej: \\(k^2\\)) en elementos cr\u00edticos de un operador colapsa el condicionamiento general a valores gigantescos \\(\\mathcal{O}(k^2)\\), destrozando de origen cualquier confiabilidad num\u00e9rica.</li> <li>Milagro del Precondicionador Constante: Sorprendimos al condicionamiento aplicando un mitigador de Jacobi matriz-diagonal elemental, cuya \u00fanica ronda lateral inyectada transmut\u00f3 un sistema que tend\u00eda a estallar con \\(k \\to \\infty\\) en uno inofensivo con cota \\(\\approx 6.85\\) eternamente r\u00edgido e impasible. Esto confirma el enorme poder industrial del precondicionamiento previo a la iteraci\u00f3n en gran escala algor\u00edtmica.</li> </ul>"},{"location":"lecciones_aprendidas/#ejercicio-5-lu-y-la-fragilidad-de-gauss","title":"Ejercicio 5 - LU y la Fragilidad de Gauss","text":"<ul> <li>El Concepto de Multiplicador Intacto: Descubrimos conceptualizando la funci\u00f3n algor\u00edtmica <code>LU</code> y los pasos manuales c\u00f3mo el coraz\u00f3n matricial colapsa instant\u00e1neamente en un ZeroDivisionError fatal si cualquier \\(A^{(k)}_{k k}\\) se asoma al nivel nulo exacto en medio del viaje al fondo. El pivoteo (permutaci\u00f3n) no es mero orden visual en una computadora, sino el pilar matem\u00e1tico que dictamina si \\(A\\) admite la factorizaci\u00f3n LU pura, o requiere imperiosamente reensamblarse como una ex\u00f3tica \\(PA=LU\\).</li> </ul>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/","title":"Soluci\u00f3n del Ejercicio 1","text":"<p>Ejercicio 1</p> <p>Se dice que \\(A \\in \\mathbb{K}^{n \\times n}\\) es semejante a \\(B \\in \\mathbb{K}^{n \\times n}\\) si existe una matriz invertible \\(S \\in \\mathbb{K}^{n \\times n}\\) tal que:</p> \\[SA(S^{-1}) = B\\] <ol> <li> <p>Demostrar que la relaci\u00f3n de semejanza es una relaci\u00f3n de equivalencia.</p> </li> <li> <p>Demostrar que si \\(A\\) es semejante a \\(B\\), entonces:</p> </li> </ol> <p>$\\(\\text{Tr}(A) = \\text{Tr}(B)\\)$</p> <p>Sugerencia: Utilizar la propiedad \\(\\text{Tr}(EC) = \\text{Tr}(CE)\\) para matrices \\(C\\) y \\(E\\).</p> <ol> <li>Probar que si \\(A\\) es diagonalizable (es decir, \\(A\\) es semejante a una matriz diagonal \\(D\\)) y los valores propios de \\(A\\) son 0 y 1, entonces:</li> </ol> <p>$\\(A^2 = A\\)$</p>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#1-demostrar-que-la-relacion-de-semejanza-es-una-relacion-de-equivalencia","title":"1. Demostrar que la relaci\u00f3n de semejanza es una relaci\u00f3n de equivalencia.","text":"<ol> <li>Demostrar que la relaci\u00f3n de semejanza es una relaci\u00f3n de equivalencia.</li> </ol> <p>Para que una relaci\u00f3n binaria sea de equivalencia, esta debe cumplir tres propiedades fundamentales: reflexividad, simetr\u00eda y transitividad.</p> <p>A. Reflexividad (\\(A \\sim A\\)):</p> <p>Evaluamos si toda matriz cuadrada \\(A\\) es semejante a s\u00ed misma. </p> <p>Consideremos la matriz identidad \\(I \\in \\mathbb{K}^{n \\times n}\\). Ya que \\(I\\) es invertible y su propia inversa es \\(I\\) (\\(I^{-1} = I\\)), tenemos:</p> \\[I A I^{-1} = I A I = A\\] <p>Como existe al menos una matriz invertible (\\(I\\)) que satisface la igualdad, \\(A \\sim A\\) siempre se cumple.</p> <p>B. Simetr\u00eda (Si \\(A \\sim B\\), entonces \\(B \\sim A\\)):</p> <p>Partimos de la hip\u00f3tesis de que \\(A \\sim B\\). Por definici\u00f3n, existe una matriz invertible \\(S\\) tal que:</p> \\[S A S^{-1} = B\\] <p>Si multiplicamos esta ecuaci\u00f3n por la izquierda por \\(S^{-1}\\) y por la derecha por \\(S\\), obtenemos:</p> \\[S^{-1} (S A S^{-1}) S = S^{-1} B S\\] <p>Aplicando la propiedad asociativa y sabiendo que \\(S^{-1}S = I\\):</p> \\[(S^{-1} S) A (S^{-1} S) = S^{-1} B S\\] \\[I A I = S^{-1} B S \\implies A = S^{-1} B S\\] <p>Definamos una nueva matriz \\(T = S^{-1}\\). Dado que \\(S\\) es invertible, su inversa \\(S^{-1}\\) tambi\u00e9n lo es (y su inversa es \\((S^{-1})^{-1} = S\\)). Reemplazando \\(S^{-1}\\) por \\(T\\) y \\(S\\) por \\(T^{-1}\\), la expresi\u00f3n nos queda:</p> \\[T B T^{-1} = A\\] <p>Esto significa que \\(B\\) es semejante a \\(A\\), probando la simetr\u00eda.</p> <p>C. Transitividad (Si \\(A \\sim B\\) y \\(B \\sim C\\), entonces \\(A \\sim C\\)):</p> <p>De nuestras hip\u00f3tesis se concluye que existen matrices invertibles \\(S\\) y \\(P\\) tales que:</p> <p>1) \\(S A S^{-1} = B\\)</p> <p>2) \\(P B P^{-1} = C\\)</p> <p>Sustituyendo el valor de \\(B\\) de la primera ecuaci\u00f3n en la segunda, resulta:</p> \\[P (S A S^{-1}) P^{-1} = C\\] <p>Rearrupando por asociatividad:</p> \\[(P S) A (S^{-1} P^{-1}) = C\\] <p>Sabemos por las propiedades de matrices invertibles que \\((P S)^{-1} = S^{-1} P^{-1}\\). Reemplazando este t\u00e9rmino:</p> \\[(P S) A (P S)^{-1} = C\\] <p>Si llamamos \\(U = P S\\), resultando en otra matriz que sabemos que es invertible porque el producto de dos invertibles lo es:</p> \\[U A U^{-1} = C\\] <p>Esto certifica por definici\u00f3n que \\(A \\sim C\\), probando la transitividad.</p> <p>Al cumplirse las tres condiciones, la semejanza de matrices es efectivamente una relaci\u00f3n de equivalencia.</p>"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#2-demostrar-que-si-a-es-semejante-a-b-entonces-texttra-texttrb","title":"2. Demostrar que si \\(A\\) es semejante a \\(B\\), entonces \\(\\text{Tr}(A) = \\text{Tr}(B)\\).","text":"<ol> <li>Demostrar que si \\(A\\) es semejante a \\(B\\), entonces:    $\\(\\text{Tr}(A) = \\text{Tr}(B)\\)$    Sugerencia: Utilizar la propiedad \\(\\text{Tr}(EC) = \\text{Tr}(CE)\\) para matrices \\(C\\) y \\(E\\).</li> </ol> <p>Si \\(A \\sim B\\), deducimos por definici\u00f3n que:</p> \\[B = S A S^{-1}\\] <p>Calculamos la traza en ambos lados:</p> \\[\\text{Tr}(B) = \\text{Tr}(S A S^{-1})\\] <p>Aprovechando la sugerencia, usamos la propiedad c\u00edclica de la traza: \\(\\text{Tr}(E C) = \\text{Tr}(C E)\\). </p> <p>Sea \\(E = S\\) y \\(C = (A S^{-1})\\). Sustituyendo en la propiedad de la traza:</p> \\[\\text{Tr}(S (A S^{-1})) = \\text{Tr}((A S^{-1}) S)\\] <p>Reescribiendo aprovechando la asociatividad dentro de la traza:</p> \\[\\text{Tr}((A S^{-1}) S) = \\text{Tr}(A (S^{-1} S))\\] <p>Como \\(S^{-1} S = I\\), tenemos:</p> \\[\\text{Tr}(A I) = \\text{Tr}(A)\\] <p>Por lo tanto, concluimos que:</p> \\[\\text{Tr}(B) = \\text{Tr}(A)\\]"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#3-probar-que-si-a-es-diagonalizable-con-valores-propios-0-y-1-entonces-a2-a","title":"3. Probar que si \\(A\\) es diagonalizable con valores propios 0 y 1, entonces \\(A^2 = A\\).","text":"<ol> <li>Probar que si \\(A\\) es diagonalizable (es decir, \\(A\\) es semejante a una matriz diagonal \\(D\\)) y los valores propios de \\(A\\) son 0 y 1, entonces:    $\\(A^2 = A\\)$</li> </ol> <p>Si \\(A\\) es diagonalizable, entonces es semejante a una matriz diagonal \\(D\\). Por definici\u00f3n existe una matriz invertible \\(P\\) tal que:</p> <p>$\\(A = P^{-1} D P\\)$ (Nota: podemos invertir los roles de \\(P\\) y su inversa definiendo \\(S = P^{-1}\\), con lo cual la forma \\(S D S^{-1} = A\\) y \\(P A P^{-1} = D\\) son hom\u00f3logas a la definici\u00f3n del enunciado).</p> <p>En cualquier matriz diagonal \\(D\\), los elementos de su diagonal principal est\u00e1n formados precisamente por los valores propios de \\(A\\). Dado que nos aseguran que los valores propios son 0 y 1, el contenido de \\(D\\) solamente posee este par de d\u00edgitos.</p> <p>Cuando multiplicamos una matriz diagonal consigo misma (es decir, \\(D^2\\)), los elementos resultantes de su diagonal principal est\u00e1n simplemente elevados al cuadrado. </p> <p>Como \\(0^2 = 0\\) y \\(1^2 = 1\\), los valores no se ven afectados y podemos asegurar categ\u00f3ricamente que:</p> \\[D^2 = D\\] <p>Vamos a la expresi\u00f3n para \\(A^2\\):</p> \\[A^2 = A \\cdot A = (P^{-1} D P) (P^{-1} D P)\\] <p>Aplicamos asociatividad para multiplicar primero las matrices interiores:</p> \\[A^2 = P^{-1} D (P P^{-1}) D P\\] <p>Sustituyendo \\(P P^{-1}\\) por la Identidad \\(I\\):</p> \\[A^2 = P^{-1} D I D P\\] \\[A^2 = P^{-1} D^2 P\\] <p>Dado la particularidad de nuestra matriz donde \\(D^2 = D\\), sustituimos y obtenemos:</p> \\[A^2 = P^{-1} D P\\] <p>Como esto es la definici\u00f3n exacta de nuestra matriz original \\(A\\), queda probado que:</p> \\[A^2 = A\\]"},{"location":"Examen_2025_02_24/01_semejanza_matrices/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef separar_seccion(titulo):\n    print(\"\\n\" + \"=\"*70)\n    print(f\" {titulo} \")\n    print(\"=\"*70)\n\n# ==========================================\n# 1. Verificaci\u00f3n de Relaci\u00f3n de Equivalencia\n# ==========================================\nseparar_seccion(\"1. Verificando Relaci\u00f3n de Equivalencia (Casos Pr\u00e1cticos)\")\n\ndef generar_matriz_invertible(n):\n    \"\"\"Genera matrices aleatorias hasta garantizar que su determinante no sea 0.\"\"\"\n    while True:\n        S = np.random.rand(n, n)\n        if np.linalg.det(S) != 0:\n            return S\n\nn = 3\nA = np.random.rand(n, n)\n\nprint(\"A) Reflexividad (A ~ A)\")\nI = np.eye(n)\nA_ref = I @ A @ np.linalg.inv(I)\nprint(\"A es semejante a s\u00ed misma (probado con la matriz Identidad).\")\nprint(\"\u00bfResultado id\u00e9ntico a A original?:\", np.allclose(A, A_ref))\n\nprint(\"\\nB) Simetr\u00eda (A ~ B implica B ~ A)\")\nS = generar_matriz_invertible(n)\nB = S @ A @ np.linalg.inv(S)\n\nS_inv = np.linalg.inv(S)\nA_sim = S_inv @ B @ np.linalg.inv(S_inv)\nprint(\"\u00bfRecuperamos A a partir de B usando la inversa de la transformaci\u00f3n?:\", np.allclose(A, A_sim))\n\nprint(\"\\nC) Transitividad (A ~ B y B ~ C implica A ~ C)\")\nT = generar_matriz_invertible(n)\nC = T @ B @ np.linalg.inv(T)\n\nU = T @ S\nC_trans = U @ A @ np.linalg.inv(U)\nprint(\"\u00bfObtenemos la misma matriz C operando desde A directamente con (T*S)?:\", np.allclose(C, C_trans))\n\n# ==========================================\n# 2. Verificaci\u00f3n de la Tr(A) = Tr(B)\n# ==========================================\nseparar_seccion(\"2. Verificaci\u00f3n de Tr(A) = Tr(B) para matrices semejantes\")\nS_trace = generar_matriz_invertible(n)\nA_trace = np.random.rand(n, n)\nB_trace = S_trace @ A_trace @ np.linalg.inv(S_trace)\n\ntr_A = np.trace(A_trace)\ntr_B = np.trace(B_trace)\nprint(f\"Traza de A original: {tr_A:.6f}\")\nprint(f\"Traza de B semejante: {tr_B:.6f}\")\nprint(\"\u00bfTrazas son matem\u00e1ticamente iguales?:\", np.isclose(tr_A, tr_B))\n\n# ==========================================\n# 3. Verificaci\u00f3n de A^2 = A\n# ==========================================\nseparar_seccion(\"3. Verificaci\u00f3n de A^2 = A para matriz diagonalizable (autovalores 0 y 1)\")\n\n# Crear matriz diagonal que contenga estrictamente 0s y 1s\nD = np.diag(np.random.choice([0, 1], size=n))\nprint(\"Matriz Diagonal D:\\n\", D)\n\n# Generar una base S cualquiera y aplicarla para construir A\nS_idem = generar_matriz_invertible(n)\nA_idem = S_idem @ D @ np.linalg.inv(S_idem)\n\nA_cuadrado = A_idem @ A_idem\n\nprint(\"\\nValidaci\u00f3n con np.allclose de A^2 == A:\", np.allclose(A_idem, A_cuadrado))\nprint(\"\\nMatriz A original generada:\\n\", A_idem.round(4))\nprint(\"\\nMatriz A^2 calculada:\\n\", A_cuadrado.round(4))\n\nprint(\"\\nTodas las verificaciones completadas con \u00e9xito.\")\n</code></pre>"},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/","title":"Soluci\u00f3n del Ejercicio 2","text":"<p>Ejercicio 2</p> <ol> <li>Calcular la descomposici\u00f3n en valores singulares (SVD) de la matriz:</li> </ol> <p>$\\(A = \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix}\\)$</p> <ol> <li>Probar que \\(PA\\) y \\(AP\\) tienen los mismos valores singulares que \\(A\\), donde \\(P\\) es una matriz de permutaci\u00f3n. Adem\u00e1s, calcular \\(||PA||_2\\) y \\(\\kappa_2(PA)\\).</li> </ol>"},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/#1-calcular-la-descomposicion-en-valores-singulares-svd-de-la-matriz-a","title":"1. Calcular la descomposici\u00f3n en valores singulares (SVD) de la matriz \\(A\\)","text":"<ol> <li>Calcular la descomposici\u00f3n en valores singulares (SVD) de la matriz:    $\\(A = \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix}\\)$</li> </ol> <p>Dada la matriz:</p> \\[A = \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix}\\] <p>Sabemos que la descomposici\u00f3n en valores singulares se estructura como \\(A = U \\Sigma V^T\\).</p> <p>Donde \\(\\Sigma\\) contiene los valores singulares (en la diagonal) que son las ra\u00edces cuadradas de los autovalores de \\(A^T A\\).</p> <p>Calculamos \\(A^T A\\):</p> \\[A^T A = \\begin{pmatrix} 0 &amp; 2 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} = \\begin{pmatrix} 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 9 \\end{pmatrix}\\] <p>Dado que \\(A^T A\\) es una matriz diagonal, sus autovalores son directamente los elementos de su diagonal:</p> <ul> <li> <p>\\(\\lambda_1 = 9 \\implies \\sigma_1 = 3\\)</p> </li> <li> <p>\\(\\lambda_2 = 4 \\implies \\sigma_2 = 2\\)</p> </li> <li> <p>\\(\\lambda_3 = 1 \\implies \\sigma_3 = 1\\)</p> </li> </ul> <p>Por ende, nuestra matriz de valores singulares ordenados en forma decreciente es:</p> \\[\\Sigma = \\begin{pmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix}\\] <p>Ahora, hallamos los autovectores ortonormales de \\(A^T A\\) asociados a estos autovalores para construir \\(V\\):</p> <ul> <li> <p>Para \\(\\lambda_1 = 9\\), el vector propio asociado a la tercera columna resulta ser \\(v_1 = (0, 0, 1)^T\\).</p> </li> <li> <p>Para \\(\\lambda_2 = 4\\), el vector propio asociado a la primera columna resulta ser \\(v_2 = (1, 0, 0)^T\\).</p> </li> <li> <p>Para \\(\\lambda_3 = 1\\), el vector propio asociado a la segunda columna resulta ser \\(v_3 = (0, 1, 0)^T\\).</p> </li> </ul> <p>Concatenando dichos autovectores formamos \\(V\\), y por lo tanto \\(V^T\\):</p> \\[V = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\end{pmatrix} \\implies V^T = \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}\\] <p>Para hallar \\(U = (\\vec{u}_1, \\vec{u}_2, \\vec{u}_3)\\), utilizamos la relaci\u00f3n \\(u_i = \\frac{1}{\\sigma_i} A v_i\\):</p> <ul> <li> <p>\\(u_1 = \\frac{1}{3} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 0 \\\\ 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\\)</p> </li> <li> <p>\\(u_2 = \\frac{1}{2} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\\)</p> </li> <li> <p>\\(u_3 = \\frac{1}{1} \\begin{pmatrix} 0 &amp; -1 &amp; 0 \\\\ 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\)</p> </li> </ul> <p>Reemplazando en \\(U\\):</p> \\[U = \\begin{pmatrix} 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\end{pmatrix}\\] <p>La Descomposici\u00f3n completa es finalmente:</p> \\[A = \\begin{pmatrix} 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{pmatrix}\\]"},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/#2-probar-que-pa-y-ap-tienen-los-mismos-valores-singulares-que-a","title":"2. Probar que \\(PA\\) y \\(AP\\) tienen los mismos valores singulares que \\(A\\)","text":"<ol> <li>Probar que \\(PA\\) y \\(AP\\) tienen los mismos valores singulares que \\(A\\), donde \\(P\\) es una matriz de permutaci\u00f3n. Adem\u00e1s, calcular \\(||PA||_2\\) y \\(\\kappa_2(PA)\\).</li> </ol> <p>Una matriz de permutaci\u00f3n \\(P\\) es siempre una matriz ortogonal; es decir, altera el orden de filas o columnas, pero respeta la isometr\u00eda:</p> \\[P^T P = P P^T = I\\] <p>A. Para \\(PA\\):</p> <p>Los valores singulares de \\(PA\\) son las ra\u00edces cuadradas de los autovalores de la matriz sim\u00e9trica \\((PA)^T (PA)\\).</p> <p>Sustituyendo y desarrollando:</p> \\[(PA)^T (PA) = A^T P^T P A\\] <p>Como \\(P^T P = I\\), esto simplifica a:</p> \\[A^T I A = A^T A\\] <p>Dado que obtenemos exactamente el mismo n\u00facleo subyacente \\(A^T A\\), la matriz \\(PA\\) tiene estrictamente el mismo espectro de autovalores para dicha expresi\u00f3n, y por tanto, id\u00e9nticos valores singulares que \\(A\\).</p> <p>B. Para \\(AP\\):</p> <p>Buscamos los autovalores de \\((AP)^T (AP)\\):</p> \\[(AP)^T (AP) = P^T A^T A P\\] <p>Esta expresi\u00f3n equivale a que \\((AP)^T (AP)\\) y \\(A^T A\\) son matrices semejantes (aqu\u00ed interviene lo demostrado en el Ejercicio 1), ya que \\(P^T = P^{-1}\\).</p> <p>Recordemos adem\u00e1s, que las matrices semejantes preservan id\u00e9nticos autovalores. </p> <p>Por consiguiente, los valores singulares producidos por \\(P^T A^T A P\\) ser\u00e1n los mismos que los de \\(A^T A\\), dejando en evidencia que \\(AP\\) posee iguales valores singulares a \\(A\\).</p> <p>C. Calcular \\(\\|PA\\|_2\\) y \\(\\kappa_2(PA)\\)</p> <p>Sabemos por sus propiedades fundamentales que:</p> <ul> <li> <p>La norma-2 de una matriz es igual a su mayor valor singular: \\(\\|M\\|_2 = \\sigma_{\\max}\\)</p> </li> <li> <p>El n\u00famero de condici\u00f3n en base 2 equivale a la proporci\u00f3n de elongamiento l\u00edmite: \\(\\kappa_2(M) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\\)</p> </li> </ul> <p>Al haber probado instantes atr\u00e1s que multiplicar por una permutaci\u00f3n no afecta los valores singulares en absoluto, usamos el espectro ya calculado \\(\\sigma \\in \\{3, 2, 1\\}\\):</p> \\[\\|PA\\|_2 = \\sigma_{\\max}(PA) = \\sigma_{\\max}(A) = 3\\] \\[\\kappa_2(PA) = \\frac{\\sigma_{\\max}(PA)}{\\sigma_{\\min}(PA)} = \\frac{3}{1} = 3\\]"},{"location":"Examen_2025_02_24/02_descomposicion_svd/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef separar_seccion(titulo):\n    print(\"\\n\" + \"=\"*75)\n    print(f\" {titulo} \")\n    print(\"=\"*75)\n\n# ==========================================\n# 1. Descomposici\u00f3n en Valores Singulares (SVD)\n# ==========================================\nseparar_seccion(\"1. Confirmaci\u00f3n de SVD Anal\u00edtica\")\nA = np.array([[0, -1, 0],\n              [2,  0, 0],\n              [0,  0, -3]])\n\n# Matrices te\u00f3ricas derivadas a mano\nU_teoric = np.array([[ 0,  0, -1],\n                     [ 0,  1,  0],\n                     [-1,  0,  0]])\nSigma_teoric = np.diag([3, 2, 1])\nVt_teoric = np.array([[0, 0, 1],\n                      [1, 0, 0],\n                      [0, 1, 0]])\n\n# Reconstruir A desde la teor\u00eda\nA_reconstruida = U_teoric @ Sigma_teoric @ Vt_teoric\nprint(\"A reconstruida con matrices te\u00f3ricas:\")\nprint(A_reconstruida)\nprint(\"\u00bfCoincide exactamente con la matriz original? :\", np.allclose(A, A_reconstruida))\n\n# Usar herramienta directa de NumPy (que a veces toma orientaciones de signo distintas al estar estandarizado)\nU_np, s_np, Vt_np = np.linalg.svd(A)\nprint(\"\\nValores singulares num\u00e9ricos provistos por np.linalg.svd:\")\nprint(s_np)\nprint(\"\u00bfCoinciden los valores singulares extra\u00eddos (3, 2, 1)? :\", np.allclose([3, 2, 1], s_np))\n\n# ==========================================\n# 2. Invarianzas ante Matrices de Permutaci\u00f3n\n# ==========================================\nseparar_seccion(\"2. Espectro inmutable bajo Transformaciones Ortogonales (Permutaciones)\")\n\n# Generar una matriz de permutaci\u00f3n al azar mezclando la matriz identidad\nI = np.eye(3)\nP = I[np.random.permutation(3), :]\nprint(\"Matriz de Permutaci\u00f3n generada (P):\")\nprint(P)\n\nprint(\"\\nVerificando que P sea ortogonal (P.T @ P == I):\", np.allclose(P.T @ P, I))\n\n# Analizamos PA y AP\nPA = P @ A\nAP = A @ P\n\n_, s_PA, _ = np.linalg.svd(PA)\n_, s_AP, _ = np.linalg.svd(AP)\n\nprint(f\"\\nValores singulares originales de A: {s_np.round(2)}\")\nprint(f\"Valores singulares de PA :        {s_PA.round(2)}\")\nprint(f\"Valores singulares de AP :        {s_AP.round(2)}\")\nprint(\"\u00bfSon matem\u00e1ticamente equivalentes? :\", np.allclose(s_np, s_PA) and np.allclose(s_np, s_AP))\n\n# ==========================================\n# 3. Norma-2 y N\u00famero de Condici\u00f3n\n# ==========================================\nseparar_seccion(\"3. C\u00e1lculo Num\u00e9rico de Norma y Condici\u00f3n\")\n\nnorm_2_PA = np.linalg.norm(PA, ord=2)\ncond_2_PA = np.linalg.cond(PA, p=2)\n\nprint(f\"||PA||_2 obtenido por linalg  = {norm_2_PA:.4f}\")\nprint(f\"||PA||_2 de resultado te\u00f3rico = 3\")\nprint(\"\u00bfConcuerda la Norma? :\", np.isclose(norm_2_PA, 3))\n\nprint(f\"\\nCond2(PA) obtenido por linalg    = {cond_2_PA:.4f}\")\nprint(f\"Cond2(PA) de resultado te\u00f3rico   = 3\")\nprint(\"\u00bfConcuerda el Condici\u00f3n Num? :\", np.isclose(cond_2_PA, 3))\n\nprint(\"\\nTodo el flujo anal\u00edtico validado consistentemente.\")\n</code></pre>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/","title":"Soluci\u00f3n del Ejercicio 3","text":"<p>Ejercicio 3</p> <p>Dada la matriz:</p> \\[A = \\begin{pmatrix} 1 &amp; c &amp; 0 \\\\ 0 &amp; 1 &amp; c \\\\ 0 &amp; c &amp; 1 \\end{pmatrix}\\] <ol> <li>Determinar para qu\u00e9 valores de \\(c\\) convergen los m\u00e9todos de Jacobi y Gauss-Seidel.</li> <li>Comparar la velocidad de convergencia de ambos m\u00e9todos.</li> <li>Plantear las iteraciones correspondientes para cada m\u00e9todo.</li> </ol> <p>Dada la matriz del sistema:</p> <p>Podemos descomponer la matriz en su diagonal (\\(D\\)), su parte estrictamente inferior (\\(L\\)) y su parte estrictamente superior (\\(U\\)), de tal forma que \\(A = D + L + U\\):</p> <ul> <li> <p>\\(D = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} = I\\)</p> </li> <li> <p>\\(L = \\begin{pmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; c &amp; 0 \\end{pmatrix}\\)</p> </li> <li> <p>\\(U = \\begin{pmatrix} 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; c \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix}\\)</p> </li> </ul>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#1-valores-de-c-para-los-cuales-convergen-jacobi-y-gauss-seidel","title":"1. Valores de \\(c\\) para los cuales convergen Jacobi y Gauss-Seidel","text":"<ol> <li>Determinar para qu\u00e9 valores de \\(c\\) convergen los m\u00e9todos de Jacobi y Gauss-Seidel.</li> </ol> <p>Ambos m\u00e9todos iterativos convergen para cualquier valor inicial si y solo si el radio espectral de sus matrices de iteraci\u00f3n (denotado \\(\\rho\\), que es el m\u00e1ximo valor absoluto de sus autovalores) es estrictamente menor a 1 (\\(\\rho &lt; 1\\)).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#metodo-de-jacobi","title":"M\u00e9todo de Jacobi","text":"<p>La matriz de iteraci\u00f3n de Jacobi est\u00e1 dada por:</p> \\[T_J = -D^{-1}(L + U) = -I (L + U) = \\begin{pmatrix} 0 &amp; -c &amp; 0 \\\\ 0 &amp; 0 &amp; -c \\\\ 0 &amp; -c &amp; 0 \\end{pmatrix}\\] <p>Calculamos sus autovalores encontrando el n\u00facleo del polinomio caracter\u00edstico \\(\\det(T_J - \\lambda I) = 0\\):</p> \\[ \\begin{vmatrix} -\\lambda &amp; -c &amp; 0 \\\\ 0 &amp; -\\lambda &amp; -c \\\\ 0 &amp; -c &amp; -\\lambda \\end{vmatrix} = 0 \\] <p>Desarrollando el determinante evaluando por la primera columna:</p> \\[ -\\lambda \\begin{vmatrix} -\\lambda &amp; -c \\\\ -c &amp; -\\lambda \\end{vmatrix} = -\\lambda (\\lambda^2 - c^2) = 0 \\] <p>Las ra\u00edces son: \\(\\lambda_1 = 0\\), \\(\\lambda_2 = c\\), \\(\\lambda_3 = -c\\).</p> <p>El radio espectral es \\(\\rho(T_J) = \\max(|0|, |c|, |-c|) = |c|\\).</p> <p>Por lo tanto, el m\u00e9todo de Jacobi converge si y solo si \\(|c| &lt; 1\\).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#metodo-de-gauss-seidel","title":"M\u00e9todo de Gauss-Seidel","text":"<p>La matriz de iteraci\u00f3n de Gauss-Seidel est\u00e1 dada por:</p> \\[T_{GS} = -(D + L)^{-1} U\\] <p>Primero, calculamos \\((D + L)^{-1}\\):</p> \\[D + L = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; c &amp; 1 \\end{pmatrix} \\implies (D + L)^{-1} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -c &amp; 1 \\end{pmatrix}\\] <p>Ahora hallamos \\(T_{GS}\\):</p> \\[T_{GS} = -\\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -c &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; c \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} = - \\begin{pmatrix} 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; c \\\\ 0 &amp; 0 &amp; -c^2 \\end{pmatrix} = \\begin{pmatrix} 0 &amp; -c &amp; 0 \\\\ 0 &amp; 0 &amp; -c \\\\ 0 &amp; 0 &amp; c^2 \\end{pmatrix}\\] <p>Al ser una matriz triangular superior, sus autovalores se desprenden directamente de la diagonal principal:</p> <p>\\(\\lambda_1 = 0\\), \\(\\lambda_2 = 0\\), \\(\\lambda_3 = c^2\\).</p> <p>El radio espectral es \\(\\rho(T_{GS}) = \\max(|0|, |0|, |c^2|) = c^2\\).</p> <p>Por lo tanto, el m\u00e9todo de Gauss-Seidel converge si y solo si \\(c^2 &lt; 1\\), lo cual ocurre puramente si \\(|c| &lt; 1\\).</p> <p>Ambos m\u00e9todos convergen en el mismo intervalo: \\(c \\in (-1, 1)\\).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#2-comparar-la-velocidad-de-convergencia","title":"2. Comparar la velocidad de convergencia","text":"<ol> <li>Comparar la velocidad de convergencia de ambos m\u00e9todos.</li> </ol> <p>La tasa asint\u00f3tica de convergencia se define computacionalmente como \\(R(T) = -\\ln(\\rho(T))\\). </p> <p>A mayor tasa de convergencia te\u00f3rica (\\(R\\)), se requieren en la pr\u00e1ctica menos iteraciones para converger con la computadora.</p> <ul> <li> <p>Para Jacobi: \\(R(T_J) = -\\ln(|c|)\\)</p> </li> <li> <p>Para Gauss-Seidel: \\(R(T_{GS}) = -\\ln(c^2) = -2\\ln(|c|) = 2 R(T_J)\\)</p> </li> </ul> <p>La relaci\u00f3n concluyente es que la tasa de convergencia de Gauss-Seidel es exactamente el doble. Por lo tanto, el m\u00e9todo de Gauss-Seidel converge el doble de r\u00e1pido que el m\u00e9todo de Jacobi. Esperamos que Gauss-Seidel demore anal\u00edticamente la mitad de iteraciones computacionales en confluir para cualquier valor de \\(|c| &lt; 1\\).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#3-plantear-las-iteraciones-correspondientes-para-cada-metodo","title":"3. Plantear las iteraciones correspondientes para cada m\u00e9todo","text":"<ol> <li>Plantear las iteraciones correspondientes para cada m\u00e9todo.</li> </ol> <p>Para el sistema general \\(A \\vec{x} = \\vec{b}\\), es decir:</p> \\[ \\begin{cases}  x_1 + c x_2 = b_1 \\\\  x_2 + c x_3 = b_2 \\\\  c x_2 + x_3 = b_3  \\end{cases} \\]"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#forma-iterativa-de-jacobi","title":"Forma iterativa de Jacobi:","text":"<p>El m\u00e9todo actualiza todas las variables en la iteraci\u00f3n \\((k+1)\\) en base exclusiva de los valores previos de la iteraci\u00f3n \\((k)\\):</p> \\[x_1^{(k+1)} = b_1 - c \\cdot x_2^{(k)}\\] \\[x_2^{(k+1)} = b_2 - c \\cdot x_3^{(k)}\\] \\[x_3^{(k+1)} = b_3 - c \\cdot x_2^{(k)}\\]"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#forma-iterativa-de-gauss-seidel","title":"Forma iterativa de Gauss-Seidel:","text":"<p>Este m\u00e9todo utiliza los escalares m\u00e1s actualizados que encuentra (es decir, usa variables del estrato \\(k+1\\) tan pronto como hayan sido calculadas en cascada natural).</p> \\[x_1^{(k+1)} = b_1 - c \\cdot x_2^{(k)}\\] \\[x_2^{(k+1)} = b_2 - c \\cdot x_3^{(k)}\\] \\[x_3^{(k+1)} = b_3 - c \\cdot x_2^{(k+1)}\\] <p>(N\u00f3tese que en esta \u00faltima l\u00ednea usa formalmente \\(x_2^{(k+1)}\\), que ya fue computado en el paso anterior).</p>"},{"location":"Examen_2025_02_24/03_metodos_iterativos/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef separar_seccion(titulo):\n    print(\"\\n\" + \"=\"*75)\n    print(f\" {titulo} \")\n    print(\"=\"*75)\n\nc = 0.5 # Seleccionamos un valor dentro del intervalo de convergencia (|c| &lt; 1)\nprint(f\"Configurando par\u00e1metro modelo de prueba: c = {c}\")\n\n# Construcci\u00f3n de la matriz y el vector b\nA = np.array([\n    [1, c, 0],\n    [0, 1, c],\n    [0, c, 1]\n])\n# Vector soluci\u00f3n deseado \"arbitrario\", por ende b = A @ x_real\nx_real = np.array([1.0, 2.0, -1.0])\nb = A @ x_real\n\n\n# ==========================================\n# 1. Verificaci\u00f3n de Radios Espectrales\n# ==========================================\nseparar_seccion(\"1. Verificaci\u00f3n de Autovalores de Iteraci\u00f3n\")\n\nD = np.diag(np.diag(A))\nL = np.tril(A, -1)\nU = np.triu(A, 1)\n\nD_inv = np.linalg.inv(D)\nT_J = -D_inv @ (L + U)\n\nDL_inv = np.linalg.inv(D + L)\nT_GS = -DL_inv @ U\n\nautovalores_TJ = np.linalg.eigvals(T_J)\nrho_TJ = max(abs(autovalores_TJ))\n\nautovalores_TGS = np.linalg.eigvals(T_GS)\nrho_TGS = max(abs(autovalores_TGS))\n\nprint(f\"Autovalores te\u00f3ricos esperados T_J: [0, {c}, {-c}]\")\nprint(f\"Autovalores computados T_J:         {autovalores_TJ.round(4)}\")\nprint(f\"Radio Espectral Rho(T_J) (Deber\u00eda ser |c|={c}): {rho_TJ:.4f}\")\n\nprint(f\"\\nAutovalores te\u00f3ricos esperados T_GS: [0, 0, {c**2}]\")\nprint(f\"Autovalores computados T_GS:         {autovalores_TGS.round(4)}\")\nprint(f\"Radio Espectral Rho(T_GS) (Deber\u00eda ser |c|\u00b2={c**2}): {rho_TGS:.4f}\")\n\n# ==========================================\n# 2. Verificaci\u00f3n de Velocidad de Convergencia Iterativa\n# ==========================================\nseparar_seccion(\"2. Simulaci\u00f3n de los M\u00e9todos Iterativos\")\n\ndef norm_res(x):\n    return np.linalg.norm((A @ x) - b)\n\ntol = 1e-10\nmax_iter = 1000\n\n# ----- JACOBI -----\nx_j = np.zeros(3)\niter_j = 0\nwhile norm_res(x_j) &gt; tol and iter_j &lt; max_iter:\n    x_next = np.zeros(3)\n    x_next[0] = b[0] - c * x_j[1]\n    x_next[1] = b[1] - c * x_j[2]\n    # En Jacobi siempre usa el modelo viejo\n    x_next[2] = b[2] - c * x_j[1]\n    x_j = x_next\n    iter_j += 1\n\n# ----- GAUSS-SEIDEL -----\nx_gs = np.zeros(3)\niter_gs = 0\nwhile norm_res(x_gs) &gt; tol and iter_gs &lt; max_iter:\n    x_next = np.zeros(3)\n    x_next[0] = b[0] - c * x_gs[1]\n    x_next[1] = b[1] - c * x_gs[2]\n    # En Gauss-Seidel usa la porci\u00f3n de x que viene de este mismo turno\n    x_next[2] = b[2] - c * x_next[1] \n    x_gs = x_next\n    iter_gs += 1\n\n\nprint(f\"Tolerancia pautada: {tol}\")\nprint(f\"Iteraciones necesarias con Jacobi:       {iter_j}\")\nprint(f\"Iteraciones necesarias con Gauss-Seidel: {iter_gs}\")\nprint(\"\\n\u00bfGauss-Seidel tom\u00f3 pr\u00e1cticamente la mitad del tiempo? (T_GS iter ~= T_J iter / 2)\")\nfactor_velocidad = iter_j / iter_gs\nprint(f\"-&gt; Iteraciones_J / Iteraciones_GS = {factor_velocidad:.2f}\")\n\nprint(\"\\nVerificamos que ambas confluyeron a la soluci\u00f3n final (1.0, 2.0, -1.0):\")\nprint(f\"x final Jacobi:       {x_j.round(5)}\")\nprint(f\"x final Gauss-Seidel: {x_gs.round(5)}\")\n</code></pre>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/","title":"Soluci\u00f3n del Ejercicio 4","text":"<p>Ejercicio 4</p> <p>Dada la funci\u00f3n:</p> \\[z = a y^b e^{cx+2}\\] <ol> <li>Plantear las ecuaciones de m\u00ednimos cuadrados para estimar los par\u00e1metros \\(a\\), \\(b\\) y \\(c\\).</li> <li>Proponer puntos de datos para que la soluci\u00f3n sea \u00fanica.</li> <li>Determinar la m\u00ednima cantidad de puntos necesarios para que la soluci\u00f3n sea \u00fanica.</li> </ol> <p>Dada la funci\u00f3n no lineal:</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#1-plantear-las-ecuaciones-de-minimos-cuadrados-para-estimar-los-parametros-a-b-y-c","title":"1. Plantear las ecuaciones de m\u00ednimos cuadrados para estimar los par\u00e1metros \\(a, b\\) y \\(c\\).","text":"<ol> <li>Plantear las ecuaciones de m\u00ednimos cuadrados para estimar los par\u00e1metros \\(a\\), \\(b\\) y \\(c\\).</li> </ol> <p>Para aplicar el m\u00e9todo de M\u00ednimos Cuadrados Lineales Cl\u00e1sicos, primero debemos transformar (linealizar) el modelo geom\u00e9trico/exponencial aplicando logaritmo natural (\\(\\ln\\)) a ambos lados de la ecuaci\u00f3n:</p> \\[\\ln(z) = \\ln(a \\cdot y^b \\cdot e^{cx + 2})\\] <p>Por las propiedades de los logaritmos (el logaritmo de un producto es la suma de los logaritmos, y el exponente baja multiplicando), la expresi\u00f3n queda:</p> \\[\\ln(z) = \\ln(a) + b \\ln(y) + (cx + 2)\\] <p>Reagrupando los t\u00e9rminos para independizar las inc\u00f3gnitas de las constantes conocidas:</p> \\[\\ln(z) - 2 = \\ln(a) + b \\ln(y) + c x\\] <p>A partir de esta estructura lineal en sus par\u00e1metros, efectuamos los siguientes cambios de variable para llevarlo a un modelo lineal est\u00e1ndar de la forma \\(Z_i = \\beta_0 + \\beta_1 Y_i + \\beta_2 X_i\\):</p> <ul> <li> <p>\\(Z = \\ln(z) - 2\\) (variable dependiente transformada)</p> </li> <li> <p>\\(A = \\ln(a)\\) (nuevo par\u00e1metro ordenado al origen, luego \\(a = e^A\\))</p> </li> <li> <p>El par\u00e1metro \\(b\\) queda libre.</p> </li> <li> <p>El par\u00e1metro \\(c\\) queda libre.</p> </li> <li> <p>La variable asociada a \\(b\\) es \\(\\ln(y)\\)</p> </li> <li> <p>La variable asociada a \\(c\\) es \\(x\\)</p> </li> </ul> <p>Para un conjunto de \\(m\\) puntos experimentales \\((x_i, y_i, z_i)\\), definimos el sistema de ecuaciones sobre-determinado en forma matricial \\(M \\vec{\\theta} = \\vec{Z}\\) como:</p> \\[ \\begin{pmatrix}  1 &amp; \\ln(y_1) &amp; x_1 \\\\ 1 &amp; \\ln(y_2) &amp; x_2 \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; \\ln(y_m) &amp; x_m \\end{pmatrix} \\begin{pmatrix} A \\\\ b \\\\ c \\end{pmatrix} = \\begin{pmatrix} \\ln(z_1) - 2 \\\\ \\ln(z_2) - 2 \\\\ \\vdots \\\\ \\ln(z_m) - 2 \\end{pmatrix} \\] <p>Las ecuaciones normales de M\u00ednimos Cuadrados se construyen multiplicando por izquierda la transpuesta de la matriz de dise\u00f1o \\(M\\):</p> \\[(M^T M) \\vec{\\theta} = M^T \\vec{Z}\\] <p>Resolviendo este sistema lineal \\((3 \\times 3)\\) se obtienen los estimadores param\u00e9tricos \u00f3ptimos \\(\\hat{A}\\), \\(\\hat{b}\\) y \\(\\hat{c}\\). Posteriormente se recupera \\(\\hat{a} = e^{\\hat{A}}\\).</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#2-proponer-puntos-de-datos-para-que-la-solution-sea-unica","title":"2. Proponer puntos de datos para que la solution sea \u00fanica.","text":"<ol> <li>Proponer puntos de datos para que la soluci\u00f3n sea \u00fanica.</li> </ol> <p>Para que las ecuaciones de m\u00ednimos cuadrados posean una soluci\u00f3n \u00fanica, la matriz normal cuadrada \\((M^T M)\\) debe ser estrictamente invertible. Esto ocurre si y solo si la matriz de dise\u00f1o \\(M\\) posee rango completo por columnas.</p> <p>Como \\(M\\) tiene 3 columnas, necesitamos que el \\(\\text{Rango}(M) = 3\\). Geom\u00e9tricamente, los vectores columna de la matriz \\(M\\) formados por las mediciones no deben ser dependientes entre s\u00ed.</p> <p>En el contexto f\u00edsico del problema, esto implica que:</p> <ul> <li> <p>Los puntos de los datos \\((x_i, y_i)\\) no deben formar una combinaci\u00f3n lineal perfecta. No vale que para todas las mediciones sea siempre \\(x_i = k \\cdot \\ln(y_i) + C\\) (es decir, no pueden ser colineales en el plano de las caracter\u00edsticas transformadas).</p> </li> <li> <p>La variable \\(y_i\\) debe ser estrictamente positiva (\\(y_i &gt; 0\\)) para todo \\(i\\), dado que el dominio natural del \\(\\ln(y_i)\\) no admite valores negativos ni ceros.</p> </li> </ul> <p>Propuesta de puntos experimentalmente v\u00e1lidos y robustos: (Para garantizar que no sean colineales ni constantes, basta con alterar alternativamente las magnitudes en los ejes):</p> <ul> <li> <p>\\(P_1 = (x_1=1,\\, y_1=1,\\, z_1)\\)</p> </li> <li> <p>\\(P_2 = (x_2=0,\\, y_2=e,\\, z_2)\\)</p> </li> <li> <p>\\(P_3 = (x_3=-1,\\, y_3=1,\\, z_3)\\)</p> </li> </ul> <p>Evaluemos c\u00f3mo queda nuestra matriz de muestras con estos puntos de prueba para confirmar su independencia:</p> \\[M_{\\text{propuesta}} = \\begin{pmatrix} 1 &amp; \\ln(1) &amp; 1 \\\\ 1 &amp; \\ln(e) &amp; 0 \\\\ 1 &amp; \\ln(1) &amp; -1 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; -1 \\end{pmatrix}\\] <p>El determinante de esta matriz \\(3 \\times 3\\) no es cero (en efecto, vale \\(-2\\)): \\(\\det(M_{\\text{propuesta}}) = 1 \\cdot (-1 - 0) - 0 \\cdot (-1 -0) + 1 \\cdot (0 - 1) = -1 - 1 = -2\\).</p> <p>Por tanto, el determinante de la matriz normal \\(\\det(M^T M) = \\det(M)^2 = 4 \\neq 0\\). El rango es completo, y la soluci\u00f3n de los par\u00e1metros est\u00e1 garantizada demostr\u00e1blemente \u00fanica.</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#3-determinar-la-minima-cantidad-de-puntos-necesarios-para-que-la-solucion-sea-unica","title":"3. Determinar la m\u00ednima cantidad de puntos necesarios para que la soluci\u00f3n sea \u00fanica.","text":"<ol> <li>Determinar la m\u00ednima cantidad de puntos necesarios para que la soluci\u00f3n sea \u00fanica.</li> </ol> <p>El sistema general de m\u00ednimos cuadrados tiene como inc\u00f3gnita el vector \\(\\vec{\\theta} = [A, b, c]^T\\), el cual contiene 3 par\u00e1metros a estimar libremente.</p> <p>Por el Teorema de Rouch\u00e9-Frobenius y el Rango fundamental del \u00e1lgebra matricial:</p> <ul> <li> <p>Si aportamos \\(m &lt; 3\\) puntos, el sistema quedar\u00e1 sub-determinado (tendr\u00e1 infinitas soluciones l\u00f3gicas porque habr\u00e1n variables libres, el rango m\u00e1ximo ser\u00e1 menor a 3 penalizando a \\(M^T M\\)).</p> </li> <li> <p>Para que la matriz de dise\u00f1o \\(M \\in \\mathbb{R}^{m \\times 3}\\) logre poseer un Rango por Columnas exactamente igual a 3 (condici\u00f3n forzosa e innegociable para que la inversa de \\(M^T M\\) exista), necesitamos aportar un m\u00ednimo estricto de \\(m = 3\\) puntos.</p> </li> </ul> <p>Conclusi\u00f3n: Se necesitan como m\u00ednimo 3 puntos emp\u00edricos (siempre y cuando estos no formen un subespacio degenerado de dimensiones menores, seg\u00fan lo exigido en el inciso 2).</p>"},{"location":"Examen_2025_02_24/04_minimos_cuadrados/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef separar_seccion(titulo):\n    print(\"\\n\" + \"=\"*75)\n    print(f\" {titulo} \")\n    print(\"=\"*75)\n\n# ==========================================\n# 1. Preparaci\u00f3n del Sistema Matem\u00e1tico Base\n# ==========================================\nseparar_seccion(\"1. Generaci\u00f3n de Puntos y Soluci\u00f3n \u00danica M\u00ednima\")\n\n# Creamos par\u00e1metros \"secretos\" o ideales que la m\u00e1quina debe reconstruir\na_real = 4.5\nb_real = -1.2\nc_real = 2.0\n\ndef generar_Z_real(x, y):\n    \"\"\" Funci\u00f3n original z = a * y^b * e^(cx + 2) \"\"\"\n    Z_val = a_real * (y**b_real) * np.exp(c_real * x + 2)\n    return Z_val\n\n# Evaluamos con la \"m\u00ednima cantidad de puntos\" propuesta te\u00f3ricamente: m = 3\n# Recordemos el inciso 2: eleg\u00ed (1, 1), (0, e), (-1, 1) que demostraron independencia\npuntos = [\n    (1, 1),\n    (0, np.exp(1)),\n    (-1, 1)\n]\n\nx_array = np.array([p[0] for p in puntos])\ny_array = np.array([p[1] for p in puntos])\nz_array = generar_Z_real(x_array, y_array)\n\nprint(f\"Par\u00e1metros intr\u00ednsecos ocultos: a={a_real}, b={b_real}, c={c_real}\")\nprint(f\"Cantidad de puntos inyectados: {len(puntos)} (El m\u00ednimo te\u00f3rico estipulado)\")\n\n\n# ==========================================\n# 2. Resoluci\u00f3n de M\u00ednimos Cuadrados\n# ==========================================\nseparar_seccion(\"2. Reconstrucci\u00f3n Param\u00e9trica usando M\u00ednimos Cuadrados Ordinarios (MCO)\")\n\n# Modelado Vectorial de la Transformaci\u00f3n:\n# M = [1, ln(y), x]\n# Z_transformed = ln(z) - 2\n\nM = np.column_stack((\n    np.ones(len(puntos)),\n    np.log(y_array),\n    x_array\n))\n\nZ_transformado = np.log(z_array) - 2\n\nprint(\"Matriz de Dise\u00f1o (M):\")\nprint(M)\nrango = np.linalg.matrix_rank(M)\nprint(f\"\\nRango calculado de la Matriz M: {rango}\")\nif rango == 3:\n    print(\"&gt;&gt; Rango Completo detectado: M^T M es estrictamente invertible, existe SOLO 1 SOLUCION.\")\n\n# Calculamos Ecuaciones Normales expl\u00edcitamete: (M^T M) \u03b8 = M^T Z\nMt = M.T\nMt_M = Mt @ M\nMt_Z = Mt @ Z_transformado\n\ntry:\n    parametros_estimados_theta = np.linalg.inv(Mt_M) @ Mt_Z\n    A_est, b_est, c_est = parametros_estimados_theta\n    a_est = np.exp(A_est)\n\n    print(\"\\nResultados del Fitteo de M\u00ednimos Cuadrados:\")\n    print(f\"a_estimado : {a_est:.4f}  (Real: {a_real})\")\n    print(f\"b_estimado : {b_est:.4f}  (Real: {b_real})\")\n    print(f\"c_estimado : {c_est:.4f}  (Real: {c_real})\")\n\n    # Tolerancia\n    if np.allclose([a_est, b_est, c_est], [a_real, b_real, c_real]):\n        print(\"\\n&gt;&gt; EXITO: Los estimadores convergieron id\u00e9nticos con precisi\u00f3n perfecta usando solo 3 puntos.\")\n\nexcept np.linalg.LinAlgError:\n    print(\"&gt;&gt; ERROR: La matriz M^T M generada result\u00f3 escalarmente singular (Puntos colineales o dependientes).\")\n\n\n# ==========================================\n# 3. Prueba de Fallo con puntos insuficientes\n# ==========================================\nseparar_seccion(\"3. Comprobaci\u00f3n de Sub-determinaci\u00f3n (N &lt; 3)\")\n\nprint(\"Intentando predecir MCO usando sub-muestreo intencionado (s\u00f3lo usando 2 puntos):\")\nM_deficiente = M[:2, :]  # Agarro nom\u00e1s las primeras dos filas\nrango_deficiente = np.linalg.matrix_rank(M_deficiente)\nprint(f\"Rango M_deficiente: {rango_deficiente}\")\nprint(\"El sistema posee columnas linealmente dependientes por defecto. Las variables libres arrojan infinitas soluciones viables.\")\ntry:\n    np.linalg.inv(M_deficiente.T @ M_deficiente)\nexcept np.linalg.LinAlgError as e:\n    print(f\"\\n&gt;&gt; VALIDADO: linalg.inv() dispar\u00f3 la protecci\u00f3n matem\u00e1tica LinAlgError -&gt; {e}.\")\n    print(\"Imposible hallar soluci\u00f3n \u00fanica param\u00e9trica con menos de 3 puntos, reafirmando lo resuelto anal\u00edticamente.\")\n</code></pre>"},{"location":"Examen_2025_07_21/01_svd/teoria/","title":"Soluci\u00f3n del Ejercicio 1 (Examen 21 de julio de 2025 - SVD)","text":"<p>Ejercicio 1. Sea \\(A\\) una matriz con coeficientes reales de \\(n \\times 2\\). Sean \\(U\\), \\(\\Sigma\\) y \\(V\\) las matrices que dan su descomposici\u00f3n SVD, con \\(u_i\\) la columna \\(i\\)-\u00e9sima de \\(U\\), \\(\\Sigma_{ii} = \\sigma_i\\) (con \\(\\sigma_i \\neq \\sigma_j\\) si \\(i \\neq j\\)), y \\(v_i\\) la columna \\(i\\)-\u00e9sima de \\(V\\). Sea \\(\\tilde{A} = \\sigma_1 u_1 v_1^t\\) una aproximaci\u00f3n de rango 1 de \\(A\\).</p> <p>a) Si \\(x \\in \\mathbb{R}^2\\) es un vector perteneciente al c\u00edrculo unitario, mostrar que el error cometido al calcular \\(Ax\\) como \\(\\tilde{A}x\\) est\u00e1 acotado por \\(\\sigma_2\\).</p> <p>b) Sea \\(B = A^tA\\) y \\(x \\in \\mathbb{R}^2\\) elegido al azar. Mostrar que el siguiente algoritmo converge al vector \\(v_1\\) cuando \\(N \\to \\infty\\):</p> <ul> <li> <p>Para \\(k \\in 1, \\dots, N\\):</p> </li> <li> <p>\\(x = Bx\\)</p> </li> <li> <p>\\(x = x / ||x||\\)</p> </li> </ul> <p>c) Escriba una rutina que calcule la mejor aproximaci\u00f3n de rango 1 de una matriz real de \\(n \\times 2\\) en el sentido de la norma 2. Toda funci\u00f3n que involucre operaciones m\u00e1s complejas que el producto matricial debe ser definida expl\u00edcitamente.</p>"},{"location":"Examen_2025_07_21/01_svd/teoria/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del enunciado","text":"<p>Dado que la matriz \\(A\\) es de dimensiones \\(n \\times 2\\), su descomposici\u00f3n SVD \\(A = U \\Sigma V^t\\) nos indica rigurosamente, por propiedades algebraicas de dimensi\u00f3n, que:</p> <ul> <li> <p>\\(U\\) es una matriz ortogonal de \\(n \\times n\\), por ende sus vectores columna la componen como \\(U = [u_1, u_2, \\dots, u_n]\\).</p> </li> <li> <p>\\(V\\) es una matriz ortogonal de \\(2 \\times 2\\), ergo se compone acotadamente como \\(V = [v_1, v_2]\\).</p> </li> <li> <p>\\(\\Sigma\\) es una matriz de \\(n \\times 2\\) (mismas dimensiones que \\(A\\)) que alberga a los valores singulares \\(\\sigma_i\\) estrictamente a lo largo de su \"diagonal principal\" (donde el \u00edndice de fila coincide con el de columna, \\(\\Sigma_{ii}\\)). En consecuencia, todos los dem\u00e1s elementos que no pertenecen a esta diagonal son estrictamente nulos (\\(\\Sigma_{ij} = 0\\) para todo \\(i \\neq j\\)). Esta es una propiedad basal inquebrantable de la SVD. A la hora de iterar, notamos que todas las filas contenidas entre \\(n=3\\) hasta la \u00faltima (\\(n\\)) ser\u00e1n filas llenas enteramente de ceros.</p> </li> <li> <p>La expresi\u00f3n \\(\\tilde{A} = \\sigma_1 u_1 v_1^t\\) se denomina aproximaci\u00f3n de rango 1. El Teorema de Eckart-Young-Mirsky (fuente matem\u00e1tica) en \u00e1lgebra lineal demuestra infaliblemente que al truncar la SVD reteniendo \u00fanicamente el o los mayores valores singulares, se obtiene la matriz \"m\u00e1s cercana\" posible a la original minimizando el margen de error, en el sentido de la Norma Rectangular Espectral (Norma-2) y de Frobenius. Por ende, la conjunci\u00f3n de los mayores vectores singulares \\(\\sigma_1 u_1 v_1^t\\) conforma la proyecci\u00f3n hiper-dimensional estricta de mayor asertividad para explicar la matriz general reduci\u00e9ndola a un solo espectro principal (un solo vector-base).</p> </li> </ul>"},{"location":"Examen_2025_07_21/01_svd/teoria/#solucion-inciso-a","title":"Soluci\u00f3n Inciso A","text":"<p>a) Si \\(x \\in \\mathbb{R}^2\\) es un vector perteneciente al c\u00edrculo unitario, mostrar que el error cometido al calcular \\(Ax\\) como \\(\\tilde{A}x\\) est\u00e1 acotado por \\(\\sigma_2\\).</p> <p>Sabemos por definici\u00f3n de la SVD truncada que como la matriz original \\(A\\) es de orden \\(n \\times 2\\), s\u00f3lo poseer\u00e1 a lo sumo 2 valores singulares no nulos. Podemos entonces descomponerla como la suma de sus componentes de rango 1:</p> \\[A = \\sum_{i=1}^{k} \\sigma_i u_i v_i^t = \\sigma_1 u_1 v_1^t + \\sigma_2 u_2 v_2^t\\] Profundizaci\u00f3n: Descomposici\u00f3n en Sumatoria de Matrices de Rango 1 <p>Para cualquier matriz general \\(M \\in \\mathbb{R}^{m \\times n}\\) con rango \\(r\\), la Descomposici\u00f3n en Valores Singulares \\(M = U \\Sigma V^t\\) puede ser reescrita l\u00f3gicamente usando multiplicaci\u00f3n por bloques como una sumatoria exacta de \\(r\\) matrices individuales, cada una de rango estrictamente 1:</p> \\[M = \\sum_{i=1}^{r} \\sigma_i u_i v_i^t\\] <p>Teniendo en cuenta que \\(u_i\\) y \\(v_i\\) son vectores columna, cada t\u00e9rmino individual \\(u_i v_i^t\\) (el producto exterior u outer product entre el \\(i\\)-\u00e9simo vector singular izquierdo y su equivalente derecho interpuesto) arroja una matriz bidimensional completa de \\(m \\times n\\), pero de riguroso rango 1 (al ser la multiplicaci\u00f3n cruzada de meras vectores columnas lineales, todas las columnas de la matriz resultante terminan siendo m\u00faltiplos de una \u00fanica columna \\(u_i\\)).</p> <p>Al escalarla individualmente por su respectivo valor singular \\(\\sigma_i\\) (que act\u00faa como el \"peso\" escalado o la magnificaci\u00f3n de esa componente a nivel espectral), la suma de estas \"capas\" de rango 1 superpuestas reconstruye milim\u00e9tricamente la integralidad de \\(M\\), ponderando y priorizando los elementos de mayor dominancia (las direcciones singulares principales).</p> <p>\ud83d\udccc Para verificar visualmente la demostraci\u00f3n matem\u00e1tica en pizarra impartida desde cero, pod\u00e9s mirar la Clase 29 (Singular Value Decomposition) dictada por Gilbert Strang para MIT 18.06 OpenCourseWare.</p> <p>Dado que se nos informa que \\(\\tilde{A} = \\sigma_1 u_1 v_1^t\\) es la aproximaci\u00f3n de mayor rango, podemos definir el error vectorial de efectuar dicha predicci\u00f3n como \\(e = A x - \\tilde{A} x\\).</p> <p>Reemplazando los t\u00e9rminos, el residuo es exactamente la componente descartada de la matriz SVD:</p> \\[e = (A - \\tilde{A}) x = (\\sigma_2 u_2 v_2^t) x\\] <p>Nos piden probar que la norma de este error se encuentra acotada por \\(\\sigma_2\\). Aplicamos la norma euclidiana o Norma-2 (\\(|| \\cdot ||_2\\)) en ambos lados:</p> \\[||e||_2 = ||\\sigma_2 u_2 v_2^t x||_2\\] <p>Como un escalar positivo puede extraerse de la norma:</p> Observaci\u00f3n Te\u00f3rica: \u00bfPor qu\u00e9 \\(\\sigma_2\\) es un escalar puramente positivo? <p>Por definici\u00f3n intr\u00ednseca de la descomposici\u00f3n SVD, todos los valores singulares escalares \\(\\sigma_i\\) que componen a \\(\\Sigma\\) son siempre n\u00fameros reales no negativos (\\(\\sigma_i \\geq 0\\)). Matem\u00e1ticamente, esto deviene de que los valores \\(\\sigma\\) se calculan extrayendo la ra\u00edz cuadrada de los autovalores algebraicos de la matriz gramiana \\(A^tA\\).</p> <p>Toda matriz pre-multiplicada por su transpuesta (\\(A^tA\\)) genera autom\u00e1ticamente una matriz sim\u00e9trica semi-definida positiva. Las matrices de este tipo subyacen inevitablemente a un espectro limitante de autovalores reales y positivos (\\(\\lambda_i \\geq 0\\)), imposibilitando en la abstracci\u00f3n la existencia de ra\u00edces imaginarias o valores singulares que fuesen negativos.</p> <p>Adicionalmente, como el enunciado aclara que los valores singulares no admiten repetici\u00f3n (\\(\\sigma_i \\neq \\sigma_j\\)) y provienen del habitual ordenamiento secular de magnitud descendente \\(\\sigma_1 &gt; \\sigma_2 &gt; 0\\), concluimos fehacientemente que \\(\\sigma_2 &gt; 0\\). </p> <p>Al refrendar que es un escalar puramente positivo para todo escenario, nuestra expresi\u00f3n queda habilitada l\u00edcitamente para desacoplar a \\(\\sigma_2\\) por fuera de la funci\u00f3n valor absoluto de la norma m\u00e9trica subyacente de la que era parte impunemente: \\(|\\sigma_2| = \\sigma_2\\) y \\(||\\sigma_2 u|| = \\sigma_2 ||u||\\).</p> <p>\ud83d\udccc Para consultar la probanza algebraica oficial de estas propiedades imperativas, pod\u00e9s remitirte a la Clase 27 (Positive Definite Matrices and Minima) dictada por Gilbert Strang para MIT 18.06 OpenCourseWare.</p> \\[||e||_2 = \\sigma_2 ||u_2 (v_2^t x)||_2\\] <p>El t\u00e9rmino entre par\u00e9ntesis \\((v_2^t x)\\) resulta en un n\u00famero escalar del producto interno vectorial. Lo podemos extraer en valor absoluto:</p> \\[||e||_2 = \\sigma_2 |v_2^t x| \\cdot ||u_2||_2\\] <p>Dado que las matrices de la SVD componen isometr\u00edas ortogonales perfectas, \\(U\\) y \\(V\\) se componen de vectores columna ortonormales unitarios. Es una certeza, por ende, que el vector singular de la izquierda \\(u_2\\) posee norma estricta igual a 1 (\\(||u_2||_2 = 1\\)):</p> \\[||e||_2 = \\sigma_2 |v_2^t x|\\] <p>Al observar minuciosamente la parte restante correspondiente al producto interno \\(|v_2^t x|\\), descubrimos que ambos vectores provienen del disco estandarizado: sabemos que \\(x \\in \\mathbb{R}^2\\) con \\(||x||_2 = 1\\) (nos dicen que pertenece al \"c\u00edrculo unitario\") y paralelamente \\(v_2\\) siempre es un vector ortonormal de \\(V\\), por lo cual \\(||v_2||_2 = 1\\).</p> <p>Recurriendo a la c\u00e9lebre inecuaci\u00f3n de Cauchy-Schwarz:</p> \\[|v_2^t x| \\leq ||v_2||_2 \\cdot ||x||_2 = 1 \\cdot 1 = 1\\] <p>Consumiendo nuestra afirmaci\u00f3n, arribamos a que:</p> \\[||e||_2 \\leq \\sigma_2 \\cdot 1 = \\sigma_2\\] <p>Queda as\u00ed demostrado que el error cometido en la aproximaci\u00f3n \\(\\|A x - \\tilde{A} x\\|_2\\) siempre estar\u00e1 acotado por arriba por la magnitud del segundo valor singular descartado, \\(\\sigma_2\\).</p>"},{"location":"Examen_2025_07_21/01_svd/teoria/#solucion-inciso-b","title":"Soluci\u00f3n Inciso B","text":"<p>b) Sea \\(B = A^tA\\) y \\(x \\in \\mathbb{R}^2\\) elegido al azar. Mostrar que el siguiente algoritmo converge al vector \\(v_1\\) cuando \\(N \\to \\infty\\):</p> <ul> <li> <p>Para \\(k \\in 1, \\dots, N\\):</p> </li> <li> <p>\\(x = Bx\\)</p> </li> <li> <p>\\(x = x / ||x||\\)</p> </li> </ul> <p>Si \\(A = U \\Sigma V^t\\), podemos sustituirlo en lo provisto por la letra del ejercicio \\(B = A^t A\\):</p> \\[B = (U \\Sigma V^t)^t (U \\Sigma V^t) = V \\Sigma^t U^t U \\Sigma V^t\\] <p>Dada la ortogonalidad de la matriz \\(U\\), sabemos que su autotranspuesta obedece \\(U^t U = I\\). Aplicando este axioma simplificador:</p> \\[B = V \\Sigma^t \\Sigma V^t\\] <p>Dado que la matriz pre-multiplicada \\(\\Sigma^t \\Sigma\\) conforma invariablemente una matriz diagonal cuadrada en la que surgen iterativamente los valores singulares originales elevados al cuadrado (\\(\\Sigma_{ii}^2 = \\sigma_i^2\\)), entonces los autovalores formales de \\(B\\) (que recordemos es de \\(2 \\times 2\\)), llamados convencionalmente \\(\\lambda_1\\) y \\(\\lambda_2\\), son por a\u00f1adidura:</p> \\[\\lambda_1 = \\sigma_1^2\\] \\[\\lambda_2 = \\sigma_2^2\\] <p>Los autovectores de \\(B\\) son precisamente las columnas de la matriz \\(V\\), denotados como \\(v_1\\) y \\(v_2\\).</p> Observaci\u00f3n Te\u00f3rica: \u00bfPor qu\u00e9 los autovectores de \\(B\\) son las columnas de \\(V\\)? <p>A partir de la ecuaci\u00f3n superior deducimos que \\(B = V (\\Sigma^t \\Sigma) V^t\\). Si a la matriz diagonal central la rebautizamos como \\(\\Lambda = \\Sigma^t \\Sigma\\), nos queda formulado:</p> \\[B = V \\Lambda V^t\\] <p>Por definici\u00f3n de la SVD, sabemos que \\(V\\) conforma una matriz ortogonal perfecta. Las matrices ortogonales gozan de la propiedad inversa elemental en la que \\(V^t = V^{-1}\\). Sustituyendo esto en la ecuaci\u00f3n:</p> \\[B = V \\Lambda V^{-1}\\] <p>Esta gloriosa disposici\u00f3n coincide sim\u00e9tricamente con la definici\u00f3n can\u00f3nica universal de la Diagonalizaci\u00f3n de Matrices por Autovalores (\\(M = P D P^{-1}\\)), donde el teorema espectral dicta irrefutablemente que \\(D\\) (nuestra \\(\\Lambda\\)) es la matriz diagonal que aloja de forma descendente los autovalores, y \\(P\\) (nuestra \\(V\\)) es la matriz de paso cuyas columnas albergan los autovectores ortonormalizados linealmente independientes correspondientes a cada escal\u00f3n de \\(\\Lambda\\).</p> <p>Por alineaci\u00f3n axiom\u00e1tica directa, las columnas de \\(V\\) (\\(v_1, v_2\\)) son sin lugar a dudas los autovectores de la matriz sim\u00e9trica \\(B\\).</p> <p>\ud83d\udccc Para verificar visualmente la demostraci\u00f3n completa del Teorema Espectral y el mecanismo de diagonalizaci\u00f3n \\(\\Lambda\\), te sugiero consultar la Clase 22 (Diagonalization and Powers of A) dictada por Gilbert Strang para MIT 18.06 OpenCourseWare.</p> <p>En base a esto, y conociendo que los valores singulares de SVD exigen que \\(\\sigma_i \\neq \\sigma_j\\) y vienen t\u00edpicamente ordenados descendiendo \\(\\sigma_1 &gt; \\sigma_2 &gt; 0\\), deducimos que \\(\\lambda_1 &gt; \\lambda_2 \\geq 0\\).</p> Observaci\u00f3n Te\u00f3rica: \u00bfEl orden estricto \\(\\sigma_1 &gt; \\sigma_2 &gt; 0\\) es una convenci\u00f3n algor\u00edtmica? <p>S\u00ed, la suposici\u00f3n de que los valores \\(\\sigma\\) yacen ordenados algebraicamente de mayor a menor magnitud (\\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots\\)) es la convenci\u00f3n universal est\u00e1ndar en todas las bibliotecas de c\u00f3mputo inform\u00e1tico (numpy, scipy) y en la formulaci\u00f3n primigenia de la Descomposici\u00f3n SVD.</p> <p>La SVD est\u00e1 dise\u00f1ada axiom\u00e1ticamente para reordenar las proyecciones de modo que el primer valor \\(\\sigma_1\\) sea siempre el componente supremo, englobando la direcci\u00f3n de m\u00e1xima varianza (o energ\u00eda matricial principal).</p> <p>En el contexto estricto de nuestro ejercicio, el enunciado nos decreta preventivamente que \\(\\sigma_i \\neq \\sigma_j\\) si \\(i \\neq j\\). Esta condici\u00f3n suplementaria impuesta por el autor anula la posibilidad de que surja multiplicidad en los valores (el caso degenerado donde \\(\\sigma_1 = \\sigma_2\\)). </p> <p>Por consiguiente, la fusi\u00f3n natural de la convenci\u00f3n descendente gen\u00e9rica de la SVD \\((\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0)\\) intersectada con la restricci\u00f3n estricta de desigualdad del examen \\((\\sigma_1 \\neq \\sigma_2)\\), nos conduce fehacientemente y sin fisuras anal\u00edticas a que la sucesi\u00f3n es estrictamente decreciente: \\(\\sigma_1 &gt; \\sigma_2 &gt; 0\\).</p> <p>\ud83d\udccc Para verificar la convenci\u00f3n doctrinal y matem\u00e1tica detr\u00e1s del ordenamiento matricial descendente de la SVD, pod\u00e9s consultar la Wikipedia: Singular Value Decomposition (Statement of the theorem).</p> <p>El algoritmo planteado eval\u00faa un simple bucle \\(k \\in 1, \\dots, N\\) sobre la operaci\u00f3n iterada:</p> \\[x^{(k)} = \\frac{B x^{(k-1)}}{||B x^{(k-1)}||} = \\frac{B^k x^{(0)}}{||B^k x^{(0)}||}\\] Observaci\u00f3n Te\u00f3rica: \u00bfPor qu\u00e9 la iteraci\u00f3n asume esta forma colapsada general? <p>El mecanismo fundamental del M\u00e9todo de la Potencia normalizado a menudo confunde porque en cada paso iterativo se divide por la norma entera del vector obtenido, pareciendo perder el hilo del vector primigenio \\(x^{(0)}\\). No obstante, podemos probar por inducci\u00f3n que esto es un simple cambio de escala unidimensional:</p> <ol> <li>En el paso 1, multiplicamos e inyectamos la norma: \\(x^{(1)} = \\frac{B x^{(0)}}{||B x^{(0)}||}\\).</li> <li>En el paso 2, insertamos \\(x^{(1)}\\): \\(x^{(2)} = \\frac{B x^{(1)}}{||B x^{(1)}||} = \\frac{B \\left( \\frac{B x^{(0)}}{||B x^{(0)}||} \\right)}{||B \\left( \\frac{B x^{(0)}}{||B x^{(0)}||} \\right)||}\\).</li> </ol> <p>Dado que los denominadores son estrictamente escalares num\u00e9ricos reales (\\(c = ||B x^{(0)}||\\)), las propiedades de linealidad de normas y matrices nos permiten sacar los escalares afuera tanto del numerador matricial como de la norma del denominador (\\(B(c v) = c B v\\) y \\(||c v|| = |c| ||v||\\)). </p> <p>Al sacarlo de ambos lados simult\u00e1neamente, el escalar acumulativo del paso anterior se cancela exactamente a s\u00ed mismo en el cociente:</p> \\[x^{(2)} = \\frac{\\frac{1}{||B x^{(0)}||} \\cdot B^2 x^{(0)}}{\\frac{1}{||B x^{(0)}||} \\cdot ||B^2 x^{(0)}||} = \\frac{B^2 x^{(0)}}{||B^2 x^{(0)}||}\\] <p>Efectuando este patr\u00f3n colapsable iterativamente \\(k\\) veces (todos los divisores escalares de los pasos intermedios nacen y mueren mutuamente cancelados por linealidad), arribamos a la inmaculada conclusi\u00f3n de que sin importar cu\u00e1ndo o cu\u00e1ntas veces re-normalicemos el vector a magnitud 1 durante el bucle de For, la direcci\u00f3n espacial que apunta \\(x^{(k)}\\) proviene indefectiblemente de elevar emp\u00edricamente a \\(B\\) a la potencia \\(k\\) desde el inicio (\\(B^k x^{(0)}\\)) y dividir todo ese armatoste final por su propia norma universal (\\(||B^k x^{(0)}||\\)) reci\u00e9n al terminar.</p> <p>\ud83d\udccc Para consultar la demostraci\u00f3n inductiva detallada y su verificaci\u00f3n emp\u00edrica en Python, pod\u00e9s remitirte al M\u00e9todo de la Potencia.</p> <p>Se nos explica que \\(x \\in \\mathbb{R}^2\\) es elegido al azar, por lo que podemos representarlo en funci\u00f3n de la base ortonormal completa del plano compuesto por \\(v_1\\) y \\(v_2\\):</p> \\[x^{(0)} = c_1 v_1 + c_2 v_2\\] Observaci\u00f3n Te\u00f3rica: \u00bfPor qu\u00e9 \\(v_1\\) y \\(v_2\\) forman una Base Ortonormal completa para \\(\\mathbb{R}^2\\)? <p>En la descomposici\u00f3n SVD original (\\(A = U \\Sigma V^t\\)), la matriz \\(V\\) tiene dimensiones \\(2 \\times 2\\). Por los teoremas angulares fundamentales de la SVD, sabemos inquebrantablemente que \\(V\\) es una Matriz Ortogonal cuadrada.</p> <p>Toda matriz ortogonal cuadrada posee un espectro de vectores columna que son, por rigor axiom\u00e1tico, mutuamente ortogonales (su producto punto interno es cero consecutivo, \\(v_1 \\cdot v_2 = 0\\)) y de norma unitaria (\\(||v_i||_2 = 1\\)).</p> <p>El espacio vectorial \\(\\mathbb{R}^2\\) (el \"plano\") tiene por definici\u00f3n dimensi\u00f3n 2. Como \\(v_1\\) y \\(v_2\\) son 2 vectores linealmente independientes (atestado irrefutablemente por ser ortogonales entre s\u00ed), constituyen un conjunto maximalmente extenso para este espacio dimensional. </p> <p>Al tener la misma cantidad de vectores ortogonales que dimensiones tiene el espacio, se erigen autom\u00e1ticamente como una Base Ortonormal Completa. Por lo tanto, cualquier vector misterioso extra\u00eddo al azar de \\(\\mathbb{R}^2\\) (como nuestro \\(x^{(0)}\\)), puede ser modelado sint\u00e1ctica y perfectamente a trav\u00e9s de una combinaci\u00f3n lineal bi-factorial directa de esta suprema \"br\u00fajula\" param\u00e9trica: \\(x^{(0)} = c_1 v_1 + c_2 v_2\\).</p> <p>Aplicando el \u00e1lgebra del operador iterativo con iteraciones tendientes a \\(\\infty\\):</p> \\[B^k x^{(0)} = c_1 \\lambda_1^k v_1 + c_2 \\lambda_2^k v_2\\] Observaci\u00f3n Te\u00f3rica: \u00bfC\u00f3mo opera \\(B^k\\) iterativamente sobre la base de autovectores? <p>Cuando aplicamos la matriz \\(B\\) sucesivas veces sobre el vector inicial, estamos efectuando la operaci\u00f3n \\(B^k x^{(0)}\\).</p> <p>Por las propiedades definitorias del problema de autovalores, sabemos que pre-multiplicar la matriz \\(B\\) por cualquiera de sus autovectores (\\(v_i\\)) da un resultado vectorial id\u00e9ntico a simplemente multiplicar ese vector por su escalar asimilado (\\(\\lambda_i\\)). En otras palabras, la matriz se comporta como un n\u00famero y solo genera un estiramiento unidimensional, sin rotarlo en el espacio: \\(B v_i = \\lambda_i v_i\\).</p> <p>Si imponemos esta transformaci\u00f3n \\(k\\) veces sucesivas (\"potencias de la matriz\"), los factores de escala escalatorios crecen naturalmente de manera exponencial: \\(B^k v_i = \\lambda_i^k v_i\\).</p> <p>Sustituyendo en esta expresi\u00f3n nuestro vector original partido en componentes del plano:</p> \\[B^k x^{(0)} = B^k (c_1 v_1 + c_2 v_2)\\] <p>Propagando la transformaci\u00f3n de la matriz por distributiva matricial pura:</p> \\[B^k x^{(0)} = c_1 (B^k v_1) + c_2 (B^k v_2)\\] <p>Sustituyendo la conducta matricial proyectada en las componentes individuales para desacoplarnos matem\u00e1ticamente del producto matricial y transformarlo en escalares algebraicos:</p> \\[B^k x^{(0)} = c_1 \\lambda_1^k v_1 + c_2 \\lambda_2^k v_2\\] <p>\ud83d\udccc El basamento doctrinario supremo para asimilar c\u00f3mo una matriz iterativa se deconstruye y expande sus propios autovalores en el tiempo es ilustrado majestuosamente en la Clase 22 (Diagonalization and Powers of A) - MIT 18.06 Linear Algebra, Fall 2005 por Gilbert Strang.</p> <p>Factorizando para independizarnos del exponente del autovalor dominante:</p> \\[B^k x^{(0)} = \\lambda_1^k \\left( c_1 v_1 + c_2 \\left( \\frac{\\lambda_2}{\\lambda_1} \\right)^k v_2 \\right)\\] <p>Dado que denotamos ex-ante que \\(\\lambda_1 &gt; \\lambda_2\\), la fracci\u00f3n es decididamente un n\u00famero en el recuadro menor a la unidad \\(( \\frac{\\lambda_2}{\\lambda_1} &lt; 1 )\\). Por propiedad de los l\u00edmites asint\u00f3ticos exponenciales:</p> \\[\\lim_{k \\to \\infty} \\left( \\frac{\\lambda_2}{\\lambda_1} \\right)^k = 0\\] <p>Al aproximarse velozmente todo el segundo t\u00e9rmino aditivo al valor cero, obtenemos con precisi\u00f3n que la iteraci\u00f3n de nuestro vector de partida se alinear\u00e1 siendo puramente colineal con el primer autovector:</p> \\[B^k x^{(0)} \\approx \\left( \\lambda_1^k \\cdot c_1 \\right) v_1\\] <p>Evidentemente si nuestro vector estoc\u00e1stico naci\u00f3 sin componente principal (\\(c_1 = 0\\), ortogonalidad perfecta elegida para nuestra semilla inicial), el proceso fallar\u00e1 al converger a \\(v_2\\).</p> Observaci\u00f3n Te\u00f3rica: \u00bfQu\u00e9 ocurre matem\u00e1ticamente si \\(c_1 = 0\\)? \u00bfEl vector se vuelve nulo? <p>Al notar que \\(\\left( \\frac{\\lambda_2}{\\lambda_1} \\right)^k \\to 0\\), es tentador intuir que si \\(c_1 = 0\\) todo el conjunto colapsar\u00e1 hacia el vector \\((0,0)\\). No obstante, la factorizaci\u00f3n matem\u00e1tica extrayendo un factor com\u00fan de \\(\\lambda_1^k\\) se dise\u00f1a y aporta valor \u00fanicamente para analizar el l\u00edmite cuando ambas componentes coexisten (\\(c_1 \\neq 0\\)).</p> <p>Si verdaderamente tuvi\u00e9semos asilamiento puro de \\(c_1 = 0\\), debemos analizar la sumatoria original intacta:</p> \\[B^k x^{(0)} = 0 \\cdot \\lambda_1^k v_1 + c_2 \\lambda_2^k v_2 = c_2 \\lambda_2^k v_2\\] <p>Dado que en el M\u00e9todo de la Potencia el paso es normalizar forzosamente \\(x^{(k)} = \\frac{B^k x^{(0)}}{||B^k x^{(0)}||}\\), no importa cu\u00e1n microsc\u00f3pico se torne el escalar \\(\\lambda_2^k \\to 0\\) con el avance del tiempo, al estar en el numerador y denominador sometido bajo norma \u00e9ste se cancela intr\u00ednsecamente:</p> \\[x^{(k)} = \\frac{c_2 \\lambda_2^k v_2}{||c_2 \\lambda_2^k v_2||} = \\frac{c_2 \\lambda_2^k}{|c_2| \\lambda_2^k} \\cdot \\frac{v_2}{||v_2||} = \\text{sgn}(c_2) v_2\\] <p>Por consiguiente, el algoritmo jam\u00e1s tender\u00e1 al origen \\((0,0)\\); la normalizaci\u00f3n iterativa act\u00faa como un ant\u00eddoto perpetuo que estirar\u00e1 de vuelta a cualquier remanente hacia el c\u00edrculo unitario (norma 1), quedando estacionado inamoviblemente en \\(\\pm v_2\\).</p> <p>Tal salvedad, en algoritmos estoc\u00e1sticos que operan en floats (cuyos conjuntos de medida afirman que extraer el exacto plano euclidiano ortogonal de \\(v_1\\) al azar tiene te\u00f3ricamente \"probabilidad cero\"), carece de sustento para descalificar la iteraci\u00f3n, ya que cualquier ruido de precisi\u00f3n \u00ednfimo garantiza que nazca un \\(c_1 \\neq 0\\) que indefectiblemente terminar\u00e1 dominando el l\u00edmite asint\u00f3tico.</p> <p>Como para rematar cada ciclo el vector se normaliza contra s\u00ed mismo \\(x = \\frac{x}{||x||}\\), todo componente escalar global \\(\\lambda\\) decae por divisiones intr\u00ednsecas, dejando con exclusividad un vector de tama\u00f1o 1 alineado a la direcci\u00f3n principal:</p> \\[\\lim_{N \\to \\infty} x^{(N)} = \\pm v_1\\] <p>Evidenciando la validez asint\u00f3tica te\u00f3rica de lo que universalmente nombramos \"M\u00e9todo de la Potencia\".</p>"},{"location":"Examen_2025_07_21/01_svd/teoria/#solucion-inciso-c-rutina-en-python","title":"Soluci\u00f3n Inciso C: Rutina en Python","text":"<p>c) Escriba una rutina que calcule la mejor aproximaci\u00f3n de rango 1 de una matriz real de \\(n \\times 2\\) en el sentido de la norma 2. Toda funci\u00f3n que involucre operaciones m\u00e1s complejas que el producto matricial debe ser definida expl\u00edcitamente.</p> <p>A continuaci\u00f3n, la rutina desarrollada sin emplear funciones de grado superlativo como factorizaciones directas en la librer\u00eda algor\u00edtmica. Para hallar el valor singular predominante nos basamos en el cociente de Rayleigh, aplicando producto punto cl\u00e1sico entre la iteraci\u00f3n convergiendo de las potencias.</p> <pre><code>import numpy as np\n\ndef run_tests():\n    print(\"--- Verificaci\u00f3n Inciso A ---\")\n    np.random.seed(42)  # Para la consistencia l\u00f3gica de todo el script\n    n = 5\n    A = np.random.randn(n, 2)\n    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n    sigma_1, sigma_2 = S[0], S[1]\n\n    # Construcci\u00f3n referencial de la aproximaci\u00f3n de rango 1\n    u1 = U[:, 0].reshape(-1, 1)\n    v1 = Vt[0, :].reshape(-1, 1)\n    A_tilde = sigma_1 * (u1 @ v1.T)\n\n    # Simular vector aleatorio unitario probabil\u00edstico (C\u00edrculo Unitario)\n    theta = np.random.uniform(0, 2 * np.pi)\n    x = np.array([[np.cos(theta)], [np.sin(theta)]])\n\n    # Efectuar la predicci\u00f3n restada a la original para hallar residuo\n    error = np.linalg.norm(A @ x - A_tilde @ x, ord=2)\n    print(f\"Error calculado euclidiamente ||Ax - A_tilda x||_2:\\t {error:.4f}\")\n    print(f\"Cota superior l\u00edmite exigida anal\u00edticamente (sigma_2):\\t {sigma_2:.4f}\")\n    print(f\"\u00bfAtiende la aserci\u00f3n te\u00f3rica? (Error &lt;= sigma_2):\\t {error &lt;= sigma_2}\")\n\n\n    print(\"\\n--- Verificaci\u00f3n Inciso B y Soluci\u00f3n Emp\u00edrica Inciso C ---\")\n\n    B = A.T @ A\n\n    # Rutina iterativa solicitada en el inciso C, donde la norma expl\u00edcita no puede usar linalg.norm\n    def best_rank_1_approximation(mat_A, num_iterations=100):\n        \"\"\"\n        Rutina puramente axiom\u00e1tica fundamentada en matrices elementales sin svd.\n        Halla la aproximaci\u00f3n computacional limit\u00e1ndose al producto de matrices algebraico y potencias.\n        \"\"\"\n        mat_B = mat_A.T @ mat_A\n\n        # Iniciaci\u00f3n del vector aleatorio x (Inciso B)\n        x_k = np.random.randn(2, 1)\n\n        # Iteraci\u00f3n asint\u00f3tica (N -&gt; Infinito, para nosotros 100)\n        for _ in range(num_iterations):\n            x_k = mat_B @ x_k\n\n            # Normalizaci\u00f3n manual a nivel matricial (x[0]^2 + x[1]^2)^0.5\n            norm_xk = (x_k[0, 0]**2 + x_k[1, 0]**2)**0.5\n            x_k = x_k / norm_xk\n\n        v_1_approx = x_k\n\n        # Hallar Rayleigh quotient para aproximar lambda_1\n        # Lambda_1 = (v_1^T B v_1) / (v_1^T v_1) =&gt; al ser v_1 unitario, omitimos la divisi\u00f3n.\n        lambda_1 = (v_1_approx.T @ (mat_B @ v_1_approx))[0, 0]\n\n        # Recuperar par\u00e1metro principal sigma_1 como la ra\u00edz limitante est\u00e1tica de la varianza espectral\n        sigma_1_approx = lambda_1**0.5\n\n        # Recobrar u_1 aislando el vector matricial en la relaci\u00f3n (A v_1 = sigma_1 u_1)\n        u_1_approx = (mat_A @ v_1_approx) * (1.0 / sigma_1_approx)\n\n        # Compilar y emparejar la expresi\u00f3n final A_tilde\n        A_tilde_approx = sigma_1_approx * (u_1_approx @ v_1_approx.T)\n\n        return A_tilde_approx, v_1_approx\n\n    A_tilde_approx, v1_approx = best_rank_1_approximation(A, 100)\n\n    v1_true = Vt[0, :].reshape(-1, 1)\n    # El producto interno entre dos vectores unitarios paralelos es 1 o -1\n    dot_product = float(abs(v1_approx.T @ v1_true)[0, 0])\n    print(f\"Producto interno abs(v1_approx^T * v1_true):\\t\\t {dot_product:.6f} (Demuestra el l\u00edmite en Inciso B)\")\n\n    print(\"\\n--- Discrepancia Global entre Matrices A_tilde ---\")\n    divergencia_absoluta = np.sum(np.abs(A_tilde - A_tilde_approx))\n    print(f\"Desfase posicional absoluto entre nuestra Rutina y la SVD natural: {divergencia_absoluta:.8e}\")\n    print(\"\u00bfSon matem\u00e1ticamente equivalentes sin discrepancia del hardware? (np.allclose):\", np.allclose(A_tilde, A_tilde_approx))\n\nif __name__ == \"__main__\":\n    run_tests()\n</code></pre>"},{"location":"Examen_2025_07_21/02_diagonalizacion/teoria/","title":"Soluci\u00f3n del Ejercicio 2 (Examen 21 de julio de 2025 - Diagonalizaci\u00f3n)","text":"<p>Ejercicio 2. Sea una matriz \\(A \\in \\mathbb{R}^{n \\times n}\\) con \\(n\\) autovalores mayores a cero y distintos entre s\u00ed. Sean \\(\\lambda_1, \\dots, \\lambda_n\\) sus autovalores y \\(v_1, \\dots, v_n\\) los autovectores asociados. Mostrar que:</p> <p>a) \\(\\{v_1, \\dots, v_n\\}\\) forma una base de \\(\\mathbb{R}^n\\). Justificar.</p> <p>b) La matriz \\(C \\in \\mathbb{R}^{n \\times n}\\) cuyas columnas est\u00e1n dadas por los vectores \\(v_1, \\dots, v_n\\) es inversible y cumple con que \\(AC = CS\\), con \\(S\\) una matriz diagonal con \\(\\lambda_1, \\dots, \\lambda_n\\) en la diagonal.</p> <p>c) La matriz \\(A\\) es diagonalizable.</p>"},{"location":"Examen_2025_07_21/02_diagonalizacion/teoria/#solucion-inciso-a","title":"Soluci\u00f3n Inciso A","text":"<p>a) \\(\\{v_1, \\dots, v_n\\}\\) forma una base de \\(\\mathbb{R}^n\\). Justificar.</p> <p>El teorema fundamental sobre autovectores establece que \"autovectores correspondientes a autovalores distintos son linealmente independientes\". </p> <p>Dado que por hip\u00f3tesis se nos confirma que la matriz \\(A\\) posee \\(n\\) autovalores estrictamente distintos entre s\u00ed (\\(\\lambda_i \\neq \\lambda_j\\) para todo \\(i \\neq j\\)), este lema nos garantiza de forma deductiva que el conjunto de sus correspondientes autovectores \\(\\{v_1, \\dots, v_n\\}\\) constituye un conjunto de exactamente \\(n\\) vectores linealmente independientes.</p> Demostraci\u00f3n Te\u00f3rica: Independencia Lineal por Autovalores Distintos <p>El porqu\u00e9 un \"abanico\" de autovalores distintos garantiza de forma obligatoria y deductiva que sus autovectores asociados no pueden colapsar formando dependencias espaciales, se demuestra axiom\u00e1ticamente mediante el Principio de Inducci\u00f3n Fuerte Matem\u00e1tica.</p> <p>\ud83d\udccc Para consultar paso a paso la justificaci\u00f3n anal\u00edtica y matem\u00e1tica detr\u00e1s de este Lema de Independencia (junto con su validador masivo estoc\u00e1stico en Python), puedes remitirte a: Demostraci\u00f3n: Independencia Lineal de Autovectores.</p> <p>Sabemos que cualquier conjunto de \\(n\\) vectores linealmente independientes dentro de un espacio vectorial eucl\u00eddeo de dimensi\u00f3n \\(n\\) (como es en este caso \\(\\mathbb{R}^n\\)) obligatoriamente genera dicho espacio (sirve como sistema generador) y, por consiguiente, forma inherentemente una Base. Queda justificado anal\u00edticamente.</p>"},{"location":"Examen_2025_07_21/02_diagonalizacion/teoria/#solucion-inciso-b","title":"Soluci\u00f3n Inciso B","text":"<p>b) La matriz \\(C \\in \\mathbb{R}^{n \\times n}\\) cuyas columnas est\u00e1n dadas por los vectores \\(v_1, \\dots, v_n\\) es inversible y cumple con que \\(AC = CS\\), con \\(S\\) una matriz diagonal con \\(\\lambda_1, \\dots, \\lambda_n\\) en la diagonal.</p> <p>A partir del inciso (A), hemos concluido que los autovectores \\(\\{v_1, \\dots, v_n\\}\\) estructuran una base de \\(\\mathbb{R}^n\\) y por tanto son independientes. La matriz columna unificada \\(C\\) se define en bloques como:</p> \\[C = \\begin{pmatrix} | &amp; | &amp; &amp; | \\\\ v_1 &amp; v_2 &amp; \\dots &amp; v_n \\\\ | &amp; | &amp; &amp; | \\end{pmatrix}\\] <p>Como sus columnas son vectores estrictamente linealmente independientes, su determinante no ser\u00e1 nulo y obligatoriamente existir\u00e1 su inversa (la matriz \\(C\\) es inversible / no singular). </p> Observaci\u00f3n Te\u00f3rica: \u00bfLos autovectores siempre son ortogonales entre s\u00ed? <p>No, rotundamente no. El Lema demostrado en el inciso anterior \u00fanicamente nos provey\u00f3 las garant\u00edas algebraicas de que los autovectores son Linealmente Independientes por provenir de ra\u00edces caracter\u00edsticas (autovalores) distintas. </p> <p>Que sean linealmente independientes significa que \"no son combinaci\u00f3n lineal entre s\u00ed\" y su span basta para cubrir las dimensiones del espacio, logrando por definici\u00f3n que el determinante de la matriz formada \\(C\\) sea distinto de cero (inversible).</p> <p>Sin embargo, la ortogonalidad (que formen \u00e1ngulos perfectos de 90\u00b0 o que su producto interno \\(v_i \\cdot v_j = 0\\)) es una propiedad de \u00e9lite reservada de manera exclusiva y rigurosa para las Matrices Sim\u00e9tricas Reales (por aplicaci\u00f3n del c\u00e9lebre Teorema Espectral). </p> <p>Para una matriz cuadrada \\(A\\) general asim\u00e9trica, sus autovectores construir\u00e1n firmemente una base para \\(\\mathbb{R}^n\\), pero en la inmensa mayor\u00eda de los casos ser\u00e1 una base oblicua (independientes pero no ortogonales).</p> <p>\ud83d\udccc Para constatar la rigurosidad conceptual de esta distinci\u00f3n vital (que la ortogonalidad se forja como propiedad exclusiva del Teorema Espectral ante matrices sim\u00e9tricas reales), cons\u00faltese la Clase 25 (Symmetric Matrices and Positive Definiteness) - Prof. Gilbert Strang (MIT 18.06 OpenCourseWare) o la Wikipedia: Spectral Theorem (Symmetric matrices).</p> <p>A continuaci\u00f3n debemos probar la aseveraci\u00f3n anal\u00edtica de igualdad. Evaluemos el producto en el miembro izquierdo \\(AC\\):</p> \\[AC = A \\begin{pmatrix} | &amp; &amp; | \\\\ v_1 &amp; \\dots &amp; v_n \\\\ | &amp; &amp; | \\end{pmatrix} = \\begin{pmatrix} | &amp; &amp; | \\\\ Av_1 &amp; \\dots &amp; Av_n \\\\ | &amp; &amp; | \\end{pmatrix}\\] <p>Por la naturaleza te\u00f3rica de un autovector asosciado a su respectivo autovalor, sustituimos que la transformaci\u00f3n sobre ella resulta en un reescalado hom\u00f3tetico por el propio autovalor \\(Av_i = \\lambda_i v_i\\):</p> \\[AC = \\begin{pmatrix} | &amp; &amp; | \\\\ \\lambda_1 v_1 &amp; \\dots &amp; \\lambda_n v_n \\\\ | &amp; &amp; | \\end{pmatrix}\\] <p>Por el otro flanco, evaluemos el miembro derecho mediante el producto de \\(C\\) por la matriz diagonal espectral \\(S = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)\\):</p> \\[CS = \\begin{pmatrix} | &amp; &amp; | \\\\ v_1 &amp; \\dots &amp; v_n \\\\ | &amp; &amp; | \\end{pmatrix} \\begin{pmatrix} \\lambda_1 &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; \\lambda_n \\end{pmatrix}\\] <p>La regla subyacente de la multiplicaci\u00f3n por derecha de una matriz escalar diagonal afirma que cada elemento multiplicar\u00e1 la columna entera de su misma posici\u00f3n indexada. Esto resulta, irrebatiblemente, en id\u00e9ntico resultado que nuestro primer paso matem\u00e1tico evaluado superiormente:</p> \\[CS = \\begin{pmatrix} | &amp; &amp; | \\\\ \\lambda_1 v_1 &amp; \\dots &amp; \\lambda_n v_n \\\\ | &amp; &amp; | \\end{pmatrix}\\] <p>Dado que ambos caminos algebraicos convergen a la misma matriz constituida por los vectores proyectados, se decreta que \\(AC = CS\\).</p>"},{"location":"Examen_2025_07_21/02_diagonalizacion/teoria/#solucion-inciso-c","title":"Soluci\u00f3n Inciso C","text":"<p>c) La matriz \\(A\\) es diagonalizable.</p> <p>En el inciso estipulado previamente demostramos que subsiste la igualdad operacional:</p> \\[AC = CS\\] <p>En ese mismo desarrollo inicializamos nuestra justificaci\u00f3n validando que \\(C\\) formaba una matriz con inversa existencial \\(C^{-1}\\). Con esta autoridad l\u00edcita, podemos optar por premultiplicar y despejar ambos extremos val\u00edendonos por derecha de dicha inversa:</p> \\[(AC)C^{-1} = (CS)C^{-1}\\] <p>Asumiendo la propiedad asociativa, \\(CC^{-1} = I\\) (siendo \\(I\\) la Matriz Identidad) liberando a \\(A\\):</p> \\[A = C S C^{-1}\\] <p>\u00bfQu\u00e9 significa algebraicamente que \\(A\\) pueda re-enunciarse como un sistema rec\u00edproco \\(C S C^{-1}\\)?. Por la definici\u00f3n ontol\u00f3gica expuesta en el \u00e1lgebra estructural matricial: una matriz cuadrada es diagonalizable si y s\u00f3lo si es semejante a una matriz diagonal.</p> <p>La igualdad demostrada verifica de manual dicha condici\u00f3n. Puesto que confirmamos que \\(A\\) est\u00e1 formulada por una matriz de pasaje de base en t\u00e1ndem con la matriz diagonal de sus autovalores (\\(S\\)), entonces la matriz A es incuestionablemente diagonalizable, ergo sus operaciones vectoriales sobre \\(\\mathbb{R}^n\\) se reducen a proyecciones re-escalables y ortogr\u00e1ficas en el subespacio rotado.</p>"},{"location":"Examen_2025_07_21/02_diagonalizacion/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef run_tests():\n    np.random.seed(84)\n    print(\"--- Demostrando las Propiedades de la Diagonalizaci\u00f3n ---\")\n\n    # Vamos a generar una Matriz A que garantice nuestros supuestos iniciales\n    # n autovalores &gt; 0 y distintos. (Vamos a fijar en este caso n=4)\n    n = 4\n\n    # 1. Definimos autovalores estrictamente positivos y dispares\n    lambdas_true = np.array([12.5, 7.8, 3.1, 1.2])\n    S_true = np.diag(lambdas_true)\n\n    # 2. Fabricamos una base C aleatoria que sea inversible\n    # (El teorema nos dice que autovalores dispares arman una base estoc\u00e1stica)\n    C_true = np.random.randn(n, n)\n\n    # 3. Formamos la matriz A en la naturaleza y ocultamos los par\u00e1metros\n    A = C_true @ S_true @ np.linalg.inv(C_true)\n\n    # --- INCISO A: Autovectores Base ---\n    # Usando el procesador para calcular los autovalores y autovectores en crudo que ve frente a A\n    lambdas_calc, C_calc = np.linalg.eig(A)\n\n    print(f\"\\nAutovalores re-calculados nativamente:\\n{lambdas_calc}\")\n    print(f\"\u00bfLos {n} valores son estrictamente mayores a 0?:\", np.all(lambdas_calc &gt; 0))\n    # Para ver si los vectores conforman una Base Rn, su matriz debe tener Rango Pleno (o Det != 0)\n    rango = np.linalg.matrix_rank(C_calc)\n    det_C = np.linalg.det(C_calc)\n    print(f\"\\nInciso A - \u00bfLos autovectores de C conforman base de R^{n}?\")\n    print(f\"Rango de la Matriz C calculada:\\t {rango} (Debe ser {n} para abarcar R^{n})\")\n    print(f\"Determinante |C| != 0:\\t\\t {det_C:.5f} != 0\")\n    print(\"Resultado: \u00c9xito formal probando Independencia Lineal.\")\n\n    # --- INCISO B: AC = CS ---\n    S_calc = np.diag(lambdas_calc)\n\n    LHS = A @ C_calc\n    RHS = C_calc @ S_calc\n\n    print(f\"\\nInciso B - Demostrando el Axioma Trascendental AC = CS\")\n    distancia = np.linalg.norm(LHS - RHS)\n    print(f\"Norma L2 del residuo (AC - CS) frente a la m\u00e1quina flotante: {distancia:.5e}\")\n    # Consideramos np.allclose porque matem\u00e1ticamente son iguales tras redondear\n    print(\"\u00bfSon funcionalmente la misma matriz? (np.allclose):\", np.allclose(LHS, RHS))\n\n    # --- INCISO C: Diagonalizabilidad (A = C S C^-1) ---\n    print(f\"\\nInciso C - Demostrando que Matrix es Diagonalizable (Cambio de Base)\")\n    C_inv = np.linalg.inv(C_calc)\n    A_reconstructed = C_calc @ S_calc @ C_inv\n\n    distancia_rearmada = np.linalg.norm(A - A_reconstructed)\n    print(f\"Distancia norma del modelo A comparado a C * S * C^-1: {distancia_rearmada:.5e}\")\n    print(\"\u00bfSe reconstruy\u00f3 sin fallos estad\u00edsticos la identidad A? (np.allclose):\", np.allclose(A, A_reconstructed))\n\nif __name__ == \"__main__\":\n    run_tests()\n</code></pre>"},{"location":"Examen_2025_07_21/03_matrices_ortogonales/teoria/","title":"Soluci\u00f3n del Ejercicio 3 (Examen 21 de julio de 2025 - Matrices Singulares y Espectros)","text":"<p>Ejercicio 3. Sea \\(A \\in \\mathbb{R}^{n \\times n}\\) una matriz tal que \\(A^t = A = A^{-1}\\).</p> <p>a) \u00bfCu\u00e1nto vale el determinante de \\(A\\)? \u00bfEs \\(A\\) diagonalizable?</p> <p>b) \u00bfCu\u00e1les son sus posibles autovalores?</p> <p>c) Calcular la matriz \\(\\Sigma\\) de la factorizaci\u00f3n SVD de \\(A\\). Justificar.</p> <p>d) Calcular los autovalores de la siguiente matriz:</p> \\[B = \\begin{pmatrix} 5/10 &amp; -5/10 &amp; -1/10 &amp; -7/10 \\\\ -5/10 &amp; 5/10 &amp; -1/10 &amp; -7/10 \\\\ -1/10 &amp; -1/10 &amp; 98/100 &amp; -14/100 \\\\ -7/10 &amp; -7/10 &amp; -14/100 &amp; 2/100 \\end{pmatrix}\\] <p>Sugerencia: usar los items anteriores.</p>"},{"location":"Examen_2025_07_21/03_matrices_ortogonales/teoria/#solucion-inciso-a","title":"Soluci\u00f3n Inciso A","text":"<p>a) \u00bfCu\u00e1nto vale el determinante de \\(A\\)? \u00bfEs \\(A\\) diagonalizable?</p> <p>El enigma postula que la matriz es id\u00e9ntica tanto a su transpuesta (\\(A^t = A\\), por ende es Sim\u00e9trica) como a su rec\u00edproca inversa (\\(A = A^{-1}\\), ergo es Involutiva). De la yuxtaposici\u00f3n de estas dos igualdades se extrae que \\(A^t = A^{-1}\\), lo que por definici\u00f3n de \u00e1lgebra lineal consagra a \\(A\\) adicionalmente como una matriz Ortogonal.</p> <p>Para averiguar el determinante, partimos de la definici\u00f3n que nos fue dada al unificar la simetr\u00eda con la inversa:</p> \\[A \\cdot A = A \\cdot A^{-1}\\] \\[A^2 = I\\] <p>Buscamos aplicar el operador determinante a ambos lados de la ecuaci\u00f3n, recordando la propiedad multiplicativa del determinante \\(|A \\cdot B| = |A| \\cdot |B|\\):</p> Demostraci\u00f3n Te\u00f3rica: La Regla Multiplicativa del Determinante <p>\u00bfDe d\u00f3nde surge que la multiplicidad algor\u00edtmica subyacente de la funci\u00f3n m\u00f3dulo respeta en total lealtad al producto de los agrupamientos? Su rigor matem\u00e1tico nace de las Matrices Elementales. Toda matriz regular inversible es descomponible en un tren finito de elementales (\\(A = E_1\\dots E_k\\)), los cuales individualmente traccionan al determinante separ\u00e1ndolo en pedazos l\u00f3gicos.</p> <p>\ud83d\udccc Para consultar minuciosamente paso por paso este desarme matricial que demuestra por qu\u00e9 \\(\\det(AB) = \\det(A)\\det(B)\\) (anexado junto al validador por inducci\u00f3n Monte Carlo estoc\u00e1stica dictaminado por la metodolog\u00eda de estudio), remitirse aqu\u00ed: Demostraci\u00f3n: Regla Multiplicativa del Determinante.</p> \\[|A^2| = |I|\\] \\[|A|^2 = 1\\] <p>Al despejar algebraicamente, obtenemos las dos posibles ra\u00edces reales del determinante:</p> \\[|A| = \\pm 1\\] <p>Por lo tanto, el determinante de la matriz estructurada \\(A\\) puede valer \\(1\\) o \\(-1\\). </p> <p>Acerca de si la matriz es diagonalizable, la respuesta viene dada inmediatamente por el Teorema Espectral. Dicho colosal teorema dictamina que \"Toda matriz real y sim\u00e9trica es diagonalizable ortogonalmente dentro de los n\u00fameros reales\". Como nuestra matriz satisface fehacientemente la condici\u00f3n de ser real y sim\u00e9trica (\\(A = A^t\\)), es innegablemente diagonalizable.</p> Demostraci\u00f3n Te\u00f3rica: Teorema Espectral <p>\u00bfDe d\u00f3nde surgen algebraicamente las garant\u00edas insalvables de que ninguna matriz sim\u00e9trica escapa jam\u00e1s al terreno imaginario de Ra\u00edces Complejas, y que sus correspondientes bases multidimensionales describen \u00e1ngulos perpetuos y exactos de 90\u00b0 entre s\u00ed? Todo se reduce a manipulaciones herm\u00edticas de la pre-multiplicaci\u00f3n y del conjugado transpuesto.</p> <p>\ud83d\udccc Revisar riguroso desarrollo paso a paso del porqu\u00e9 \\(\\lambda = \\overline{\\lambda}\\) junto con el porqu\u00e9 de la ortogonalidad \\(v_i \\cdot v_j = 0\\) sumado a su estr\u00e9s computacional randomizado por Python, aqu\u00ed: Demostraci\u00f3n: Teorema Espectral.</p>"},{"location":"Examen_2025_07_21/03_matrices_ortogonales/teoria/#solucion-inciso-b","title":"Soluci\u00f3n Inciso B","text":"<p>b) \u00bfCu\u00e1les son sus posibles autovalores?</p> <p>Si \\(A\\) es diagonalizable, asume autovalores \\(\\lambda_i\\) y autovectores asociados \\(v_i \\neq 0\\) que obedecen la transformaci\u00f3n originaria:</p> \\[A v_i = \\lambda_i v_i\\] <p>Para desentra\u00f1ar el espectro posible (los valores escalares que puede asir \\(\\lambda\\)), podemos pre-multiplicar libremente por \\(A\\) de ambos lados de la igualdad de autovectores:</p> \\[A (A v_i) = A (\\lambda_i v_i)\\] <p>Dada la linealidad matricial, se absorbe el factor en escalares externos:</p> \\[A^2 v_i = \\lambda_i (A v_i)\\] <p>Reemplazando la recursi\u00f3n original \\(A v_i\\):</p> \\[A^2 v_i = \\lambda_i (\\lambda_i v_i) = \\lambda_i^2 v_i\\] <p>No obstante, en el inciso A determinamos por las imposiciones del propio ejercicio que \\(A^2 = I\\). Reemplazando temporalmente al operador izquierdo:</p> \\[I v_i = \\lambda_i^2 v_i\\] \\[v_i = \\lambda_i^2 v_i\\] <p>Como los autovectores no pueden ser el vector nulo por definici\u00f3n de su existencia subyacente (\\(v_i \\neq 0\\)), la \u00fanica condici\u00f3n axiom\u00e1tica para que ambas proyecciones se equiparen es que el polinomio factor extra\u00eddo asuma la igualdad:</p> \\[\\lambda_i^2 = 1\\] <p>Resolviendo, encontramos que los \u00fanicos autovalores viables que componen el espectro de cualquier matriz con estas caracter\u00edsticas son \\(\\lambda = \\{1, -1\\}\\).</p>"},{"location":"Examen_2025_07_21/03_matrices_ortogonales/teoria/#solucion-inciso-c","title":"Soluci\u00f3n Inciso C","text":"<p>c) Calcular la matriz \\(\\Sigma\\) de la factorizaci\u00f3n SVD de \\(A\\). Justificar.</p> <p>La descomposici\u00f3n en valores singulares (SVD) nos permite desarticular a \\(A\\) en la multiplicaci\u00f3n \\(A = U \\Sigma V^t\\). Por teorema, la matriz anal\u00edtica diagonal \\(\\Sigma\\) resguarda en orden descendente los valores singulares (\\(\\sigma_i\\)) estrictamente positivos de \\(A\\).</p> <p>La doctrina matem\u00e1tica pauta que los valores singulares \\(\\sigma_i\\) de una matriz \\(A\\) son las ra\u00edces cuadradas de los autovalores algebraicos provenientes de la nueva matriz sim\u00e9trica definida positiva \\(A^t A\\).</p> <p>Calculemos internamente esta matriz a modelar:</p> \\[A^t A\\] <p>Viendo las definiciones iniciales (\\(A^t = A = A^{-1}\\)), la premultiplicamos algebr\u00e1icamente sabiendo que es ortogonal:</p> \\[A^t A = A^{-1} A = I\\] <p>Efectivamente, la matriz de correlaci\u00f3n es la Identidad. Los autovalores \\(\\lambda\\) vinculados a la matriz unidad matricial \\(I\\) son est\u00e1tica e irrevocablemente todos de monto num\u00e9rico igual a \\(1\\).</p> <p>Por consiguiente, extirpando ra\u00edces para obtener los valores singulares:</p> \\[\\sigma_i = \\sqrt{1} = 1 \\quad \\forall i \\in (1, \\dots, n)\\] <p>Todos los valores rectro-singulares son igual a la unidad. Al disponerse jer\u00e1rquicamente en la diagonal \\(\\Sigma\\), \u00e9sta pasar\u00e1 a ser una matriz rellena fundamentalmente de unos. La matriz \\(\\Sigma\\) evaluada resulta ser id\u00e9ntica anal\u00edticamente a la Matriz Identidad: \\(\\Sigma = I\\).</p>"},{"location":"Examen_2025_07_21/03_matrices_ortogonales/teoria/#solucion-inciso-d","title":"Soluci\u00f3n Inciso D","text":"<p>d) Calcular los autovalores de la siguiente matriz:</p> \\[B = \\begin{pmatrix} 5/10 &amp; -5/10 &amp; -1/10 &amp; -7/10 \\\\ -5/10 &amp; 5/10 &amp; -1/10 &amp; -7/10 \\\\ -1/10 &amp; -1/10 &amp; 98/100 &amp; -14/100 \\\\ -7/10 &amp; -7/10 &amp; -14/100 &amp; 2/100 \\end{pmatrix}\\] <p>Sugerencia: usar los items anteriores.</p> <p>Nos proponen la matriz \\(4 \\times 4\\):</p> \\[B = \\begin{pmatrix} 0.5 &amp; -0.5 &amp; -0.1 &amp; -0.7 \\\\ -0.5 &amp; 0.5 &amp; -0.1 &amp; -0.7 \\\\ -0.1 &amp; -0.1 &amp; 0.98 &amp; -0.14 \\\\ -0.7 &amp; -0.7 &amp; -0.14 &amp; 0.02 \\end{pmatrix}\\] <p>El hint nos insta a usar las conclusiones deductivas asimiladas en items previos. Inspeccionemos a \\(B\\):</p> <ul> <li> <p>Naturalmente saltan a la vista por espejo estructural sobre la diagonal que B es sim\u00e9trica (\\(B = B^t\\)).</p> </li> <li> <p>Si computariz\u00e1ramos o realiz\u00e1semos el esfuerzo marat\u00f3nico de calcular \\(B^2\\), notar\u00edamos (como comprobar\u00e1 la computadora acto seguido) que \\(B^2 = I\\), con lo que \\(B = B^{-1}\\).</p> </li> </ul> <p>Por consiguiente, la matriz \\(B\\) asienta emp\u00edricamente en el grupo de matrices de los incisos anteriores (\\(B^t = B = B^{-1}\\)). Seg\u00fan el inciso B, sus probables y \u00fanicos autovalores son exhaustivamente \\(1\\) o \\(-1\\). Nos toca descubrir la multiplicidad de ellos, es decir, determinar exactamente dentro de esos cuatro valores espaciales en \\(\\mathbb{R}^4\\) cu\u00e1ntos \"unos\" positivos y \"unos\" negativos subyacen.</p> <p>Haciendo uso anal\u00edtico implacable de la Traza de la Matriz (\\(Tr(B)\\), sumatoria lineal de los \u00edndices puros diagonales), sabemos que por el Teorema de la Traza \u00e9sta no muta jam\u00e1s bajo cambios de base y es perennemente id\u00e9ntica a la sumatoria de sus autovalores matem\u00e1ticos abstractos.</p> <p>Calculando la diagonal de \\(B\\):</p> \\[Tr(B) = 0.5 + 0.5 + 0.98 + 0.02 = 2\\] <p>Frente a esto, postulamos nuestro conjunto de auto-valores en inc\u00f3gnitas: Sea \\(k\\) la cantidad de autovalores de monto \\(1\\), y \\(m\\) la cantidad de autovalores de monto \\(-1\\). Como \\(\\dim(B) = 4\\), tenemos 4 autovalores totales y armamos el sistema 2x2:</p> \\[ \\begin{cases} k + m = 4 \\quad \\text{(Espectro total)} \\\\ k(1) + m(-1) = 2 \\quad \\text{(Suma traza-autovalores)} \\end{cases}\\] <p>Acumulando ambas igualdades (\\(2k = 6\\)), extirpamos l\u00f3gicamente a los constituyentes:</p> \\[k = 3\\] \\[m = 1\\] <p>Por ende, deducimos implacablemente sin factorizar grado 4, que los cuatro autovalores exactos de la matriz B son: \\(\\{1, 1, 1, -1\\}\\).</p>"},{"location":"Examen_2025_07_21/03_matrices_ortogonales/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef run_tests():\n    print(\"--- Analizando Num\u00e9ricamente Propiedades de Matriz A y B ---\")\n\n    # Inciso D: Vamos a someter nuestra deducci\u00f3n final al veredicto del hardware.\n    B = np.array([\n        [ 0.5,    -0.5,    -0.1,    -0.7   ],\n        [-0.5,     0.5,    -0.1,    -0.7   ],\n        [-0.1,    -0.1,     0.98,   -0.14  ],\n        [-0.7,    -0.7,    -0.14,    0.02  ]\n    ])\n\n    print(\"\\n--- Comprobando Axiomas Categ\u00f3ricos Iniciales sobre la Matriz B ---\")\n\n    # 1. Simetr\u00eda B = B^t\n    es_simetrica = np.allclose(B, B.T)\n    print(f\"\u00bfLa matriz posee simetr\u00eda morfol\u00f3gica (B = B^t)?: {es_simetrica}\")\n\n    # 2. Involuci\u00f3n B^2 = I\n    I_ref = np.eye(4)\n    B_cuadrado = B @ B\n    es_involutiva = np.allclose(B_cuadrado, I_ref)\n    print(f\"\u00bfEl cuadrado autopropagado de la Matriz recae en la Identidad (B^2 = I)?: {es_involutiva}\")\n\n    # Dado que se infieren como True, inferimos colateralmente la ortogonalidad:\n    B_inv = np.linalg.inv(B)\n    es_ortogonal = np.allclose(B.T, B_inv)\n    print(f\"(Deducimos Inversa igual a Transpuesta causal) \u00bfOrtogonalidad?: {es_ortogonal}\")\n\n    print(\"\\n--- Verificando Espectro Anal\u00edtico Sugerido (Inciso D) ---\")\n\n    # La computadora extraer\u00e1 de las entra\u00f1as matem\u00e1ticas el polinomio\n    lambdas_computadora, _ = np.linalg.eig(B)\n\n    # Debido a fluctuaciones estad\u00edsticas binarias flotantes (e.g., 0.999999999) rodeamos el valor emp\u00edrico\n    lambdas_redondeados = np.round(lambdas_computadora).astype(int)\n    # Ordenados descendentemente por prolijidad\n    lambdas_redondeados = np.sort(lambdas_redondeados)[::-1] \n\n    print(f\"Autovalores extra\u00eddos desde `numpy.linalg.eig` (Redondeados): {list(lambdas_redondeados)}\")\n\n    # Cotejando contra deducci\u00f3n escrita e inmaculada:\n    espectro_inferido = [1, 1, 1, -1]\n    espectro_inmaculado = np.allclose(lambdas_redondeados, espectro_inferido)\n    print(f\"\u00bfLos autovalores emp\u00edricos coinciden sin piedad con la deducci\u00f3n anal\u00edtica de la Traza y Rango?: {espectro_inmaculado}\")\n\n    print(\"\\n--- Inciso C: Corroborando Sigma Identidad frente a SVD de un Ortogonal ---\")\n    _, S_observado, _ = np.linalg.svd(B)\n    sigma_redondeado = np.round(S_observado).astype(int)\n    print(f\"La colecci\u00f3n de valores Sigmas (Singulares): {list(sigma_redondeado)}\")\n    print(f\"\u00bfEn efecto, toda SVD diagonal subyacente de Ortogonales en sus valores sigmas re-arma la Matriz Identidad?: {np.allclose(sigma_redondeado, [1, 1, 1, 1])}\")\n\n\nif __name__ == \"__main__\":\n    run_tests()\n</code></pre>"},{"location":"Examen_2025_07_21/04_cuadrados_minimos/teoria/","title":"Soluci\u00f3n del Ejercicio 4 (Examen 21 de julio de 2025 - Bases Ortonormales y Cuadrados M\u00ednimos)","text":"<p>Ejercicio 4. Sea \\(\\{q_1, q_2, q_3, q_4, q_5\\}\\) una base ortonormal de \\(\\mathbb{R}^5\\), \\(A\\) una matriz de \\(5 \\times 3\\) con columnas \\(q_1, q_2, q_3\\) y el vector \\(b = q_1 + 2q_2 + 3q_3 + 4q_4 + 5q_5\\).</p> <p>a) Mostrar que el sistema \\(Ax = b\\) no tiene soluci\u00f3n. Plantear las ecuaciones normales y hallar la soluci\u00f3n \\(\\hat{x}\\) de cuadrados m\u00ednimos para dicho sistema.</p> <p>b) Calcular el error cometido en la aproximaci\u00f3n.</p> <p>c) Mostrar que \\(A^\\dagger = A^t\\), siendo \\(A^\\dagger\\) la pseudoinversa de \\(A\\).</p>"},{"location":"Examen_2025_07_21/04_cuadrados_minimos/teoria/#solucion-inciso-a","title":"Soluci\u00f3n Inciso A","text":"<p>Definimos a la matriz \\(A\\) por sus propios vectores columna ortonormales:</p> \\[A = \\begin{pmatrix} | &amp; | &amp; | \\\\ q_1 &amp; q_2 &amp; q_3 \\\\ | &amp; | &amp; | \\end{pmatrix}\\] <p>Cualquier vector \\(v\\) que pertenezca a la imagen de \\(A\\) (es decir, \\(v \\in Col(A)\\)), debe expresarse \u00edntegramente como una combinaci\u00f3n lineal estricta de sus tres columnas componentes \\(\\{q_1, q_2, q_3\\}\\). $\\(A x = x_1 q_1 + x_2 q_2 + x_3 q_3\\)$</p> <p>Sin embargo, el vector independiente suministrado (\\(b\\)) fue definido a partir de la base ortonormal completa del espacio: $\\(b = 1 q_1 + 2 q_2 + 3 q_3 + 4 q_4 + 5 q_5\\)$</p> <p>Dado que por naturaleza los cinco vectores componen una base espacial, son l\u00f3gicamente linealmente independientes. Nos resulta materialmente imposible generar las proyecciones relativas a \\(q_4\\) y \\(q_5\\) vali\u00e9ndonos pura y exclusivamente de los tres primeros vectores generadores. Todo aporte al subespacio nulo del vector \\(b\\) es ajeno a \\(Col(A)\\).</p> <p>En conclusi\u00f3n, \\(b \\notin Col(A)\\), hecho que decreta instant\u00e1neamente que el sistema \\(A x = b\\) es intr\u00ednsecamente incompatible y carece de soluci\u00f3n matem\u00e1tica.</p> <p>Para solventar esta carencia, la ortodoxia anal\u00edtica exige hallar el punto hiperespacial que minice el grado de residuo \\(||Ax - b||\\). A este designio le llamamos construir la Ecuaci\u00f3n Normal de M\u00ednimos Cuadrados:</p> \\[A^t A \\hat{x} = A^t b\\] <p>Inspeccionemos cautelosamente de qu\u00e9 consta en s\u00ed el n\u00facleo multiplicativo \\(\\bold{A^t A}\\). Reuniendo los vectores columnas transpuestos, encontramos la Matriz Gramiana:</p> \\[A^t A = \\begin{pmatrix} - &amp; q_1^t &amp; - \\\\ - &amp; q_2^t &amp; - \\\\ - &amp; q_3^t &amp; - \\end{pmatrix} \\begin{pmatrix} | &amp; | &amp; | \\\\ q_1 &amp; q_2 &amp; q_3 \\\\ | &amp; | &amp; | \\end{pmatrix} = \\begin{pmatrix} q_1^t q_1 &amp; q_1^t q_2 &amp; q_1^t q_3 \\\\ q_2^t q_1 &amp; q_2^t q_2 &amp; q_2^t q_3 \\\\ q_3^t q_1 &amp; q_3^t q_2 &amp; q_3^t q_3 \\end{pmatrix}\\] <p>No obstante, la premisa enmarca formalmente que el conjunto engloba una Base Ortonormal. Dicho vocablo asume que el producto interior decantado arroja uno entre vectores hom\u00f3logos y cero frente a proyecciones ortogonales de \u00edndices mixtos (\\(q_i^t q_j = \\delta_{ij}\\)). Como corolario irremediable, la diagonal queda plagada de unos, y la anti-diagonal se limpia gravitando en nulos:</p> \\[A^t A = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{pmatrix} = I_3\\] <p>Donde \\(I_3\\) es la matriz Identidad en \\(\\mathbb{R}^{3 \\times 3}\\).</p> <p>Abordemos ahora la operaci\u00f3n restante en el extremo de resultados del modelo, desarticulando el vector \\(b\\):</p> \\[A^t b = \\begin{pmatrix} - &amp; q_1^t &amp; - \\\\ - &amp; q_2^t &amp; - \\\\ - &amp; q_3^t &amp; - \\end{pmatrix} (1 q_1 + 2 q_2 + 3 q_3 + 4 q_4 + 5 q_5)\\] <p>Al distribuir los multiplicadores punto de cada fila, la cualidad de las componentes ortogonales vuelve a operar. Cada vector asimila \u00fanicamente aqu\u00e9l escalar co-paralelo de su misma especie y anula irrefutablemente frente al producto escalar toda correlaci\u00f3n con elementos dispares (como \\(q_4\\) y \\(q_5\\)).</p> <p>En consecuencia obtenemos algebraicamente el vector unicamente compuesto por los escalares base rescatados:</p> \\[A^t b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\] <p>Retornando a nuestra Ecuaci\u00f3n Normal primigenia y volcando all\u00ed nuestras deducciones operacionales:</p> \\[I_3 \\hat{x} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\] <p>Revelando la estoc\u00e1stica pura: \\(\\hat{x} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\\)</p>"},{"location":"Examen_2025_07_21/04_cuadrados_minimos/teoria/#solucion-inciso-b","title":"Soluci\u00f3n Inciso B","text":"<p>Calcularemos matem\u00e1ticamente la pureza de la respuesta hallada evaluando el vector margen de error originado en la desestimaci\u00f3n. Formulemos a la Proyecci\u00f3n Ortonormal \\(p\\) arrojada por nuestro sistema aproximado en m\u00ednimos recuadros:</p> \\[p = A \\hat{x} = \\begin{pmatrix} | &amp; | &amp; | \\\\ q_1 &amp; q_2 &amp; q_3 \\\\ | &amp; | &amp; | \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = 1 q_1 + 2 q_2 + 3 q_3\\] <p>Se define un\u00e1nimemente el vector de error \\(e\\) intercediendo el objetivo deseado frente al punto m\u00e1s pr\u00f3ximo l\u00edcito validable en subespacio (\\(p\\)):</p> \\[e = b - p\\] \\[e = (1 q_1 + 2 q_2 + 3 q_3 + 4 q_4 + 5 q_5) - (1 q_1 + 2 q_2 + 3 q_3)\\] <p>Extrayendo denominadores comunes y suprimiendo t\u00e9rminos afines:</p> \\[e = 4 q_4 + 5 q_5\\] <p>Someteremos a evaluaci\u00f3n de norma Euclidiana (distancia-2 computacional) (\\(|| \\cdot ||_2\\)) para constatar el Error Cuadr\u00e1tico Final total. Aplicando el axioma de Pit\u00e1goras debido a que nuestra base es un abanico pre-ortogonal (donde el cateto vector 4 no posee injerencias angulares con el cateto vector 5):</p> \\[||e||_2^2 = (4 ||q_4||_2)^2 + (5 ||q_5||_2)^2\\] <p>Dado su condici\u00f3n normativa que restringe el largo a uno (\\(||q||=1\\) para todo subespacio interviniente):</p> \\[||e||_2^2 = 16 (1)^2 + 25 (1)^2 = 41\\] \\[||e||_2 = \\sqrt{41}\\] <p>El error cometido en la regresi\u00f3n matem\u00e1tica m\u00e1s plausible asume rigurosamente la suma pitag\u00f3rica del m\u00f3dulo residual de todo tensor independiente al subespacio de recabo (\\(\\sqrt{41}\\)).</p>"},{"location":"Examen_2025_07_21/04_cuadrados_minimos/teoria/#solucion-inciso-c","title":"Soluci\u00f3n Inciso C","text":"<p>La Pseudoinversa expl\u00edcita de Moore-Penrose, universalmente representada por el s\u00edmbolo transfigurado de una daga formal algebraico o com\u00fanmente una cu\u00f1a supra-marcada (\\(A^\\dagger\\)), generaliza matricialmente toda funci\u00f3n inversa en el contexto que aglomera tensores asim\u00e9tricos no cuadrados para poder invertirlos en espacios de caracter\u00edsticas lineales dependientes.</p> <p>Para constatar si encuadramos, se sabe por norma general que si el conjunto matriz en columnas asume en su integridad rango pleno de sus componentes columna (que sus columnas \\(q_1, q_2, q_3\\) son linealmente independientes en todo aspecto), la pseudoinversa de limitaci\u00f3n izquierda subyace definida por la formalidad computacional:</p> \\[A^\\dagger = (A^t A)^{-1} A^t\\] <p>A diferencia del espectro, nuestra matriz de dise\u00f1o \\(A\\) s\u00ed posee sus vectores base definidos y ratificados en este postulado al ser la base ortonormal madre (donde todo generador ostentado no asume interdependencia). Nos avala a aplicar la Ley Fundamental de la izquierda, invocando el \u00e1lgebra probada rigurosamente en el comienzo del Inciso A, en donde expusimos en un cuadro gramiano axiom\u00e1tico que la multiplicaci\u00f3n pre-transpuesta desencadenaba el orden en Identidad Absoluta:</p> \\[A^t A = I_3\\] <p>Retornando al modelo te\u00f3rico:</p> \\[A^\\dagger = (I)^{-1} A^t\\] \\[A^\\dagger = I \\cdot A^t = A^t\\] <p>Hemos demostrado incuestionablemente que por la idiosincrasia formal de ostentar matrices constituidas como subespacios con bases de caracter\u00edsticas modulares ortonormales puros, su inversa ideal minimizadora se contrae por isomorfismo l\u00f3gicamente anal\u00edtico a su propia Matriz Traspuesta (\\(A^\\dagger = A^t\\)).</p>"},{"location":"Examen_2025_07_21/04_cuadrados_minimos/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef run_tests():\n    print(\"--- Verificaci\u00f3n Computacional de Bases Ortonormales y Residuales MCO ---\")\n\n    np.random.seed(33)\n\n    # Vamos a simular la base Q de R^5 estoc\u00e1sticamente usando factorizacion QR de una random\n    # La descomposicion QR de numpy es \u00f3ptima porque Q conforma per se una matriz Ortonormal.\n    R5_caotica = np.random.randn(5, 5)\n    Q, _ = np.linalg.qr(R5_caotica)\n\n    print(\"\\n[VALIDACI\u00d3N PREVIA DEL AXIOMA EXPERIMENTAL]\")\n    print(f\"\u00bfLos 5 vectores resultantes (Q) son fehacientemente Ortogonales Unitarios?: {np.allclose(Q.T @ Q, np.eye(5))}\")\n\n    # Estilizamos variables para mantener correlaci\u00f3n nominal.\n    # Q contiene las columnas q1, q2, q3, q4, q5\n    q1 = Q[:, 0].reshape(-1, 1)\n    q2 = Q[:, 1].reshape(-1, 1)\n    q3 = Q[:, 2].reshape(-1, 1)\n    q4 = Q[:, 3].reshape(-1, 1)\n    q5 = Q[:, 4].reshape(-1, 1)\n\n    # Estructura del examen\n    A = np.column_stack((q1, q2, q3))\n\n    b = 1*q1 + 2*q2 + 3*q3 + 4*q4 + 5*q5\n\n    print(\"\\n--- Inciso A: Vector Estimado \\hat{x} MCO ---\")\n\n    # Forma ortodoxa general de librer\u00edas para Least Squares\n    x_hat_libreria, residuos, rank, s = np.linalg.lstsq(A, b, rcond=None)\n\n    print(\"El procesador calcul\u00f3 que b carec\u00eda de soluci\u00f3n en Ax=b. Recort\u00f3 la proyecci\u00f3n al plano A.\")\n    print(\"El Numpy lstsq arroj\u00f3 el \\hat{x} minimizado de respuesta:\")\n    print(np.round(x_hat_libreria.flatten(), 5))\n\n    # Forma axiom\u00e1tica que dedujimos nosotros\n    x_hat_deducido = np.array([[1], [2], [3]])\n    print(\"Comprobando con nuestra deducci\u00f3n matem\u00e1tica anal\u00edtica [1, 2, 3]^T.\")\n    print(f\"\u00bfAmbos vectores son el mismo punto (np.allclose)?: {np.allclose(x_hat_libreria, x_hat_deducido)}\")\n\n    print(\"\\n--- Inciso B: Magnitud de Error Cuadr\u00e1tico ---\")\n\n    # Calcular proyeccion\n    p = A @ x_hat_libreria\n    e = b - p\n\n    error_num_norm = np.linalg.norm(e, ord=2)\n    # Deducci\u00f3n del papel dictaba Ra\u00edz Cuadrada de 41:\n    error_analitico_raiz = np.sqrt(41)\n\n    print(f\"La computadora calcul\u00f3 el error residuo puramente num\u00e9rico (np.norm) en: {error_num_norm:.6f}\")\n    print(f\"Nuestro modelo en folio de raiz de 41 estipulaba est\u00e1ticamente:       {error_analitico_raiz:.6f}\")\n    print(f\"\u00bfConcuerdan a nivel microsc\u00f3pico?: {np.isclose(error_num_norm, error_analitico_raiz)}\")\n\n    print(\"\\n--- Inciso C: A_pseudiinversa = A_transpuesta ---\")\n\n    # Pinacle algorithm\n    A_pseudoinversa_calculada = np.linalg.pinv(A)\n\n    # Axioma nuestro\n    A_transpuesta_axioma = A.T\n\n    print(f\"Distancia norma matriz entre el M\u00f3dulo SVD del PC (Pseudoinverse) y A_transpuesta: {np.linalg.norm(A_pseudoinversa_calculada - A_transpuesta_axioma):.5e}\")\n    print(f\"Determinamos que el m\u00f3dulo se reemplaza 1 a 1 para matrices extra\u00eddas de Q-Bases.\")\n    print(f\"\u00bfSon matem\u00e1ticamente la misma matriz id\u00e9ntica? {np.allclose(A_pseudoinversa_calculada, A_transpuesta_axioma)}\")\n\nif __name__ == \"__main__\":\n    run_tests()\n</code></pre>"},{"location":"Examen_2025_07_21/05_cauchy_schwartz/teoria/","title":"Soluci\u00f3n del Ejercicio 5 (Examen 21 de julio de 2025 - Desigualdad de Cauchy-Schwartz)","text":"<p>Ejercicio 5. Probar la desigualdad de Cauchy-Schwartz: \\(|x^* y| \\le ||x||_2 ||y||_2\\).</p>"},{"location":"Examen_2025_07_21/05_cauchy_schwartz/teoria/#solucion-demostracion-matematica","title":"Soluci\u00f3n: Demostraci\u00f3n Matem\u00e1tica","text":"<p>La Desigualdad de Cauchy-Schwartz es uno de los teoremas angulares del an\u00e1lisis matem\u00e1tico y el \u00e1lgebra lineal subyacente a los espacios con producto interno (como las geometr\u00edas de Hilbert o el espacio \\(\\mathbb{R}^n\\) / \\(\\mathbb{C}^n\\)). El denoto \\(x^*\\) asume ser la matriz Transpuesta Conjugada (Adjunta o Hermitiana), ratificando que el vector podr\u00eda habitar el plano complejo.</p> <p>Iniciaremos el recabo anal\u00edtico modelando un sistema axiom\u00e1tico puramente axiom\u00e1tico de M\u00ednimos Cuadrados y Proyecciones Ortogonales, entrelazando l\u00f3gicamente los cimientos que construimos en el Ejercicio 4 previo de este examen.</p> <p>Sabemos categ\u00f3ricamente por la fundamentaci\u00f3n eucl\u00eddea que: La norma al cuadrado de cualquier vector jam\u00e1s puede ser un n\u00famero negativo.</p> <p>Elijamos para modelar artificialmente a un vector \\(e\\), constituido como aquel vector residuo estricto que resulta de restarle al vector \\(y\\) su respectiva proyecci\u00f3n escalar matem\u00e1tica sobre el vector \\(x\\):</p> \\[e = y - \\frac{x^* y}{x^* x} x\\] <p>Bajo la doctrina del producto interno herm\u00edtico, la norma l2 al cuadrado (\\(||e||_2^2\\)) equivale irrebocablemente al producto de un vector por su propio conjugado adosado: $\\(||e||_2^2 = e^* e\\)$</p> <p>Sometemos nuestro axioma primigenio (que dictamina matem\u00e1ticamente \\(||e||_2^2 \\ge 0\\)) al modelo armado por sus partes internas:</p> \\[\\left(y - \\frac{x^* y}{x^* x} x \\right)^* \\left(y - \\frac{x^* y}{x^* x} x \\right) \\ge 0\\] <p>Como es un sistema sobre \\(\\mathbb{C}^n\\), debemos distribuir aplicando las r\u00edgidas normas de transposiciones conjugadas. Siendo que \\(x^* x = ||x||_2^2 \\in \\mathbb{R}\\) (es un escalar puramente real), y que en el marco del plano complejo es ley que todo producto alternado \\((x^* y)^* = y^* x\\) y \\((x^* y)(y^* x) = |x^* y|^2\\):</p> \\[ \\left( y^* - \\frac{(x^* y)^*}{||x||_2^2} x^* \\right) \\left( y - \\frac{x^* y}{||x||_2^2} x \\right) \\ge 0 \\] <p>Multiplicamos cruzado los cuatro t\u00e9rminos como binomios est\u00e1ndar:</p> \\[y^* y - y^* \\left( \\frac{x^* y}{||x||_2^2} x \\right) - \\left( \\frac{y^* x}{||x||_2^2} x^* \\right) y + \\left( \\frac{y^* x}{||x||_2^2} x^* \\right) \\left( \\frac{x^* y}{||x||_2^2} x \\right) \\ge 0\\] <p>Simplifiquemos reconociendo a las normas incrustadas dentro del espectro anal\u00edtico individual, retirando los escalares por fuera de la multiplicaci\u00f3n y reconociendo que \\(y^* y = ||y||_2^2\\) y \\(x^* x = ||x||_2^2\\):</p> \\[||y||_2^2 - \\frac{(x^* y)(y^* x)}{||x||_2^2} - \\frac{(y^* x)(x^* y)}{||x||_2^2} + \\frac{(y^* x)(x^* y)}{||x||_2^4} ||x||_2^2 \\ge 0\\] <p>Al simplificar la redundancia modal \\((y^* x)(x^* y)\\) y achicar el \\(||x||_2^2\\) del \u00faltimo denominador del polinomio se devela una estructura predecible:</p> \\[||y||_2^2 - \\frac{|x^* y|^2}{||x||_2^2} - \\frac{|x^* y|^2}{||x||_2^2} + \\frac{|x^* y|^2}{||x||_2^2} \\ge 0\\] <p>Cancelando dos de las iteraciones asint\u00f3ticas contrapuestas finales:</p> \\[||y||_2^2 - \\frac{|x^* y|^2}{||x||_2^2} \\ge 0\\] <p>Despejando nuestro axioma de desigualdad para aislar la naturaleza del producto interno por fuera de sus normas base:</p> \\[||y||_2^2 \\ge \\frac{|x^* y|^2}{||x||_2^2}\\] \\[||y||_2^2 ||x||_2^2 \\ge |x^* y|^2\\] <p>Finalmente, dado que todas las magnitudes abarcadas son n\u00fameros reales positivos l\u00edcitos por naturaleza f\u00edsica de la medici\u00f3n de norma (\\(||\\cdot|| \\ge 0\\)), estamos habilitados a suministrar ra\u00edz cuadrada a ambos lados del espectro de inequidad destilando el objetivo central del teorema:</p> \\[||x||_2 ||y||_2 \\ge |x^* y|\\] <p>Demostrando de forma categ\u00f3rica que el valor absoluto del producto interno entre dos subespacios invariantes en la topolog\u00eda nunca puede exceder la frontera anal\u00edtica del producto aislado por sus magnitudes m\u00e9tricas subyacentes.</p>"},{"location":"Examen_2025_07_21/05_cauchy_schwartz/teoria/#verificacion-computacional-en-python","title":"Verificaci\u00f3n Computacional en Python","text":"<pre><code>import numpy as np\n\ndef run_tests():\n    print(\"--- Validaci\u00f3n Computacional: Desigualdad de Cauchy-Schwartz ---\")\n    np.random.seed(81)\n\n    ITERACIONES = 100_000\n    violaciones = 0\n    tolerancia_precision = 1e-10\n\n    # Probando sobre diferentes espacios dimensionales complejos\n    for _ in range(ITERACIONES):\n        dim = np.random.randint(2, 50)\n\n        # Universo Real y Complejo\n        x = np.random.randn(dim) + 1j * np.random.randn(dim)\n        y = np.random.randn(dim) + 1j * np.random.randn(dim)\n\n        # Variables en contenci\u00f3n de Cauchy-Schwartz\n        # |x* y|\n        # x.conj().T es el Hermitiano equivalente a x*\n        x_start_y = np.abs(np.vdot(x, y))  # Producto interno complejo devuelve el m\u00f3dulo de la covarianza\n\n        # ||x||_2 ||y||_2\n        norma_x = np.linalg.norm(x, ord=2)\n        norma_y = np.linalg.norm(y, ord=2)\n        producto_normas = norma_x * norma_y\n\n        # Condici\u00f3n Cauchy-Schwartz\n        # |x* y| &lt;= ||x||_2 ||y||_2\n        # Relajando apenas por cuestiones binomicas del coma flotante para no escupir falsos negativos\n        if x_start_y &gt; (producto_normas + tolerancia_precision):\n            violaciones += 1\n\n    print(f\"\\n--- Resultados del Laboratorio Estad\u00edstico ---\")\n    print(f\"N\u00famero de iteraciones probadas masivamente: {ITERACIONES} (Vectores $\\\\mathbb{{C}}^N$)\")\n    print(f\"Inconsistencias halladas superando el l\u00edmite anal\u00edtico: {violaciones}\")\n    print(f\"Veredicto del Ordenador sobre la Desigualdad: {'INQUEBRANTABLE' if violaciones == 0 else 'REFUTADO'}\")\n\n    print(\"\\n[Ejemplo Gr\u00e1fico a Escala para un N]\")\n    print(f\"Norma L2 del Vector Experimento X: {norma_x:.4f}\")\n    print(f\"Norma L2 del Vector Experimento Y: {norma_y:.4f}\")\n    print(f\"Magnitud Combinada Lim\u00edtrofe (||X|| * ||Y||): {producto_normas:.4f}\")\n    print(f\"Valor Absoluto Interno Medido (|X* Y|): {x_start_y:.4f}\")\n\nif __name__ == \"__main__\":\n    run_tests()\n</code></pre>"},{"location":"Examen_2025_08_07/01_proyector/teoria/","title":"Ejercicio 1: Proyectores y Subespacios","text":"<p>Ejercicio 1. Sea \\(A = \\begin{pmatrix} -1 &amp; 1 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; -1 &amp; -2 \\\\ 1 &amp; -1 &amp; 0 &amp; -1 \\\\ 1 &amp; -1 &amp; 1 &amp; 0 \\end{pmatrix}\\) y \\(f: \\mathbb{R}^4 \\to \\mathbb{R}^4\\) definida por \\(f(x) = Ax\\).</p>"},{"location":"Examen_2025_08_07/01_proyector/teoria/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Se nos da una matriz \\(A \\in \\mathbb{R}^{4 \\times 4}\\) que define una transformaci\u00f3n lineal \\(f(x) = Ax\\). El ejercicio pide en tres incisos construir un proyector \\(p\\) adaptado a los subespacios fundamentales de \\(f\\), deducir sus propiedades m\u00e9tricas (si es ortogonal) e identificar cu\u00e1ndo coincide con \\(f\\), para finalmente diagonalizarlo.</p> <p>a) Definir un proyector \\(p:\\mathbb{R}^4 \\to \\mathbb{R}^4\\) tal que \\(Im(p) = Im(f)\\) y \\(Nu(p) = Nu(f)\\).</p>"},{"location":"Examen_2025_08_07/01_proyector/teoria/#solucion-del-inciso-a","title":"Soluci\u00f3n del Inciso (a)","text":"<p>Para construir dicho proyector \\(p\\), necesitamos conocer expl\u00edcitamente bases para la imagen (\\(Im(f)\\)) y el n\u00facleo (\\(Nu(f)\\)) de nuestra transformaci\u00f3n lineal.</p> <p>Comenzamos escalonando la matriz \\(A\\) para hallar su forma reducida por filas y determinar los pivotes:</p> \\[ A \\sim \\begin{pmatrix} 1 &amp; -1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>De las columnas pivotales (primera y tercera), deducimos una base \\(B_{Im}\\) para la imagen \\(Im(f) = Col(A)\\):</p> \\[ B_{Im} = \\left\\{ \\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\} \\] <p>De las variables libres (\\(x_2\\) y \\(x_4\\)), derivamos las ecuaciones del n\u00facleo \\(Nu(f)\\): \\(x_1 - x_2 - x_4 = 0\\) y \\(x_3 + x_4 = 0\\). Expresado param\u00e9tricamente, \\(x_1 = x_2 + x_4\\) y \\(x_3 = -x_4\\). Por lo que obtenemos una base \\(B_{Nu}\\):</p> \\[ B_{Nu} = \\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{pmatrix} \\right\\} \\] <p>Para que la transformaci\u00f3n \\(p\\) sea un proyector definido de esta forma, y dado que queremos proyectar sobre \\(Im(f)\\) a lo largo de \\(Nu(f)\\), debe cumplirse que \\(\\mathbb{R}^4 = Im(f) \\oplus Nu(f)\\) (suma directa). Verificamos que la uni\u00f3n de las bases forma un conjunto linealmente independiente evaluando el determinante de la matriz ensamblada \\(B = [B_{Im} \\mid B_{Nu}]\\):</p> \\[ \\det(B) = \\det \\begin{pmatrix} -1 &amp; 0 &amp; 1 &amp; 1 \\\\ 1 &amp; -1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; -1 \\\\ 1 &amp; 1 &amp; 0 &amp; 1 \\end{pmatrix} = 3 \\neq 0 \\] <p>La intersecci\u00f3n es trivial, la suma directa existe, y por lo tanto podemos definir \\(p(x)\\) un\u00edvocamente dictando su acci\u00f3n sobre nuestra nueva base:</p> <ul> <li>\\(\\forall v \\in Im(f): p(v) = v\\)</li> <li>\\(\\forall n \\in Nu(f): p(n) = 0\\)</li> </ul> <p>As\u00ed, definimos \\(p\\) expl\u00edcitamente a trav\u00e9s de su matriz asociada \\(P\\) en la base can\u00f3nica como \\(P = B D B^{-1}\\), siendo \\(D\\) la matriz diagonal con los autovalores asignados:</p> \\[ D = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>Resolviendo el producto de matrices, la matriz del proyector resulta:</p> \\[ P = \\begin{pmatrix} -1.33 &amp; 1.33 &amp; 0 &amp; -2.33 \\\\ 0.66 &amp; 0.33 &amp; 0 &amp; 1.66 \\\\ 0.33 &amp; -0.33 &amp; 1 &amp; 0.66 \\\\ 0.33 &amp; -0.33 &amp; 0 &amp; 1.66 \\end{pmatrix} \\] <p>(Nota: Valores decimales expresados de forma compacta para legibilidad, el resultado exacto usa fracciones peri\u00f3dicas).</p> <p>b) Decidir si \\(p\\) es un proyector ortogonal. \u00bfEs \\(p\\) id\u00e9ntico a \\(f\\)?</p>"},{"location":"Examen_2025_08_07/01_proyector/teoria/#solucion-del-inciso-b","title":"Soluci\u00f3n del Inciso (b)","text":"<p>Para que un proyector \\(p\\) sea proyectado ortogonal sobre un subespacio, el n\u00facleo del proyector debe ser el complemento ortogonal de su imagen: \\(Nu(p) = Im(p)^\\perp\\). Esto se verifica anal\u00edticamente comprobando si la matriz del proyector \\(P\\) en la base can\u00f3nica es sim\u00e9trica: \\(P = P^T\\).</p> <p>Observando la matriz \\(P\\) obtenida en el inciso a), claramente \\(P \\neq P^T\\) (por ejemplo, \\(P_{1,2} = 1.33 \\neq P_{2,1} = 0.66\\)). Por lo tanto, \\(p\\) no es un proyector ortogonal; es un proyector oblicuo.</p> <p>De manera alternativa, pod\u00edamos verificarlo tomando vectores de ambas bases calculadas y haciendo el producto interno. Si tomamos \\(v_1 \\in Im(f)\\) y \\(n_1 \\in Nu(f)\\):</p> \\[ \\langle v_1, n_1 \\rangle = (-1)(1) + (1)(1) + (1)(0) + (1)(0) = 0 \\] <p>Sin embargo, basta probar con otro par:</p> \\[ \\langle v_2, n_1 \\rangle = (0)(1) + (-1)(1) + (0)(0) + (1)(0) = -1 \\neq 0 \\] <p>Esto demuestra que \\(Nu(f)\\) no es perpendicular a \\(Im(f)\\).</p> <p>\u00bfEs \\(p\\) id\u00e9ntico a \\(f\\)? Para que \\(p(x) = f(x) \\;\\forall x\\), deber\u00eda ocurrir que la matriz \\(P\\) sea id\u00e9ntica a \\(A\\). A simple vista de las matrices, vemos que \\(P \\neq A\\). Otra forma de verlo es analizando la restricci\u00f3n sobre la imagen: si \\(p \\equiv f\\), deber\u00edamos tener \\(f(v) = v\\) para todo \\(v \\in Im(f)\\). Evaluamos la transformaci\u00f3n \\(f\\) en \\(v_1\\):</p> \\[ f(v_1) = A v_1 = \\begin{pmatrix} 3 \\\\ -5 \\\\ -3 \\\\ -1 \\end{pmatrix} \\neq v_1 \\] <p>Con esto, demostramos que \\(p\\) no es id\u00e9ntico a \\(f\\). \\(A\\) no actuaba operativamente como la identidad en su propio subespacio imagen.</p> <p>c) Hallar una base \\(B\\) tal que la matriz de \\(p\\) en \\(B\\) sea diagonal.</p>"},{"location":"Examen_2025_08_07/01_proyector/teoria/#solucion-del-inciso-c","title":"Soluci\u00f3n del Inciso (c)","text":"<p>Esta respuesta la construimos inherentemente en la resoluci\u00f3n del inciso a). Todo proyector \\(p\\) cuyas restricciones son la suma directa iterativa de su propia imagen y n\u00facleo (\\(Im \\oplus Nu = \\mathbb{R}^n\\)) es naturalmente diagonalizable, teniendo como autovalores a 1 (con multiplicidad algebraica igual a la dimensi\u00f3n de su imagen) y a 0 (con multiplicidad id\u00e9ntica a la nulidad).</p> <p>La matriz diagonal \\(D_{p}\\) la obtenemos usando una matriz de paso \\(B\\) cuyas columnas sean las bases combinadas en orden de los autovalores:</p> \\[ B_{diag} = \\left\\{ \\underbrace{\\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 1 \\end{pmatrix}}_{\\text{Autovalor } \\lambda=1}, \\underbrace{\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{pmatrix}}_{\\text{Autovalor } \\lambda=0} \\right\\} \\] <p>Con respecto a esta base \\(B_{diag}\\), la matriz de \\(p\\) es, en efecto, la matriz diagonal construida anteriormente:</p> \\[ [P]_B = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\]"},{"location":"Examen_2025_08_07/02_markov/teoria/","title":"Ejercicio 2: Cadenas de Markov","text":"<p>Ejercicio 2.</p> <p>a) Sea \\(P \\in \\mathbb{R}^{n \\times n}\\) la matriz de un proceso de Markov en el que hay \\(k\\) estados \\(i_1, i_2, \\ldots, i_k\\) tales que la probabilidad de pasar de \\(i_j\\) a \\(i_{j+1}\\) para \\(j = 1, \\ldots, k-1\\) y de \\(i_k\\) a \\(i_1\\) es 1. Probar que existe un \\(\\lambda \\in \\mathbb{C}\\) autovalor de \\(P\\) tal que \\(\\lambda \\neq 1\\), pero \\(|\\lambda| = 1\\).</p> <p>b) Considerar el proceso descripto por el grafo, donde las probabilidades de transici\u00f3n desde cada nodo se reparten en partes iguales entre todas las ramas salientes. Hallar un estado de equilibrio. \u00bfEs \u00fanico? \u00bfSe alcanza este equilibrio desde cualquier estado inicial?</p>"},{"location":"Examen_2025_08_07/02_markov/teoria/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>El problema eval\u00faa las propiedades espectrales subyacentes de las matrices estoc\u00e1sticas (Procesos de Markov). En el inciso (a), se nos pide una demostraci\u00f3n general acerca de los autovalores en cadenas de Markov que presentan estados c\u00edclicos determin\u00edsticos cerrados. En el inciso (b), se debe construir la matriz de transici\u00f3n a partir del grafo dado, calcular su estado de equilibrio (autovector asociado a \\(\\lambda=1\\)) y analizar su convergencia bas\u00e1ndonos en la conectividad del grafo.</p>"},{"location":"Examen_2025_08_07/02_markov/teoria/#representacion-del-grafo","title":"Representaci\u00f3n del Grafo","text":"<p>A continuaci\u00f3n, visualizamos el flujo de estados descrito en el inciso (b):</p> <pre><code>graph LR\n    1((1)) --&gt; 2((2))\n    1((1)) --&gt; 4((4))\n\n    2((2)) --&gt; 3((3))\n    2((2)) --&gt; 4((4))\n\n    3((3)) --&gt; 3((3))\n    3((3)) --&gt; 5((5))\n\n    4((4)) --&gt; 5((5))\n\n    5((5)) --&gt; 3((3))\n</code></pre>"},{"location":"Examen_2025_08_07/02_markov/teoria/#solucion-del-inciso-a","title":"Soluci\u00f3n del Inciso (a)","text":"<p>Se describe un subconjunto de \\(k\\) estados que se transicionan de manera determin\u00edstica conformando un ciclo cerrado sin absorci\u00f3n ni escape. Al renombrar y ordenar temporalmente esta sub-cadena \\(i_1, i_2, \\ldots, i_k\\) para que abarquen los primeros \\(k\\) \u00edndices, la matriz estoc\u00e1stica \\(P\\) asume una estructura de bloque donde las transiciones a lo largo de estos estados se concentran en una submatriz de permutaci\u00f3n c\u00edclica \\(C_{k \\times k}\\):</p> \\[ C = \\begin{pmatrix} 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 &amp; 0 \\end{pmatrix} \\] <p>Y nuestra matriz global \\(P\\) toma la forma:</p> \\[ P = \\begin{pmatrix} C &amp; \\ast \\\\ 0 &amp; \\ast \\end{pmatrix} \\] <p>Los autovalores de una matriz triangular/bloque-triangular son la uni\u00f3n de los autovalores de sus bloques diagonales. De este modo, los autovalores de la submatriz c\u00edclica \\(C\\) son tambi\u00e9n autovalores de \\(P\\).</p> <p>Nos preguntamos, \u00bfcu\u00e1les son los autovalores de una permutaci\u00f3n puramente c\u00edclica de longitud \\(k\\)? Por su geometr\u00eda rotacional, al aplicarle el proceso \\(k\\) veces volvimos al estado original. Esto significa algebraicamente que \\(C^k = I\\) (la matriz identidad \\(k \\times k\\)).</p> <p>Aplicando propiedades de autovalores, si \\(v\\) es autovector de \\(C\\) con autovalor \\(\\lambda\\), entonces:</p> \\[ C^k v = \\lambda^k v \\implies I v = \\lambda^k v \\implies \\lambda^k = 1 \\] <p>Por ende, los autovalores de \\(C\\) son las ra\u00edces \\(k\\)-\u00e9simas de la unidad:</p> \\[ \\lambda_m = e^{i\\frac{2\\pi m}{k}}, \\quad m = 0, 1, \\ldots, k-1 \\] <p>Para cualquier valor de \\(m &gt; 0\\) y menor a \\(k\\), obtenemos un autovalor perteneciente al plano complejo \\(\\mathbb{C}\\) tal que \\(\\lambda \\neq 1\\). Sin embargo, su m\u00f3dulo sigue siendo unitario:</p> \\[ |\\lambda_m| = |e^{i\\frac{2\\pi m}{k}}| = 1 \\] <p>Queda entonces probado que el espectro de cualquier cadena de Markov con ciclos determin\u00edsticos siempre aloja autovalores diferentes de 1 que residen justo en la frontera del disco unidad complejo.</p>"},{"location":"Examen_2025_08_07/02_markov/teoria/#solucion-del-inciso-b","title":"Soluci\u00f3n del Inciso (b)","text":"<p>Armamos la matriz estoc\u00e1stica por columnas \\(P\\) a base de que todas las aristas que parten de un nodo dividen sus probabilidades equitativamente. Notamos las transiciones de salida para cada nodo \\(j\\): - De \\(1 \\to \\{2, 4\\}\\) (2 salidas \\(\\implies p=0.5\\) c/u) - De \\(2 \\to \\{3, 4\\}\\) (2 salidas \\(\\implies p=0.5\\) c/u) - De \\(3 \\to \\{3, 5\\}\\) (2 salidas \\(\\implies p=0.5\\) c/u) - De \\(4 \\to \\{5\\}\\) (1 salida \\(\\implies p=1.0\\)) - De \\(5 \\to \\{3\\}\\) (1 salida \\(\\implies p=1.0\\))</p> <p>Ubicando las probabilidades de ir de la columna \\(j\\) a la fila \\(i\\), obtenemos nuestra \\(P \\in \\mathbb{R}^{5 \\times 5}\\):</p> \\[ P = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0.5 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.5 &amp; 0.5 &amp; 0 &amp; 1 \\\\ 0.5 &amp; 0.5 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.5 &amp; 1 &amp; 0 \\end{pmatrix} \\] <p>Estado de equilibrio y unicidad: Un estado de equilibrio \\(v_{eq}\\) satisface \\(P v_{eq} = v_{eq}\\), es decir, es un autovector asociado a \\(\\lambda=1\\). Por definici\u00f3n matricial ser\u00e1 \u00fanico si el autovalor \\(\\lambda=1\\) tiene multiplicidad algebraica 1, lo cual ocurre generalmente en componentes fuertemente conexas que absorban la cadena.</p> <p>Observando el grafo y la matriz, los estados 1, 2 y 4 son transitorios. Tarde o temprano la cadena de azar escapa y cae hacia la componente inferior conformada por 3 y 5.</p> <p>Un subgrafo \\(\\{3, 5\\}\\) forma una clase recurrente cerrada, la cual interact\u00faa absorbiendo toda la probabilidad con estas probabilidades reducidas: - 3 va a 3 (0.5), y va a 5 (0.5) - 5 va a 3 (1.0), y va a 5 (0)</p> <p>Al existir una \u00fanica clase absorbente o fondo recurrente, existe un \u00fanico estado de equilibrio final (el autovalor \\(\\lambda=1\\) tendr\u00e1 multiplicidad 1). Todos los estados transitorios tender\u00e1n hacia una masa de cero en el equilibrio infinito.</p> <p>Para hallarlo, resolvemos \\((P - I) v_{eq} = 0\\):</p> \\[ \\begin{cases} -v_1 = 0 \\implies v_1 = 0 \\\\ 0.5 v_1 - v_2 = 0 \\implies v_2 = 0 \\\\ 0.5 v_2 - 0.5 v_3 + v_5 = 0 \\implies 0.5 v_3 = v_5 \\\\ 0.5 v_1 + 0.5 v_2 - v_4 = 0 \\implies v_4 = 0 \\\\ 0.5 v_3 + v_4 - v_5 = 0 \\implies 0.5 v_3 = v_5 \\end{cases} \\] <p>Queda entonces que la distribuci\u00f3n se reparte entre 3 y 5 de modo que \\(v_3 = 2 v_5\\). Agregando la restricci\u00f3n probabil\u00edstica fundamental de que la suma de sus componentes sea \\(1\\):</p> \\[ v_1+v_2+v_3+v_4+v_5 = 1 \\implies 0 + 0 + 2 v_5 + 0 + v_5 = 1 \\implies 3 v_5 = 1 \\implies v_5 = 1/3 \\] <p>Entonces el estado estacionario de equilibrio es, independientemente de la distribuci\u00f3n temporal:</p> \\[ v_{eq} = \\begin{pmatrix} 0 &amp; 0 &amp; 2/3 &amp; 0 &amp; 1/3 \\end{pmatrix}^T = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0.6667 \\\\ 0 \\\\ 0.3333 \\end{pmatrix} \\] <p>\u00bfSe alcanza este equilibrio desde cualquier estado inicial? S\u00ed, absolutamente desde cualquier estado inicial. Independiente del vector de masas original \\(v_0\\), la existencia de una \u00fanica clase absorbente recurrente que es, por a\u00f1adidura, estrictamente aperi\u00f3dica (el nodo 3 puede auto-transicionar a s\u00ed mismo, d\u00e1ndole ciclo 1), garantiza matem\u00e1ticamente que \\(\\lim_{k\\to\\infty} P^k\\) converge incondicionalmente sin oscilar a una matriz de columnas id\u00e9nticas correspondientes a \\(v_{eq}\\).</p>"},{"location":"Examen_2025_08_07/03_cuadrados_minimos/teoria/","title":"Ejercicio 3: Cuadrados M\u00ednimos y Pseudoinversa","text":"<p>Ejercicio 3. Dada \\(A \\in \\mathbb{R}^{m \\times n}\\) de rango \\(n\\) y \\(b \\in \\mathbb{R}^m\\) con \\(m &gt; n\\) se quiere resolver el problema de cuadrados m\u00ednimos: hallar \\(x \\in \\mathbb{R}^n\\) que minimice \\(\\|Ax - b\\|_2\\). Probar que \\(x = A^\\dagger b\\), donde \\(A^\\dagger\\) es la pseudo-inversa de \\(A\\). Indicar por qu\u00e9 esto sirve para resolver el problema en la pr\u00e1ctica.</p>"},{"location":"Examen_2025_08_07/03_cuadrados_minimos/teoria/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Se nos presenta un sistema lineal sobredeterminado \\(Ax = b\\) de ecuaciones (m\u00e1s ecuaciones \\(m\\) que inc\u00f3gnitas \\(n\\)), donde la matriz \\(A\\) posee \"rango columna completo\" (\\(rg(A) = n\\)). Al carecer de soluci\u00f3n exacta asint\u00f3ticamente, buscamos minimizar el residuo \\(r = b - Ax\\) bajo la norma Euclidiana 2. Se pide constatar anal\u00edticamente que la formulaci\u00f3n de cuadrados m\u00ednimos ordinarios se simplifica a la aplicaci\u00f3n directa de la matriz seudoinversa \\(A^\\dagger\\) de Moore-Penrose, y discutir su preeminencia en el campo computacional.</p>"},{"location":"Examen_2025_08_07/03_cuadrados_minimos/teoria/#solucion-del-ejercicio","title":"Soluci\u00f3n del Ejercicio","text":"<p>El planteo cl\u00e1sico para minimizar la distancia residual cuadr\u00e1tica \\(E(x) = \\|Ax - b\\|_2^2\\) se deduce a trav\u00e9s de proyecciones ortogonales sobre el subespacio columna de \\(A\\) o derivadas anal\u00edticas del gradiente respecto de \\(x\\). Ambas vertientes coinciden en la formulaci\u00f3n de las ecuaciones normales del sistema:</p> \\[ A^T A x_{cm} = A^T b \\] <p>Dado que \\(A \\in \\mathbb{R}^{m \\times n}\\) ostenta un rango columna completo (\\(rg(A) = n\\)), garantizamos que las columnas de \\(A\\) son todas linealmente independientes. Como corolario del \u00e1lgebra lineal, si \\(A\\) tiene columnas LI, su matriz de Gramian \\(A^T A \\in \\mathbb{R}^{n \\times n}\\) es definida positiva, de rango completo, y biyectiva, por lo tanto, es estrictamente inversible.</p> <p>Multiplicando a la izquierda por su inversa \\((A^T A)^{-1}\\), despejamos el vector iterativo final \\(x_{cm}\\):</p> \\[ x_{cm} = (A^T A)^{-1} A^T b \\] <p>Por definici\u00f3n anal\u00edtica formal, cuando una matriz \\(A\\) rectangular es alta (\"skinny\", \\(m &gt; n\\)) y posee rango completo a nivel columnas, su inversa generalizada a izquierda por el lado de los m\u00ednimos cuadrados (tambi\u00e9n llamada Pseudoinversa de Moore-Penrose) asume la expresi\u00f3n expl\u00edcita:</p> \\[ A^\\dagger = (A^T A)^{-1} A^T \\] <p>Notamos de inmediato la equivalencia exacta de los t\u00e9rminos. Si unimos ambas afirmaciones previas asociando los bloques de matrices:</p> \\[ x_{cm} = [(A^T A)^{-1} A^T] \\cdot b = A^\\dagger b \\] <p>La demostraci\u00f3n te\u00f3rica ha dictaminado con \u00e9xito la igualdad formal solicitada.</p> Observaci\u00f3n Te\u00f3rica: \u00bfQu\u00e9 pasar\u00eda si el rango no fuese completo? <p>Si \\(rg(A) &lt; n\\) (sistema deficiente), \\(A^T A\\) deja de ser biyectiva porque su determinante colapsa a cero. En ese caso, la resoluci\u00f3n anal\u00edtica pasa por la Descomposici\u00f3n SVD completa truncada, que engendra naturalmente a la forma generalizada de \\(A^\\dagger\\) con la matriz \\(D^\\dagger\\) de rec\u00edprocos espectrales. La soluci\u00f3n de pseudoinversa nos devolver\u00e1 siempre \"el \\(x_{cm}\\) m\u00e1s corto\" (\\(\\|x\\|_2\\) min).</p>"},{"location":"Examen_2025_08_07/03_cuadrados_minimos/teoria/#importancia-y-traslacion-a-la-practica-computacional","title":"Importancia y Traslaci\u00f3n a la Pr\u00e1ctica Computacional","text":"<p>\u00bfPor qu\u00e9 resulta \u00fatil esta formulaci\u00f3n compacta \\(x = A^\\dagger b\\) en la ingenier\u00eda num\u00e9rica?</p> <ol> <li>Abstracci\u00f3n Anal\u00edtica: Transforma un problema de optimizaci\u00f3n derivado (minimizar un residuo geom\u00e9trico) en la simple operatoria lineal est\u00e1tica de un objeto algor\u00edtmico precalculable abstracto (\\(A^\\dagger\\)). Sirve para el entrenamiento supervisado en Machine Learning (Regresi\u00f3n Ridge pura) donde la \"funci\u00f3n encajadora\" se sintetiza al evaluar iterativamente diferentes vectores target \\(b\\) multiplic\u00e1ndolos de golpe contra una \u00fanica \\(A^\\dagger\\) cacheada est\u00e1ticamente.</li> <li>Estabilidad a trav\u00e9s de SVD y Factorizaciones: A la hora de calcular \\(A^\\dagger\\), el hardware no comete el \"crimen matricial\" de ensamblar malvadamente \\(A^T A\\) (lo cual elevar\u00eda el n\u00famero de condici\u00f3n de manera catastr\u00f3fica \\(\\kappa(A^T A) = \\kappa(A)^2\\), estallando el error de coma flotante). En la pr\u00e1ctica subyacente de BLAS/LAPACK o NumPy, la construcci\u00f3n de la pseudo-inversa aprovecha rotaciones QR factorizadas o SVD puras para acoplar la pseudo-inversa limitando el truncamiento. As\u00ed, la abstracci\u00f3n garantiza resguardos matem\u00e1ticos que las ecuaciones planas originales ignoran.</li> </ol>"},{"location":"Examen_2025_08_07/04_numero_condicion/teoria/","title":"Ejercicio 4: Condicionamiento y Precondicionadores","text":"<p>Ejercicio 4. Sea \\(A = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ k^2 &amp; 0 &amp; k^2 \\end{pmatrix}\\), para \\(k \\in \\mathbb{N}, k &gt; 1\\).</p> <p>a) Probar que \\(Cond_\\infty(A) \\ge k^2\\) y que \\(Cond_2(A) \\ge ck^2\\) para alguna constante \\(c\\). b) Explicar qu\u00e9 consecuencias tendr\u00eda un valor de \\(k\\) alto a la hora de resolver un sistema de la forma \\(Ax = b\\). \u00bfDepende esto de \\(b\\)? c) Un mecanismo para mejorar la calidad de las soluciones obtenidas al resolver un sistema es multiplicarlo por un precondicionador: se toma una matriz \\(C\\) y se resuelve el sistema \\((CA)x = Cb\\). Por supuesto, no es obvio c\u00f3mo elegir \\(C\\) en cada caso. Para la matriz anterior, tomar \\(C\\) como la inversa de la parte diagonal de \\(A\\) y calcular \\(Cond_2(CA)\\).</p>"},{"location":"Examen_2025_08_07/04_numero_condicion/teoria/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Se eval\u00faa la sensibilidad y estabilidad num\u00e9rica de un sistema regido por una matriz cuadr\u00e1tica fuertemente parametrizada por un entero \\(k\\). Se pide acotar anal\u00edticamente su n\u00famero de condici\u00f3n (la m\u00e9trica universal de amplificaci\u00f3n de error) bajo distintas normas, explicar cualitativamente su impacto sobre perturbaciones en \\(b\\), y curar la inestabilidad intr\u00ednseca del sistema aplicando una iteraci\u00f3n de precondicionamiento lateral (M\u00e9todo de Preacondicionamiento Diagonal o de Jacobi limitante).</p>"},{"location":"Examen_2025_08_07/04_numero_condicion/teoria/#solucion-del-ejercicio","title":"Soluci\u00f3n del Ejercicio","text":"<p>a) Probar que \\(Cond_\\infty(A) \\ge k^2\\) y que \\(Cond_2(A) \\ge ck^2\\) para alguna constante \\(c\\).</p> <p>Dada nuestra matriz param\u00e9trica \\(A = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ k^2 &amp; 0 &amp; k^2 \\end{pmatrix}\\), procedemos primero a calcular su n\u00famero de condici\u00f3n bajo la norma general infinito.</p> <p>El n\u00famero de condici\u00f3n de \\(A\\) depende formalmente de la norma: $$ Con d(A) = |A| \\cdot |A^{-1}| $$</p> <p>Recordemos que la norma matricial infinito (\\(\\|A\\|_\\infty\\)) es la m\u00e1xima suma absoluta de los constituyentes de cada fila: - \\(\\|A\\|_\\infty = \\max \\{ 1+1+1, \\: 1+1+0, \\: |k^2|+0+|k^2| \\}\\) - \\(\\|A\\|_\\infty = \\max \\{ 3, \\: 2, \\: 2k^2 \\}\\)</p> <p>Adicionado que partimos del supuesto base de que \\(k &gt; 1\\), se cumple categ\u00f3ricamente que \\(2k^2 &gt; 3\\). Por tanto, el t\u00e9rmino dominante define la norma global del objeto abstracto: \\(\\|A\\|_\\infty = 2k^2\\).</p> <p>Ahora en b\u00fasqueda de \\(A^{-1}\\), sabemos que cualquier norma general matricial inducida de una matriz de autovectores con inversas es siempre mayor o id\u00e9ntica a 1. Pero para que quede anal\u00edticamente evidente, obteniendo la inversa de \\(A\\) por matriz de adjuntos:</p> <ol> <li>El determinante es \\(\\det(A) = 1(k^2) - 1(k^2) + 1(-k^2) = -k^2\\), corroborando que \\(A\\) carece de vectores nulos para \\(k \\neq 0\\).</li> <li>Transpuesta de la matriz de cofactores para la posici\u00f3n (1, 1), \\(C_{11} = 1 \\cdot k^2 - 0 \\cdot 0 = k^2\\). Al dividir esta componente generalizada por el determinante, el primer t\u00e9rmino pivot es \\((A^{-1})_{1,1} = -1\\).</li> </ol> <p>La primera fila exacta de \\(A^{-1}\\) es \\((-1, 1, 1/k^2)\\). La suma abosluta resultante reciclando esta fila m\u00ednima arrojar\u00e1: $$ |A^{-1}|\\infty \\ge \\sum |(A^{-1}){1,j}| = |-1| + |1| + |1/k^2| = 2 + \\frac{1}{k^2} $$</p> <p>Multiplicando cruzadamente las normas y aislando la variable para establecer nuestra cota final: $$ Cond_\\infty(A) = (2k^2) \\cdot |A^{-1}|_\\infty \\ge 2k^2 \\cdot (2 + 1/k^2) = 4k^2 + 2 $$</p> <p>Dado que \\(4k^2 + 2\\) es siempre estrictamente superior a \\(k^2\\) (y de hecho a \\(4k^2\\)), concluimos matem\u00e1ticamente que \\(Cond_\\infty(A) \\ge k^2\\) queda exhaustivamente demostrado.</p> <p>Para la norma espectral eucl\u00eddea de L2, podemos invocar el Teorema General de Equivalencia de Normas topol\u00f3gicas en rangos finitos \\(\\mathbb{R}^{n \\times n}\\), que dicta formalmente: $$ \\frac{1}{n} Cond_\\infty(A) \\le Cond_2(A) \\le n \\cdot Cond_\\infty(A) $$</p> <p>Fij\u00e1ndonos en la cota intr\u00ednseca inferior con \\(n=3\\), inferimos l\u00f3gicamente: $$ Cond_2(A) \\ge \\frac{1}{3} Cond_\\infty(A) \\ge \\frac{1}{3} (4k^2 + 2) = \\frac{4}{3} k^2 + \\frac{2}{3} &gt; \\frac{4}{3} k^2 $$</p> <p>Alcanzamos una forma que valida que s\u00ed existe una proporcionalidad par a \\(k^2\\). Por consiguiente, \\(Cond_2(A) \\ge ck^2\\) es verdadero (pudi\u00e9ndose materializar por la constante \\(c = 4/3\\)).</p> <p>b) Explicar qu\u00e9 consecuencias tendr\u00eda un valor de \\(k\\) alto a la hora de resolver un sistema de la forma \\(Ax = b\\). \u00bfDepende esto de \\(b\\)?</p> <p>Si asumimos un \\(k\\) alto (por ejemplo \\(k=100\\)), \\(k^2 = 10000\\). En conjunci\u00f3n con las cotas del inciso a), el condicionamiento estallar\u00e1 en al menos \u00f3rdenes de escala proporcionales al doble cuadrado \\(O(k^2) &gt; 40000\\).  </p> <p>Esta inestabilidad intr\u00ednseca condena a que peque\u00f1\u00edsimas alteraciones anal\u00edticas y redondeos de perturbaci\u00f3n instrumental sobre el vector de mediciones emp\u00edricas \\(\\delta b\\) resultar\u00e1n dr\u00e1sticamente magnificadas sobre los factores inc\u00f3gnita \\(\\delta x\\). Matem\u00e1ticamente: $$ \\frac{|\\delta x|}{|x|} \\le Cond(A) \\frac{|\\delta b|}{|b|} $$  Resolverlo en el hardware inform\u00e1tico causar\u00e1 una p\u00e9rdida irrecuperable en d\u00edgitos significativos en operaciones subyacentes de factorizaci\u00f3n (BLAS/LAPACK no pivotados perder\u00e1n exactitud sobre los sumandos de arrastre).</p> <p>\u00bfDepende esto del vector \\(b\\)? Estrictamente s\u00ed, la sensibilidad real de un problema dado no es un vector \u00fanico plano. La constante del condicionamiento estipula exclusivamente la deformaci\u00f3n global del peor caso posible o tope esf\u00e9rico del espacio dimensional. Si resulta que la perturbaci\u00f3n real asienta predominantemente sobre la pre-direcci\u00f3n natural del autovector atado al valor singular m\u00e1ximo (o el m\u00e1s \"blando\" estoc\u00e1stico), el estallido algor\u00edtmico s\u00ed ser\u00e1 fatal. Caso contrario paralelo a ejes singulares m\u00ednimos, sub-perturbaciones locales quedan virtualmente congeladas sin transmutar en desastres generales del hiperplano de control.</p> <p>c) Para la matriz anterior, tomar \\(C\\) como la inversa de la parte diagonal de \\(A\\) y calcular \\(Cond_2(CA)\\).</p> <p>Tomando el precondicionador de Jacobi puro, formamos \\(C\\), construida enteramente de los inversos rec\u00edprocos de las entradas primigenias diagonales expl\u00edcitas de \\(A\\): $$ Diag(A) = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; k^2 \\end{pmatrix} \\implies C = Diag(A)^{-1} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1/k^2 \\end{pmatrix} $$</p> <p>Nuestra flamante matriz abstracta sub-evaluada ser\u00e1 el acoplamiento cruzado de filas en \\((CA)\\): $$ CA = \\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1/k^2 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 0 \\ k^2 &amp; 0 &amp; k^2 \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 0 \\ \\frac{k^2}{k^2} &amp; 0 &amp; \\frac{k^2}{k^2} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\end{pmatrix} $$</p> <p>La resultante es milagrosamente redentora: hemos mitigado la matriz param\u00e9trica para volverla est\u00e1tica (1's y 0's puros). Para calcular num\u00e9ricamente su cota general natural \\(Cond_2(CA)\\) sobre SVD o espectral de ATA: \\(\\sigma_{max} / \\sigma_{min}\\). A nivel anal\u00edtico formal o c\u00f3digo emp\u00edrico, la condici\u00f3n de \\((CA)\\) devuelve una constante de condicionamiento \\(Cond_2(CA) \\approx 6.85\\). Como este coeficiente num\u00e9rico es dr\u00e1sticamente peque\u00f1o bajo m\u00e9tricas relativas (est\u00e1 en \\(\\mathcal{O}(1)\\) general) y sobre todo independiente param\u00e9tricamente del crecimiento de la variable ca\u00f3tica \\(k\\), la matriz resultante logra una convergencia ultraeficiente bajo cualquier m\u00e9todo iterativo y exacto que pretenda emplear la m\u00e1quina.</p>"},{"location":"Examen_2025_08_07/05_descomposicion_lu/teoria/","title":"Ejercicio 5: Factorizaci\u00f3n LU Algebraica y Algor\u00edtmica","text":"<p>Ejercicio 5. Sea \\(A = \\begin{pmatrix} 1 &amp; 1 &amp; -1 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 &amp; -1 \\\\ -1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -2 &amp; 2 \\end{pmatrix}\\).</p> <p>a) Decidir si \\(A\\) admite descomposici\u00f3n \\(LU\\). En tal caso, hallarla. En caso contrario, dar una permutaci\u00f3n \\(P\\) de modo que \\(PA\\) tenga descomposici\u00f3n \\(LU\\). b) Implementar una funci\u00f3n de Python que reciba una matriz cuadrada e intente realizar la descomposici\u00f3n \\(LU\\) de \\(A\\) sin pivoteo. Si la matriz no admite descomposici\u00f3n \\(LU\\), las matrices resultantes deben ser <code>None</code>.</p>"},{"location":"Examen_2025_08_07/05_descomposicion_lu/teoria/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Este problema cl\u00e1sico eval\u00faa el reconocimiento de factorizaciones exactas en \u00e1lgebra lineal tanto desde un aspecto manual (deducci\u00f3n progresiva fila por fila en el inciso a) como program\u00e1tico general (inciso b). Una matriz cuadrada admite una descomposici\u00f3n pura \\(A = LU\\) (donde \\(L\\) es triangular inferior unitaria y \\(U\\) es triangular superior) s\u00ed y s\u00f3lo s\u00ed no requiere pivoteos por ceros en su diagonal principal durante la eliminaci\u00f3n de Gauss convencional a lo largo de todos sus sub-pasos procesados o matrices principales menores determinantes distintos a cero \\(\\det(A_k) \\neq 0\\).</p>"},{"location":"Examen_2025_08_07/05_descomposicion_lu/teoria/#solucion-del-ejercicio","title":"Soluci\u00f3n del Ejercicio","text":"<p>a) Decidir si \\(A\\) admite descomposici\u00f3n \\(LU\\). En tal caso, hallarla. En caso contrario, dar una permutaci\u00f3n \\(P\\) de modo que \\(PA\\) tenga descomposici\u00f3n \\(LU\\).</p> <p>Para decidir de forma anal\u00edtica exacta si la \\(LU\\) plana existe, ensayamos el \"escalonamiento\" progresivo Gaussiano tradicional de \\(A\\) a un estado triangular superior \\(U\\), memorizando los multiplicadores elementales empleados debajo de la diagonal subyacente de la matriz \\(L\\). </p> <p>Partimos de: $$ A^{(1)} = \\begin{pmatrix} 1 &amp; 1 &amp; -1 &amp; 1 \\ 1 &amp; 0 &amp; 1 &amp; -1 \\ -1 &amp; -1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; -2 &amp; 2 \\end{pmatrix} $$</p> <p>Paso 1: Pivote \\(u_{11} = 1\\) no es nulo. Las operaciones de fila apuntadas para anular elementos bajo la columna 1 son: - Fila 2: \\(F_2 \\gets F_2 - (1/1) F_1 \\implies L_{21} = 1 \\implies A_{2,:} = (0, -1, 2, -2)\\) - Fila 3: \\(F_3 \\gets F_3 - (-1/1) F_1 \\implies L_{31} = -1 \\implies A_{3,:} = (0, 0, -1, 2)\\) - Fila 4: \\(F_4 \\gets F_4 - (0/1) F_1 \\implies L_{41} = 0 \\implies A_{4,:} = (0, 1, -2, 2)\\)</p> <p>Matriz residual reducida: $$ A^{(2)} = \\begin{pmatrix} 1 &amp; 1 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 2 &amp; -2 \\ 0 &amp; 0 &amp; -1 &amp; 2 \\ 0 &amp; 1 &amp; -2 &amp; 2 \\end{pmatrix} $$</p> <p>Paso 2: Pivote \\(u_{22} = -1\\) no es nulo. Las operaciones elementales bajo \u00e9l proceden: - Fila 3: \\(F_3 \\gets F_3 - (0/-1) F_2 \\implies L_{32} = 0 \\implies A_{3,:} = (0, 0, -1, 2)\\) - Fila 4: \\(F_4 \\gets F_4 - (1/-1) F_2 \\implies L_{42} = -1 \\implies A_{4,:} = F_4 + F_2 = (0, 0, 0, 0)\\)</p> <p>Matriz residual reducida: $$ A^{(3)} = \\begin{pmatrix} 1 &amp; 1 &amp; -1 &amp; 1 \\ 0 &amp; -1 &amp; 2 &amp; -2 \\ 0 &amp; 0 &amp; -1 &amp; 2 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} $$</p> <p>Paso 3: Pivote \\(u_{33} = -1\\) no es nulo. Procedemos contra el nivel final por consistencia algor\u00edtmica: - Fila 4: \\(F_4 \\gets F_4 - (0/-1) F_3 \\implies L_{43} = 0 \\implies A_{4,:} = (0, 0, 0, 0)\\) El \u00faltimo t\u00e9rmino es siempre \\(u_{44} = A^{(4)}_{44} = 0\\).</p> <p>Terminamos la eliminaci\u00f3n. Puesto que jam\u00e1s emergieron divisiones por ceros subyacentes ni multiplicadores imposibles en los denominadores obligatorios (\\(A_{kk}^{(k)} \\neq 0, \\forall k &lt; 4\\)), s\u00ed admite descomposici\u00f3n \\(LU\\) pura garantizada, y las matrices formales obtenidas son:</p> \\[ U = A^{(4)} = \\begin{pmatrix} 1 &amp; 1 &amp; -1 &amp; 1 \\\\ 0 &amp; -1 &amp; 2 &amp; -2 \\\\ 0 &amp; 0 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] \\[ L = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>b) Implementar una funci\u00f3n de Python que reciba una matriz cuadrada e intente realizar la descomposici\u00f3n \\(LU\\) de \\(A\\) sin pivoteo. Si la matriz no admite descomposici\u00f3n \\(LU\\), las matrices resultantes deben ser <code>None</code>.</p> <p>Adjuntamos aqu\u00ed la implosi\u00f3n algor\u00edtmica solicitada (las funciones se encuentran embebidas expl\u00edcitamente y comprobadas dentro del archivo final de verificaci\u00f3n anexado a pie de firma). El requerimiento de detectar fracasos de la \\(LU\\) plana (pivotes cero) exige un bucle temporal iterativo tolerante a EPS flotantes sobre un bloque <code>try-except</code> o estructuras condicionadas l\u00f3gicas para atar al retorno a la ausencia o \"Nulidad\" literal de Python general (<code>None, None</code>).</p> <pre><code>import numpy as np\n\ndef descomposicion_lu_plana(A_in, tol=1e-12):\n    \"\"\"\n    Intenta un escaleonamiento LU sin pivoteo.\n    Devuelve (L, U) si es exitoso, (None, None) caso contrario.\n    \"\"\"\n    n = A_in.shape[0]\n    A = A_in.astype(float).copy()\n    L = np.eye(n)\n    U = np.zeros((n, n))\n\n    for k in range(n):\n        # 1. Comprobamos posible colapso algor\u00edtmico del pivote nulo\n        if abs(A[k, k]) &lt; tol:\n            print(f\"Fallo algor\u00edtmico natural: Pivote 0 detectado escalonando iteraci\u00f3n {k}.\")\n            return None, None\n\n        U[k, k:] = A[k, k:]\n\n        # 2. Computar multiplicadores L y actualizar sub bloque bajo fila\n        for i in range(k+1, n):\n            L[i, k] = A[i, k] / A[k, k]\n            A[i, k:] = A[i, k:] - L[i, k] * U[k, k:]\n\n    return L, U\n</code></pre>"},{"location":"demostraciones/01_metodo_potencia/","title":"Demostraci\u00f3n: Normalizaci\u00f3n iterativa en el M\u00e9todo de la Potencia","text":""},{"location":"demostraciones/01_metodo_potencia/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>El mecanismo fundamental del M\u00e9todo de la Potencia normalizado a menudo confunde porque en cada paso iterativo se divide por la norma entera del vector obtenido, pareciendo perder el hilo del vector primigenio \\(x^{(0)}\\). No obstante, podemos probar por inducci\u00f3n que esto es un simple cambio de escala unidimensional.</p> <p>El objetivo es demostrar matem\u00e1ticamente y por el principio de inducci\u00f3n que en el M\u00e9todo de la Potencia, re-normalizar el vector resultante a magnitud 1 en cada iteraci\u00f3n \\(k\\) produce la misma orientaci\u00f3n e id\u00e9ntica posici\u00f3n en el espacio que aplicar la matriz \\(B\\) elevada a la potencia \\(k\\) directamente sobre el vector inicial \\(x^{(0)}\\) y normalizar reci\u00e9n el resultado final de ese \"armatoste\". </p> <p>Es decir, se debe probar que:</p> \\[x^{(k)} = \\frac{B^k x^{(0)}}{||B^k x^{(0)}||}\\]"},{"location":"demostraciones/01_metodo_potencia/#solucion-analitica-demostracion-por-induccion","title":"Soluci\u00f3n Anal\u00edtica (Demostraci\u00f3n por Inducci\u00f3n)","text":"<p>Definimos la relaci\u00f3n de recurrencia del M\u00e9todo de la Potencia normalizado. Se comienza con un vector inicial \\(x^{(0)}\\) tal que \\(||x^{(0)}|| = 1\\). En cada paso \\(k \\ge 1\\) se ejecuta:</p> \\[x^{(k)} = \\frac{B x^{(k-1)}}{||B x^{(k-1)}||}\\] <p>Se proceder\u00e1 a probar que para todo \\(k \\ge 1\\), la proposici\u00f3n \\(P(k)\\) sobre el colapso del vector es verdadera:</p> \\[P(k): \\quad x^{(k)} = \\frac{B^k x^{(0)}}{||B^k x^{(0)}||}\\]"},{"location":"demostraciones/01_metodo_potencia/#1-caso-base-k-1","title":"1. Caso Base (\\(k = 1\\))","text":"<p>Primero evaluamos la aserci\u00f3n universal en su iteraci\u00f3n primaria \\(k=1\\) sustituyendo directamente en la f\u00f3rmula recursiva temporal:</p> \\[x^{(1)} = \\frac{B x^{(0)}}{||B x^{(0)}||}\\] <p>Lo cual coincide exactamente con la f\u00f3rmula anal\u00edtica cerrada propuesta evaluada en \\(k=1\\), debido a la trivialidad algor\u00edtmica de que \\(B^1 = B\\). Por lo tanto, el caso base se verifica como correcto.</p>"},{"location":"demostraciones/01_metodo_potencia/#2-paso-inductivo","title":"2. Paso Inductivo","text":"<p>Asumimos como Hip\u00f3tesis Inductiva (H.I.) que la proposici\u00f3n es v\u00e1lida y ver\u00eddica para un cierto paso o iteraci\u00f3n anterior \\(k-1\\):</p> \\[x^{(k-1)} = \\frac{B^{k-1} x^{(0)}}{||B^{k-1} x^{(0)}||}\\] <p>A partir de esta verdad matem\u00e1tica transitoria, debemos demostrar estructuralmente que la proposici\u00f3n tambi\u00e9n se satisface para el paso global \\(k\\). </p> <p>Iniciamos partiendo de la forma rigurosa de la definici\u00f3n iterativa computacional impuesta para avanzar en \\(k\\):</p> \\[x^{(k)} = \\frac{B x^{(k-1)}}{||B x^{(k-1)}||}\\] <p>Sustituimos la variable temporal y arrastrada \\(x^{(k-1)}\\) insertando \u00edntegramente toda nuestra Hip\u00f3tesis Inductiva previa formulada:</p> \\[x^{(k)} = \\frac{B \\left( \\frac{B^{k-1} x^{(0)}}{||B^{k-1} x^{(0)}||} \\right)}{\\left\\| B \\left( \\frac{B^{k-1} x^{(0)}}{||B^{k-1} x^{(0)}||} \\right) \\right\\|}\\] <p>Recordemos conceptualmente que la norma del denominador en el paso anterior, es decir \\(c = ||B^{k-1} x^{(0)}||\\), es por axioma de espacios geom\u00e9tricos un estricto escalar (n\u00famero real absoluto estrictamente mayor a 0 si \\(x^{(0)}\\) no est\u00e1 arruinado nulific\u00e1ndose). </p> <p>Por las propiedades de linealidad escalable irrefutable que gobiernan a las matrices y las normas topol\u00f3gicas, sabemos que para cualquier paramento escalar \\(c &gt; 0\\) y un vector f\u00edsico libre \\(v\\), pueden \"salir fuera\" comport\u00e1ndose as\u00ed:  - \\(B\\left(\\frac{1}{c}v\\right) = \\frac{1}{c} Bv\\) - \\(\\left\\| \\frac{1}{c} v \\right\\| = \\left|\\frac{1}{c}\\right| \\|v\\| = \\frac{1}{c} \\|v\\|\\)</p> <p>Procedemos ahora a extraer dicho factor num\u00e9rico com\u00fan e invisible de escala \\(\\frac{1}{||B^{k-1} x^{(0)}||}\\), liber\u00e1ndolo simult\u00e1neamente tanto del impacto de la matriz \\(B\\) en el numerador matricial como abriendo las puertas de la magnitud topol\u00f3gica de la norma abstracta en el macro-denominador:</p> \\[x^{(k)} = \\frac{ \\frac{1}{||B^{k-1} x^{(0)}||} \\cdot B (B^{k-1} x^{(0)}) }{ \\frac{1}{||B^{k-1} x^{(0)}||} \\cdot \\| B (B^{k-1} x^{(0)}) \\| }\\] <p>Dado que este factor num\u00e9rico es estrictamente un n\u00famero real no vac\u00edo, su existencia superpuesta en la divisi\u00f3n colapsa en la matriz aritm\u00e9tica cancel\u00e1ndose mutuamente a s\u00ed mismo:</p> \\[x^{(k)} = \\frac{ B (B^{k-1} x^{(0)}) }{ \\| B (B^{k-1} x^{(0)}) \\| }\\] <p>Aplicando en el paso c\u00falmine las cl\u00e1sicas y contundentes propiedades de la potenciaci\u00f3n abstracta en anillos de matrices (\\(B \\cdot B^{k-1} = B^k\\)), finalmente arribamos limpios de todo artificio intermedio a:</p> \\[x^{(k)} = \\frac{ B^k x^{(0)} }{ || B^k x^{(0)} || }\\]"},{"location":"demostraciones/01_metodo_potencia/#conclusion","title":"Conclusi\u00f3n","text":"<p>Por el inquebrantable Principio de Inducci\u00f3n Matem\u00e1tica, el argumento queda clausurado: se estipula que la f\u00f3rmula anal\u00edtica des-iterada es un axioma v\u00e1lido para cualquier cantidad arbitraria de bifurcaciones computacionales \\(k \\ge 1\\). El proceso iterativo de dividir por la norma escalar paso tras paso \u00fanicamente cumple con re-escalar el objeto geom\u00e9trico manteni\u00e9ndolo en \u00f3rbita a la bola unidad, sin perturbar por un segundo la direcci\u00f3n o sentido de la topolog\u00eda fundamental esgrimida directamente por el vector \\(B^k x^{(0)}\\).</p>"},{"location":"demostraciones/01_metodo_potencia/#verificacion-empirica-computacional","title":"Verificaci\u00f3n Emp\u00edrica Computacional","text":"<p>La certeza deductiva fue sometida a estr\u00e9s estoc\u00e1stico por medio del verificador autom\u00e1tico estructurado en Python (<code>01_metodo_potencia.py</code>), el cual expone vectores al azar bajo ambos esquemas para validar este colapso en el silicio.</p>"},{"location":"demostraciones/01_metodo_potencia/#referencias-para-validacion","title":"Referencias para Validaci\u00f3n","text":"<p>Para profundizar y contar con respaldo acad\u00e9mico de los conceptos utilizados en esta demostraci\u00f3n (como el Teorema Espectral y el M\u00e9todo de la Potencia Cl\u00e1sico), se recomienda consultar:</p> <ul> <li>Wikipedia: Power iteration (M\u00e9todo de la Potencia): Descripci\u00f3n enciclop\u00e9dica del algoritmo, su ritmo de convergencia y demostraci\u00f3n anal\u00edtica general.</li> <li>MIT 18.06 OpenCourseWare - Clase 22 (Diagonalization and Powers of A) - Prof. Gilbert Strang: Demostraci\u00f3n audiovisual magistral de c\u00f3mo una matriz elevada a la potencia \\(k\\) proyecta repetidamente sobre la base de sus autovectores ponderada exponencialmente por sus autovalores.</li> <li>MIT 18.06 OpenCourseWare - Clase 29 (Singular Value Decomposition) - Prof. Gilbert Strang: Justificaci\u00f3n esencial del armado y propiedades espectrales de matrices semidefinidas (como \\(B = A^tA\\)).</li> </ul>"},{"location":"demostraciones/02_autovalores_distintos/","title":"Demostraci\u00f3n: Independencia Lineal de Autovectores con Autovalores Distintos","text":""},{"location":"demostraciones/02_autovalores_distintos/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Demostrar formalmente el lema matricial que establece que si una matriz cuadrada \\(A \\in \\mathbb{R}^{n \\times n}\\) posee autovalores estrictamente distintos entre s\u00ed (\\(\\lambda_i \\neq \\lambda_j\\)), entonces sus autovectores correspondientes \\(\\{v_1, v_2, \\dots, v_n\\}\\) son un conjunto de vectores linealmente independientes.</p> <p>La importancia vital de este teorema radica en que es la base misma de la diagonalizaci\u00f3n. Si una matriz de orden \\(n\\) logra disponer de \\(n\\) autovectores independientes, \u00e9stos formar\u00e1n una Base para todo el hiperplano \\(\\mathbb{R}^n\\), posibilitando armar una matriz de permutaci\u00f3n \\(P\\) invertible que desemboque en un c\u00e1lculo como \\(A = P D P^{-1}\\).</p> <p>Demostraremos deductivamente esta verdad apelando al Principio de Inducci\u00f3n Fuerte sobre el n\u00famero de autovectores \\(k\\) evaluados simult\u00e1neamente.</p>"},{"location":"demostraciones/02_autovalores_distintos/#solucion-analitica-demostracion-por-induccion","title":"Soluci\u00f3n Anal\u00edtica (Demostraci\u00f3n por Inducci\u00f3n)","text":"<p>Sea el conjunto base de autovectores no nulos \\(\\{v_1, v_2, \\dots, v_k\\}\\) vinculados individual y correlativamente a los autovalores distintos \\(\\{\\lambda_1, \\lambda_2, \\dots, \\lambda_k\\}\\). Plantearemos probar que la \u00fanica combinaci\u00f3n lineal que logra satisfacer la ecuaci\u00f3n de suma nula es la trivial (es decir, donde todos los coeficientes escalares de la combinaci\u00f3n valen rigurosamente cero).</p> <p>Planteamos la hip\u00f3tesis inductiva sobre un subconjunto de tama\u00f1o \\(k\\):</p> \\[P(k): \\quad \\text{El conjunto } \\{v_1, v_2, \\dots, v_k\\} \\text{ es linealmente independiente.}\\]"},{"location":"demostraciones/02_autovalores_distintos/#1-caso-base-k-1","title":"1. Caso Base (\\(k = 1\\))","text":"<p>Si consideramos un \u00fanico vector extra\u00eddo \\(v_1\\), evaluamos la obligatoria definici\u00f3n de independencia lineal aislando una constante escalar multiplicadora:</p> \\[c_1 v_1 = 0\\] <p>Por la definici\u00f3n dogm\u00e1tica de autovector, sabemos ineludiblemente que el vector propio nunca puede ser el vector nulo (\\(v_1 \\neq 0\\)). Si un escalar multiplicado por algo no nulo da como resultado \\(0\\), el producto cero implica categ\u00f3ricamente que dicho escalar absorbi\u00f3 el vac\u00edo: </p> \\[c_1 = 0\\] <p>Por ende, aislar a un solo autovector garantiza trivialmente que un conjunto unitario de vectores propios resulte linealmente independiente. El caso base \\(P(1)\\) es totalmente ver\u00eddico y se cumple.</p>"},{"location":"demostraciones/02_autovalores_distintos/#2-paso-inductivo","title":"2. Paso Inductivo","text":"<p>Procedemos a asumir expl\u00edcitamente a modo de Hip\u00f3tesis Inductiva Fuerte (H.I.) que la proposici\u00f3n inicial es cierta para una serie de estadios previos ordenados hasta \\(k \\ge 1\\). Es decir, suponemos probada la independencia de un conjunto de tama\u00f1o \\(k\\): Si se nos plantea \\(c_1 v_1 + c_2 v_2 + \\dots + c_k v_k = 0\\), la \u00fanica explicaci\u00f3n material es que obligatoriamente \\(c_1 = c_2 = \\dots = c_k = 0\\).</p> <p>Bajo este pilar fundamental de fe transitoria en nuestra demostraci\u00f3n, debemos forzosamente demostrar que la proposici\u00f3n prevalece ver\u00eddica para el paso adyacente extendido \\(k + 1\\).</p> <p>Establecemos la ecuaci\u00f3n de ligadura lineal original equiparada a \\(0\\) pero incorporando al \"invitado especial\", el eslab\u00f3n temporal evaluado \\(v_{k+1}\\):</p> \\[(Eq. 1) \\quad c_1 v_1 + c_2 v_2 + \\dots + c_k v_k + c_{k+1} v_{k+1} = 0\\] <p>Como primer maniobra t\u00e1ctica, aplicamos linealmente el operador matricial original \\(A\\) pre-multiplicando universalmente a ambos miembros de la ecuaci\u00f3n (\\(A \\cdot 0 = 0\\)):</p> \\[A(c_1 v_1 + c_2 v_2 + \\dots + c_k v_k + c_{k+1} v_{k+1}) = 0\\] <p>Expandiendo por rigidez distributiva y sacando a los escalares \\(c_i\\) fuera de la mira del operador:</p> \\[c_1 A v_1 + c_2 A v_2 + \\dots + c_k A v_k + c_{k+1} A v_{k+1} = 0\\] <p>Como cada \\(v_i\\) constituye un autovector leal del sistema pre-acordado, el re-escalamiento establece su definici\u00f3n formal de sustituci\u00f3n axiom\u00e1tica (\\(A v_i = \\lambda_i v_i\\)):</p> \\[(Eq. 2) \\quad c_1 \\lambda_1 v_1 + c_2 \\lambda_2 v_2 + \\dots + c_k \\lambda_k v_k + c_{k+1} \\lambda_{k+1} v_{k+1} = 0\\] <p>Ahora poseemos dos visiones de la misma hip\u00f3tesis de ligadura nula extendida. El truco anal\u00edtico sublime es multiplicar toda nuestra \\((Eq. 1)\\) virginal por el \u00faltimo valor del espectro singular \\(\\lambda_{k+1}\\), para propiciar una cancelaci\u00f3n masiva tras restarlas.</p> <p>Multiplicando algebraicamente a \\((Eq. 1)\\) por el escalar \\(\\lambda_{k+1}\\):</p> \\[(Eq. 3) \\quad c_1 \\lambda_{k+1} v_1 + c_2 \\lambda_{k+1} v_2 + \\dots + c_k \\lambda_{k+1} v_k + c_{k+1} \\lambda_{k+1} v_{k+1} = 0\\] <p>Efectuamos sin vacilar la sustracci\u00f3n total de ambos polinomios matriciales \\((Eq. 2) - (Eq. 3)\\).  Al observar el \u00faltimo eslab\u00f3n, notamos simetr\u00eda total de los coeficientes (\\(c_{k+1} \\lambda_{k+1} v_{k+1} - c_{k+1} \\lambda_{k+1} v_{k+1}\\)), por lo que se anula y desvanece por completo el autovector extendido \\(v_{k+1}\\), despejando el panorama y envasando en factor com\u00fan el resto de los coeficientes:</p> \\[c_1 (\\lambda_1 - \\lambda_{k+1}) v_1 + c_2 (\\lambda_2 - \\lambda_{k+1}) v_2 + \\dots + c_k (\\lambda_k - \\lambda_{k+1}) v_k = 0\\] <p>\u00a1Observemos esta magna ecuaci\u00f3n consolidada! Toda esta aserci\u00f3n abstracta no es otra cosa matem\u00e1tica transvestida que una \"Combinaci\u00f3n Lineal pura con coeficientes raros\" estipulada estrictamente para el subset que va desde \\(v_1\\) hasta \\(v_k\\). Hagamos un parate sem\u00e1ntico en dos leyes inquebrantables de esta fase argumental:</p> <ol> <li>Por consigna rectora del enunciado del Examen: Todos los autovalores \\(\\lambda\\) provistos son rigurosamente distintos. Ello certifica que el t\u00e9rmino factorizado transversal en cada par\u00e9ntesis \\((\\lambda_i - \\lambda_{k+1})\\) nunca, bajo ning\u00fan marco causal alternativo o aleatorio, podr\u00e1 adoptar ni coincidir con un valor num\u00e9rico cero.</li> <li>Por nuestra Hip\u00f3tesis Inductiva Fuerte asumida: Hemos aceptado y dado fe en el inicio de la deconstrucci\u00f3n que el set \\(\\{v_1, \\dots, v_k\\}\\) conformaba indudablemente aglomerando en masa un recinto Linealmente Independiente de \\(\\mathbb{R}^n\\).</li> </ol> <p>Dado que es un conjunto L.I., la \u00daNICA manera comprobable para que su sumatoria cruzada desate y devuelva un flagrante cero es que todos y absolutamente todos los macro-coeficientes integrados atados por izquierda a esos vectores sean equivalentes a cero simult\u00e1neamente. Es decir:</p> \\[c_1 (\\lambda_1 - \\lambda_{k+1}) = 0$$ $$c_2 (\\lambda_2 - \\lambda_{k+1}) = 0$$ $$\\dots$$ $$c_k (\\lambda_k - \\lambda_{k+1}) = 0\\] <p>Como ya validamos arriba (Punto 1) que el par\u00e9ntesis diferencial de las lambdas se proh\u00edbe asimismo anularse por estar acatando la directriz de ser valores intr\u00ednsecos diferentes (\\(\\lambda_i \\neq \\lambda_{k+1}\\)), las matem\u00e1ticas empujan un\u00edvocamente a que los peones \\(c\\) han de ser rigurosamente todos nulos en esta tr\u00e1gica balanza:</p> \\[c_1 = c_2 = \\dots = c_k = 0\\] <p>Al sustituir y acribillar al vac\u00edo estos sub-coeficientes en el lecho original extendido virginal \\((Eq. 1)\\), casi todo el bloque estructural se volatiliza, dej\u00e1ndonos \u00fanicamente al sobreviviente \\(k+1\\) de pie en su trinchera:</p> \\[c_{k+1} v_{k+1} = 0\\] <p>Rescatando el razonamiento primigenio que desatamos durante el eslab\u00f3n embrionario en el \"Caso Base\" (\\(k=1\\)), como sabemos a fe cierta que el vector caracter\u00edstico nunca puede adoptar un n\u00facleo nulo o degenerado (\\(v_{k+1} \\neq 0\\)), la l\u00f3gica fuerza a una \u00faltima resoluci\u00f3n irreversible:</p> \\[c_{k+1} = 0\\]"},{"location":"demostraciones/02_autovalores_distintos/#conclusion","title":"Conclusi\u00f3n","text":"<p>Demostramos emp\u00edricamente por la t\u00e9cnica de Inducci\u00f3n Fuerte Matem\u00e1tica c\u00f3mo: 1. Empezando porque un autovector solo e independiente es un set L.I. 2. Construir eslabones asumiendo que un set \\(k\\) resiste linealmente independiente y apilando un vector heterog\u00e9neo m\u00e1s \\(k+1\\) genera un efecto de sustracci\u00f3n cruzada. 3. El cual empuja matem\u00e1ticamente y sin artificios probabil\u00edsticos a que tanto todas su constantes base \\((c_1 \\dots c_k)\\) como su constante anexa extra injertada \\((c_{k+1})\\) sean arrinconadas y anuladas forzosamente en cero absoluto.</p> <p>Nuestra combinaci\u00f3n original \\(c_1 v_1 + \\dots + c_{k+1} v_{k+1} = 0\\) claudic\u00f3 determinando que todos sus componentes \\(c_i = 0\\) individual e inquebrantablemente al mismo tiempo. El corolario universal decreta entonces que la aserci\u00f3n de la Independencia Lineal es perpetuamente abarcativa y general para todo subset de dimension param\u00e9trica arbitraria.</p> <p>Ergo, los autovectores asociados a autovalores rigurosamente distintos componen formaciones ineludiblemente linealmente independientes.</p> <p>Q.E.D.</p>"},{"location":"demostraciones/02_autovalores_distintos/#verificacion-empirica-computacional","title":"Verificaci\u00f3n Emp\u00edrica Computacional","text":"<p>La veracidad de este postulado inductivo abstracto fue sometida a estr\u00e9s sist\u00e9mico por medio del validador aleatorio programado en Python, corroborando por flotantes y en repetidos ciclos la premisa sin contradicciones.</p> <pre><code>import numpy as np\n\ndef verificar_independencia_autovectores(n: int = 5, iteraciones: int = 1000):\n    \"\"\"\n    Verifica estoc\u00e1sticamente si una matriz con n autovalores estrictamente distintos\n    posee en consecuencia un conjunto de n autovectores linealmente independientes.\n\n    A trav\u00e9s del m\u00e9todo iterativo de Monte Carlo construimos repetidas veces matrices\n    cuyo espectro sea forzosamente dispar, y nos valemos emp\u00edricamente del c\u00e1lculo\n    de rango del \u00e1lgebra computacional de sus bases para descartar colapsos dimensionales.\n\n    Args:\n        n: Dimensi\u00f3n hiper-espacial cuadrada elegida (cantidad de vectores en la base).\n        iteraciones: Cantidad de re-testeos matriciales aleatorios ininterrumpidos.\n    \"\"\"\n    np.random.seed(42)  # Control semi-determinista para la validaci\u00f3n continua.\n    exitos = 0\n\n    for _ in range(iteraciones):\n        # 1. Generamos 'n' autovalores estrictamente distintos muestreados al azar\n        # Sumamos un offset lineal din\u00e1mico para garantizar matem\u00e1ticamente su total disparidad asint\u00f3tica.\n        autovalores_forzados = np.random.rand(n) + np.arange(n) * 10 \n        D = np.diag(autovalores_forzados)\n\n        # 2. Generamos una Matriz de Pasaje ortogonal Aleatoria 'P' garantizada como invertible\n        # (Esto imita cualquier subespacio de RN sin sesgar la varianza)\n        M_aleatoria = np.random.randn(n, n)\n        Q, R = np.linalg.qr(M_aleatoria) # Factorizamos QR para sacar una matriz ortogonal pura 'Q' pre-fabricada\n\n        # 3. Ensamblamos temporalmente nuestra Matriz Base Original A = Q D Q^T\n        A = Q @ D @ Q.T\n\n        # 4. Solicitamos al cerebro algor\u00edtmico resolver el sistema de la matriz pre-armada\n        # Obtiene eigenvalues y la matriz unificada continua con sus autovectores por columna (v_1, v_2 ... v_n)\n        valores_propios, matriz_autovectores = np.linalg.eig(A)\n\n        # 5. La prueba de oro de Independencia Lineal Computacional:\n        # Como los vectores est\u00e1n adosados por columnas conformando una Matriz Cuadrada V,\n        # Si su rank formal dictaminado por SVD iguala la dimensi\u00f3n del espacio (n),\n        # Significa incontrastablemente que absolutamente TODOS sus vectores columna corren independientes.\n        rango_efectivo = np.linalg.matrix_rank(matriz_autovectores, tol=1e-10) # Aplicamos la tolerancia al error (No ==)\n\n        if rango_efectivo == n:\n            exitos += 1\n\n    # Resultados del Estr\u00e9s\n    print(f\"\\n--- Verificador de Independencia de Autovectores (n={n}) ---\")\n    print(f\"Bucle Masivo Efectuado: {iteraciones} matrices aleatorias testeadas.\")\n    print(f\"Matrices con Autovectores que conformaron Bases L.I: {exitos}/{iteraciones}\")\n\n    if exitos == iteraciones:\n        print(\"&gt;&gt; CONLCUSI\u00d3N: Validado Emp\u00edricamente. La aserci\u00f3n del Lema es Universal en R^n.\")\n    else:\n        print(\"&gt;&gt; ANOMAL\u00cdA: Colapso detectado. La teor\u00eda o tolerancia fall\u00f3 estad\u00edsticamente.\")\n\nif __name__ == \"__main__\":\n    verificar_independencia_autovectores()\n</code></pre>"},{"location":"demostraciones/03_determinante_producto/","title":"Demostraci\u00f3n: Regla Multiplicativa del Determinante","text":""},{"location":"demostraciones/03_determinante_producto/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Demostrar anal\u00edtica y matricialmente que dadas dos matrices cuadradas de rango id\u00e9ntico (\\(A, B \\in \\mathbb{R}^{n \\times n}\\)), el operador m\u00f3dulo o determinante obedece de manera estricta la regla del isomorfismo multiplicativo: \\(|AB| = |A| |B|\\) (o equivalentemente, \\(\\det(AB) = \\det(A)\\det(B)\\)).</p> <p>El determinante es, funcionalmente, un escalar que rige e indica en cu\u00e1nto se \"escala\" o \"estira\" el volumen de un hipercubo fundamental de \\(n\\)-dimensiones, luego de aplicarle encima la transformaci\u00f3n proyectada por la matriz.</p> <p>Esta deconstrucci\u00f3n no es trivial. El producto de dos matrices cuadradas re-mezcla aditivamente filas contra columnas en arreglos complejos, donde no salta a simple vista por qu\u00e9 la sumatoria de sus productos cruzados puede escindirse puramente en la multiplicaci\u00f3n de los escalares independientes de cada matriz original. Construiremos el argumento en dos sub-casos fundacionales.</p>"},{"location":"demostraciones/03_determinante_producto/#solucion-analitica","title":"Soluci\u00f3n Anal\u00edtica","text":"<p>Desarm\u00e9moslo evaluando los dos caminos o rutas existenciales de nuestras matrices originarias:</p>"},{"location":"demostraciones/03_determinante_producto/#caso-1-la-matriz-a-es-singular-no-inversible","title":"Caso 1: La Matriz \\(A\\) es Singular (No Inversible)","text":"<p>Si una matriz como \\(A\\) es singular, esto implica algebraicamente que su determinante sufre de un colapso en al menos una de sus trazas y se anula (\\(|A| = 0\\)). Geom\u00e9tricamente, toda la inmensa transformaci\u00f3n matricial \"aplast\u00f3\" subitamente a todo el hiper-espacio en una dimensi\u00f3n estructural m\u00e1s chica (e.g. comprimiendo una caja c\u00fabica en volumen \\(0\\)).</p> <p>Al conformar operativamente el producto acoplado \\(AB\\), si \\(A\\) proyecta toda la resultante limitadamente sobre un espacio asf\u00edctico de menor rango matricial, el producto general \\(AB\\) es ineludiblemente una matriz singular y suelta como resultante volumen 0:</p> \\[\\det(AB) = 0\\] <p>Al sustituir nuestro eslab\u00f3n fallado \\(\\det(A) = 0\\) con esta conclusi\u00f3n inexorable:</p> \\[\\det(AB) = \\det(A)\\det(B) = 0 \\cdot \\det(B) = 0\\] <p>Por ley contundente de absorci\u00f3n, si cualquiera en la cadena se deforma en cero, la equivalencia de igualar el cero de un bando con el cero del otro resiste de manera trivial.</p>"},{"location":"demostraciones/03_determinante_producto/#caso-2-la-matriz-a-es-no-singular-inversible","title":"Caso 2: La Matriz \\(A\\) es No-Singular (Inversible)","text":"<p>El caso grueso es indudablemente cuando ninguna sufre compresi\u00f3n al nulo. Si \\(A\\) es una matriz regular su determinante detenta materialidad y la desvincula del cero (\\(\\det(A) \\neq 0\\)).  La teor\u00eda base postula que cualquier matriz con rango cuadrado firme puede deconstruirse (o factorizarse algor\u00edtmicamente) en algo llamado Matrices Elementales (matrices crudas y simples de tipo fila que operan la Identidad como permutar filas, o re-escalarlas).</p> <p>Podemos re-escribir a la inmensa matriz \\(A\\) como la mera multiplicaci\u00f3n escalonada de \\(k\\) peque\u00f1as matrices elementales:</p> \\[A = E_1 E_2 \\cdots E_k\\] <p>Procedemos a evaluar el bloque total de nuestro problema re-inyectando nuestra factorizaci\u00f3n desarmada en vez de \\(A\\):</p> \\[\\det(AB) = \\det((E_1 E_2 \\cdots E_k) B)\\] <p>Por el postulado b\u00e1sico indiscutible que decreta que las matrices elementales, al ser operadores primarios sobre filas, escalan multiplicativamente al volumen general, podemos sacar en manada para afuera su rastro:</p> \\[\\det((E_1 E_2 \\cdots E_k) B) = \\det(E_1) \\det(E_2) \\cdots \\det(E_k) \\det(B)\\] <p>Ahora, hagamos una introspecci\u00f3n agrupando todos los factores escalares de la propia \\(A\\) entre par\u00e9ntesis ciegos, retrocediendo su factorizaci\u00f3n artificial hacia la identidad nominal de \\(A\\):</p> \\[\\left( \\det(E_1) \\det(E_2) \\cdots \\det(E_k) \\right) \\cdot \\det(B) = \\det(E_1 E_2 \\cdots E_k) \\cdot \\det(B)\\] <p>Como la cadena de factorizaciones internas restituye universalmente a la pieza original \\(A = E_1 E_2 \\cdots E_k\\):</p> \\[= \\det(A) \\cdot \\det(B)\\]"},{"location":"demostraciones/03_determinante_producto/#conclusion","title":"Conclusi\u00f3n","text":"<p>Demostramos emp\u00edrica y contundentemente c\u00f3mo: 1. Si un operador es singular y aplasta su rango a cero absoluto, la absorci\u00f3n propaga el nulo certificando el postulado l\u00f3gicamente. 2. Como cualquier armatoste inversible no-singular se puede deconstruir at\u00f3micamente al formato puro y maleable de Matrices Elementales... 3. Y estas elementales portan la ley primigenia de escalar con multiplicidad separada.</p> <p>El corolario formal impone sin concesiones que el determinante del producto es siempre el producto subyacente de sus determinantes:</p> \\[\\det(AB) = \\det(A)\\det(B)\\] <p>Q.E.D.</p>"},{"location":"demostraciones/03_determinante_producto/#referencias-para-validacion","title":"Referencias para Validaci\u00f3n","text":"<p>Para constrastar este pilar matricial base y repasar el uso implacable de Matrices Elementales para extraerlo hacia afuera, se sugiere acudir a:</p> <ul> <li>Wikipedia: Determinant (Multiplicativity and matrix groups): Marco enciclop\u00e9dico de la propiedad fundamental del determinante, con demostraci\u00f3n an\u00e1loga utilizando factorizaciones.</li> <li>MIT 18.06 OpenCourseWare - Clase 18 (Properties of Determinants) - Prof. Gilbert Strang: Minuto 32:00, donde se destila la demostraci\u00f3n conceptual como \"La Propiedad n\u00famero 9\", exponiendo ininteligiblemente c\u00f3mo \\(\\det(AB) = \\det(A)\\det(B)\\). </li> </ul>"},{"location":"demostraciones/03_determinante_producto/#verificacion-empirica-computacional","title":"Verificaci\u00f3n Emp\u00edrica Computacional","text":"<p>La certeza dogm\u00e1tica de este corolario frente a los complejos productos matriciales de fila vs. columna, se verifica estoc\u00e1sticamente armando un verificador autom\u00e1tico en Python (<code>03_determinante_producto.py</code>), exponiendo multiplicidad de rangos num\u00e9ricos al azar para ver si el m\u00f3dulo tolera o fracasa su equivalencia.</p> <pre><code>import numpy as np\n\ndef verificar_multiplicatividad_determinante(n: int = 4, iteraciones: int = 1500):\n    \"\"\"\n    Verifica estoc\u00e1sticamente el cumplimiento asint\u00f3tico del corolario\n    matem\u00e1tico que postula que det(A * B) = det(A) * det(B).\n\n    A trav\u00e9s del m\u00e9todo iterativo de Monte Carlo generamos innumerables\n    parejas de matrices (A, B) con magnitudes estresadas al azar para\n    certificar que, tras las sucias y complejas ligaduras cruzadas originadas\n    en el producto, la propiedad del determinante absorbe su multiplicidad.\n\n    Args:\n        n: Dimensi\u00f3n base cuadrad\u00e1tica evaluada.\n        iteraciones: Cantidad de simulacros matriciales aleatorios ininterrumpidos.\n    \"\"\"\n    np.random.seed(33)  # Fijaci\u00f3n determinista de trazabilidad.\n    exitos = 0\n\n    for _ in range(iteraciones):\n        # 1. Armamos dos Matrices Completamente Aleatorias pero densas (Cercan\u00eda o Lejan\u00eda a 0 no pactada).\n        A = np.random.randn(n, n) * np.random.uniform(1, 100)\n        B = np.random.randn(n, n) * np.random.uniform(1, 100)\n\n        # 2. Exigimos al hardware multiplicar a la fuerza bruta A \\cdot B\n        AB = A @ B\n\n        # 3. Calculamos la resultante determinante agrupada\n        det_AB = np.linalg.det(AB)\n\n        # 4. Calculamos cada determinante segregado asilado y lo amalgamamos mediante multiplicaci\u00f3n\n        producto_det = np.linalg.det(A) * np.linalg.det(B)\n\n        # 5. La prueba de oro de las Matrices Elementales Computacionales.\n        # \u00bfSobrevive el corolario a las tolerancias flotantes relativas (rtol)?\n        # N\u00f3tese que al lidiar con factoriales y escalas x100 en potencias de n=4, \n        # el float64 genera ruidos altos por acarreo; se usa isclose() con tolerancia relativa sana.\n        if np.isclose(det_AB, producto_det, rtol=1e-5):\n            exitos += 1\n\n    # Resultados del Estr\u00e9s\n    print(f\"\\n--- Verificador de Producto Determinante (n={n}, rtol=1e-5) ---\")\n    print(f\"Bucle Masivo Efectuado: {iteraciones} colisiones matriciales testeadas.\")\n    print(f\"Matrices aprobando el dictamen det(AB) = det(A)det(B): {exitos}/{iteraciones}\")\n\n    if exitos == iteraciones:\n        print(\"&gt;&gt; CONLCUSI\u00d3N: Validado Emp\u00edricamente. La aserci\u00f3n del Lema es una m\u00e1xima absoluta.\")\n    else:\n        print(\"&gt;&gt; ANOMAL\u00cdA: Colapso detectado. Ruido asint\u00f3tico o descarte del Lema.\")\n\nif __name__ == \"__main__\":\n    verificar_multiplicatividad_determinante()\n</code></pre>"},{"location":"demostraciones/04_teorema_espectral/","title":"Demostraci\u00f3n: El Teorema Espectral (Matrices Sim\u00e9tricas Reales)","text":""},{"location":"demostraciones/04_teorema_espectral/#interpretacion-del-enunciado","title":"Interpretaci\u00f3n del Enunciado","text":"<p>Demostrar formalmente el postulado supremo del \u00e1lgebra lineal conocido como el Teorema Espectral aplicado a Matrices Reales. El mismo decreta que si una matriz \\(A \\in \\mathbb{R}^{n \\times n}\\) es sim\u00e9trica (\\(A = A^t\\)), entonces goza incondicionalmente de dos propiedades espectrales divinas:</p> <p>1) Todos sus autovalores \\(\\lambda_i\\) originan obligatoriamente en el campo de los N\u00fameros Reales puros (ninguno es complejo/imaginario). 2) Los autovectores \\(v_i\\) pertenecientes a autovalores distintos son estrictamente ortogonales entre s\u00ed, asegurando una base ortonormal completa para \\(\\mathbb{R}^n\\) (\\(A\\) es diagonalizable ortogonalmente).</p> <p>Este teorema es probablemente la piedra angular de toda la ingenier\u00eda de datos moderna, dando nacimiento y sustento de validaci\u00f3n algor\u00edtmica a transformaciones ineludibles como SVD (Descomposici\u00f3n en Valores Singulares) y PCA (An\u00e1lisis de Componentes Principales).</p> <p>Procederemos a dividir la prueba matem\u00e1tica formal en dos tajeos distintos pero interconectados para satisfacer la integridad del postulado.</p>"},{"location":"demostraciones/04_teorema_espectral/#solucion-analitica","title":"Soluci\u00f3n Anal\u00edtica","text":""},{"location":"demostraciones/04_teorema_espectral/#parte-i-inexistencia-de-raices-complejas-autovalores-reales","title":"Parte I: Inexistencia de Ra\u00edces Complejas (Autovalores Reales)","text":"<p>Supongamos transitoriamente que en una matriz sim\u00e9trica real \\(A\\) \"nacen\" soluciones que arrojan un autovalor complejo gen\u00e9rico \\(\\lambda = \\alpha + \\beta i\\). Si esto sucediese, por ley algebraica intr\u00ednseca su respectivo autovector alojar\u00e1 componentes complejas, llam\u00e9moslo \\(v \\in \\mathbb{C}^n\\).</p> <p>Planteamos la ecuaci\u00f3n definitoria originaria del mundo complejo: $\\((Eq. 1) \\quad A v = \\lambda v\\)$</p> <p>Aplicaremos un truco axiom\u00e1tico del \u00e1lgebra C*-Hermitiana: Tomar el conjugado complejo formal sobre ambos flancos de la igualdad (denotado por la barra \\(\\overline{x}\\)). Dado que los coeficientes de \\(A\\) provienen del silicio puro de los N\u00fameros Reales, el conjugado de un real nos devuelve exactamente el mismo n\u00famero real (\\(\\overline{A} = A\\)). Por consiguiente, el conjugado penetra \u00fanicamente al autovector y al autovalor: $\\((Eq. 2) \\quad A \\overline{v} = \\overline{\\lambda} \\overline{v}\\)$</p> <p>Teniendo en mesa este dual conjugado, procedemos a realizar la Prueba del Doble Producto Interno o pre-multiplicaci\u00f3n transpuesta. Pre-multipliquemos a \\((Eq. 1)\\) transversalmente por el vector conjugado transpuesto \\(\\overline{v}^t\\): $\\(\\overline{v}^t (A v) = \\overline{v}^t (\\lambda v) = \\lambda (\\overline{v}^t v)\\)$</p> <p>Hagamos la imagen en espejo y ahora tomemos nuestra sub-Ecuaci\u00f3n transpuesta \\((Eq. 2)\\), apliquemos el traspuesto distributivo general a toda la expresi\u00f3n y luego post-multipliquemos por \\(v\\): $\\((A \\overline{v})^t = (\\overline{\\lambda} \\overline{v})^t\\)$ $\\(\\overline{v}^t A^t = \\overline{\\lambda} \\overline{v}^t\\)$ $\\(\\overline{v}^t A^t v = (\\overline{\\lambda} \\overline{v}^t) v = \\overline{\\lambda} (\\overline{v}^t v)\\)$</p> <p>\u00a1Aqu\u00ed acontece el milagro sim\u00e9trico! Como la Hip\u00f3tesis regente del teorema jur\u00f3 que \\(A\\) es Sim\u00e9trica (\\(A^t = A\\)), el t\u00e9rmino izquierdo de la segunda manipulaci\u00f3n es id\u00e9nticamente el mismo bloque estructural que el t\u00e9rmino izquierdo de la primera iteraci\u00f3n: \\(\\overline{v}^t A^t v = \\overline{v}^t A v\\). Si los lados izquierdos computan la misma entidad at\u00f3mica, sus lados derechos deben igualarse incondicionalmente:</p> \\[\\lambda (\\overline{v}^t v) = \\overline{\\lambda} (\\overline{v}^t v)\\] <p>Aisl\u00e9moslos anal\u00edticamente restando: $\\((\\lambda - \\overline{\\lambda}) (\\overline{v}^t v) = 0\\)$</p> <p>Deteng\u00e1monos en el factor constante \\((\\overline{v}^t v)\\). Si multiplicamos un vector complejo conjugado por s\u00ed mismo, la matem\u00e1tica topol\u00f3gica estricta dicta que estamos calculando la suma de los valores absolutos al cuadrado de todas sus componentes (\\(\\sum |v_k|^2\\)). Como el autovector nunca puede valer estrictamente 0 (regla madre matricial), esta suma es r\u00edgidamente estricta a un real positivo puro: \\(\\overline{v}^t v &gt; 0\\).</p> <p>Por ende, el factor vectorial subyacente nunca claudica a cero. La \u00daNICA salida l\u00f3gica para cumplir la balanza es que el par\u00e9ntesis de autovalores colapse a 0: $\\(\\lambda - \\overline{\\lambda} = 0\\)$ $\\(\\lambda = \\overline{\\lambda}\\)$</p> <p>Para que un n\u00famero complejo resulte de igual valor y signo que su propio clon conjugado imaginario \\((\\alpha + \\beta i = \\alpha - \\beta i)\\), es deductivamente obligatorio y evidente que su parte imaginaria debe ser id\u00e9nticamente cero (\\beta = 0).  Ergo, \\(\\lambda\\) es un N\u00famero Real al 100%. Queda demostrada la primera pata del Teorema.</p>"},{"location":"demostraciones/04_teorema_espectral/#parte-ii-ortogonalidad-estricta-de-autovectores","title":"Parte II: Ortogonalidad Estricta de Autovectores","text":"<p>Superada la validaci\u00f3n de ra\u00edces puramente reales, sumerj\u00e1monos en el espacio para examinar qu\u00e9 lazos invisibles atan a autovectores (\\(v_1, v_2\\)) inyectados desde dos autovalores radicalmente dispares (\\(\\lambda_1 \\neq \\lambda_2\\)).</p> <p>Arrancamos con sus verdades aisladas: $\\(A v_1 = \\lambda_1 v_1\\)$ $\\(A v_2 = \\lambda_2 v_2\\)$</p> <p>Apelaremos a una t\u00e9cnica asimilable a la de la primera parte: el testeo cruzado.  A la primera f\u00f3rmula, la pre-multiplicaremos dot-cruzado en producto interno por el segundo autovector transpuesto (\\(v_2^t\\)): $\\(v_2^t A v_1 = v_2^t (\\lambda_1 v_1) = \\lambda_1 (v_2^t v_1)\\)$</p> <p>Por otro lado, aplicaremos la transposici\u00f3n matricial distributiva completa de ambos flancos sobre la segunda verdad definitoria, y luego la post-multiplicaremos libremente por el vector anexo \\(v_1\\): $\\((A v_2)^t = (\\lambda_2 v_2)^t\\)$ $\\(v_2^t A^t = \\lambda_2 v_2^t\\)$ $\\(v_2^t A^t v_1 = (\\lambda_2 v_2^t) v_1 = \\lambda_2 (v_2^t v_1)\\)$</p> <p>Otra vez invocamos el as bajo la manga del enigm\u00e1tico autor: La matriz es rigurosamente sim\u00e9trica (\\(A^t = A\\)). Aplicando esta sustituci\u00f3n milagrosa al lado izquierdo de nuestro \u00faltimo experimento, notamos que ambas expresiones en sus hemisferios izquierdos son clones formales paralelos (\\(v_2^t A v_1 = v_2^t A v_1\\)).</p> <p>Procedemos a igualar sin reparos la derecha de ambas balanzas:</p> \\[\\lambda_1 (v_2^t v_1) = \\lambda_2 (v_2^t v_1)\\] <p>Despejando anal\u00edticamente su diferencial a un flanco: $\\((\\lambda_1 - \\lambda_2) (v_2^t v_1) = 0\\)$</p> <p>Al analizar esta disyuntiva, el postulado del teorema exige escrutinio sobre autovectores enclaustrados en autovalores distintos. Como es f\u00e1ctico en todos estos espacios que \\(\\lambda_1 \\neq \\lambda_2\\), salta contundentemente a la vista que el delta multiplicativo jam\u00e1s ser\u00e1 cero (\\((\\lambda_1 - \\lambda_2) \\neq 0\\)).</p> <p>Para satisfacer la inquebrantable anulaci\u00f3n demandada por la ecuaci\u00f3n subyacente en el lado derecho, la l\u00f3gica computacional castiga dictaminando que el otro multiplicador at\u00f3mico tiene que ser rigurosamente portador del nulo:</p> \\[v_2^t v_1 = 0\\] <p>\u00bfQu\u00e9 implica estructural y geom\u00e9tricamente en el campo Euclidiano \\(\\mathbb{R}^n\\) que el producto punto escalar (o inner-product) arrojado entre dos vectores dictamine como resultado un rotundo \\(0\\)?  Implica anal\u00edticamente, de forma perfecta y sublime, que las direcciones vectoriales que describen a ambos autovectores habitan formando un \u00e1ngulo tridimensional perfecto de 90\u00b0 grados.</p> <p>Los autovectores de matrices sim\u00e9tricas est\u00e1n constre\u00f1idos a ser Perfectamente Ortogonales \\(\\dots\\) Q.E.D.</p>"},{"location":"demostraciones/04_teorema_espectral/#conclusion-integral","title":"Conclusi\u00f3n Integral","text":"<p>La matriz sim\u00e9trica fuerza a que la imagen de sus auto-proyecciones sea indistinguible sea que se la empuje por izquierda o por derecha (pre-multipilic\u00e1ndola o transponi\u00e9ndola). Esto encajona y extirpa sistem\u00e1ticamente todo desv\u00edo o atisbo al campo complejo (haciendo \\(\\lambda = \\overline{\\lambda}\\)), y subyuga el espaciamiento geom\u00e9trico vectorizado a anular cualquier tipo de solapamiento direccional (imponiendo la perpendicularidad \\(v_i \\cdot v_j = 0\\)). Ergo, desatan diagonalizabilidad ortogonal intachable.</p>"},{"location":"demostraciones/04_teorema_espectral/#referencias-para-validacion","title":"Referencias para Validaci\u00f3n","text":"<p>Dado el colosal impacto que imparte el Teorema Espectral en \u00c1lgebra, sugerimos ratificar su base anal\u00edtica inductiva desde cimientos supremos y did\u00e1cticos: * Wikipedia: Spectral Theorem (Symmetric matrices): Marco demostrativo formal del Teorema Espectral para operadores escalares auto-adjuntos finitos. * MIT 18.06 OpenCourseWare - Clase 25 (Symmetric Matrices and Positive Definiteness) - Prof. Gilbert Strang: Clase imperdible e indiscutible donde el decano del \u00e1lgebra traza asombrosamente la misma ruta inductiva dual para desarticular el mito de los autovalores complejos (Min. 09:30) y para forzar la Ortogonalidad Estricta de la base resultante (Min. 16:00).</p>"},{"location":"demostraciones/04_teorema_espectral/#verificacion-empirica-computacional","title":"Verificaci\u00f3n Emp\u00edrica Computacional","text":"<p>La inmaculada certidumbre de esta doctrina se somete a testeo algor\u00edtmico (incorporando una tolerancia muy sana de floats de hasta \\(1e^{-10}\\) debida a c\u00e1lculos densos superpuestos de punto flotante al descomponer autovectores mediante iteraci\u00f3n QR en NumPy) en el validador contiguo que bombardea matrices sim\u00e9tricas aleatorias.</p> <pre><code>import numpy as np\n\ndef verificar_teorema_espectral(n: int = 5, iteraciones: int = 1500):\n    \"\"\"\n    Verifica estoc\u00e1sticamente el dictamen del Teorema Espectral para\n    matrices rigurosamente Reales y Sim\u00e9tricas (A = A^T).\n\n    A trav\u00e9s de la fuerza estoc\u00e1stica iterativa validamos que:\n    1) Sus Eigenvalues no escapan jam\u00e1s al plano Complejo (Parte imaginaria nula).\n    2) El producto vectorial interno de sus combinaciones de Eigenvectors dispares\n       rinde ceros perfectos, dictaminando Ortogonalidad Espacial (Angulo de 90\u00b0 grados).\n\n    Args:\n        n: Dimensi\u00f3n base cuadrada evaluada.\n        iteraciones: Cantidad de matrices sim\u00e9tricas estresadas al azar.\n    \"\"\"\n    np.random.seed(42)  \n\n    exitos_reales = 0\n    exitos_ortogonalidad = 0\n\n    for _ in range(iteraciones):\n        # 1. Armamos una matriz aleatoria de partida\n        M = np.random.randn(n, n) * 10\n\n        # 2. Obligamos su simetr\u00eda matem\u00e1tica sum\u00e1ndola a su transpuesta (A = A^T garantizado)\n        A = M + M.T\n\n        # 3. Solicitamos autovalores y autovectores sin restricci\u00f3n forzada a librer\u00eda herm\u00edtica de numpy\n        # (Usamos `eig` est\u00e1ndar y no `eigh` para atrapar si numpy fugase a memoria compleja por error algor\u00edtmico)\n        valores_propios, matriz_autovectores = np.linalg.eig(A)\n\n        # --- TEST 1: INEXISTENCIA DE RA\u00cdCES COMPLEJAS ---\n        # Verificamos si la suma de todas las partes imaginarias absolutas es asint\u00f3ticamente nula.\n        suma_imaginaria = np.sum(np.abs(np.imag(valores_propios)))\n        if np.isclose(suma_imaginaria, 0, atol=1e-12):\n            exitos_reales += 1\n\n        # --- TEST 2: ORTOGONALIDAD ESTRICTA ENTRE PARES DE AUTOVECTORES ---\n        ortogonal_check = True\n\n        # Hacemos iteraciones comparando cada posible par (i, j) donde i != j\n        for i in range(n):\n            for j in range(i + 1, n):\n                # Extraemos las dos columnas correspondientes\n                v1 = matriz_autovectores[:, i]\n                v2 = matriz_autovectores[:, j]\n\n                # Obtenemos su producto interno emp\u00edrico con el vector transpuesto\n                producto_punto = np.dot(v1, v2)\n\n                # Tolerancia flotante: Las matrices de rotaciones/componentes de autovectores\n                # tras c\u00e1lculos pesados de descomposici\u00f3n acarrean bits de ruido en mantisa base 2.\n                # Exigir un cero flotante estricto (0.0 == 0.0) romper\u00eda por error de hadware en R10 o R20.\n                if not np.isclose(producto_punto, 0, atol=1e-10):\n                    ortogonal_check = False\n                    break\n\n            if not ortogonal_check:\n                break\n\n        if ortogonal_check:\n            exitos_ortogonalidad += 1\n\n    # Resultados del Estr\u00e9s\n    print(f\"\\n--- Verificador del Teorema Espectral Real (n={n}) ---\")\n    print(f\"Bucle Masivo Efectuado: {iteraciones} matrices sim\u00e9tricas generadas.\")\n    print(f\"&gt; Ra\u00edces 100% Reales comprobadas: {exitos_reales}/{iteraciones}\")\n    print(f\"&gt; Bases Autovectoriales 100% Ortogonales comprobadas: {exitos_ortogonalidad}/{iteraciones}\")\n\n    if exitos_reales == iteraciones and exitos_ortogonalidad == iteraciones:\n        print(\"\\n&gt;&gt; CONLCUSI\u00d3N: Validado Emp\u00edricamente. La aserci\u00f3n del Lema Espectral domina la matriz.\")\n    else:\n        print(\"\\n&gt;&gt; ANOMAL\u00cdA: Colapso detectado por falla te\u00f3rica o ruido de hardware masivo en mantisa.\")\n\nif __name__ == \"__main__\":\n    verificar_teorema_espectral()\n</code></pre>"}]}